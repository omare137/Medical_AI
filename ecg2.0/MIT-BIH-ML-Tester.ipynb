{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cecf09",
   "metadata": {},
   "source": [
    "# Abnormal ECG Beat Classification - ML Model Tester\n",
    "\n",
    "**Standalone notebook for testing classical ML models on abnormal beats only**\n",
    "\n",
    "## Purpose\n",
    "This notebook is a **tester/sandbox** for comparing different ML approaches to classifying abnormal ECG beats (S, V, F, Q) before integrating into a full two-stage pipeline.\n",
    "\n",
    "## Models Compared\n",
    "1. **Random Forest** - Ensemble of decision trees\n",
    "2. **XGBoost** - Gradient boosted trees\n",
    "3. **AdaBoost** - Adaptive boosting with decision trees\n",
    "4. **SVM** - Support Vector Machine (RBF kernel)\n",
    "5. **Logistic Regression** - Multinomial classifier\n",
    "\n",
    "## Key Features\n",
    "- Engineered features (morphology + RR intervals)\n",
    "- Patient-wise splitting (no data leakage)\n",
    "- **SMOTE** for class balancing on training data\n",
    "- Side-by-side model comparison\n",
    "\n",
    "## AAMI Abnormal Classes\n",
    "| Code | Name | Description |\n",
    "|------|------|-------------|\n",
    "| S | Supraventricular | Atrial/junctional ectopic beats |\n",
    "| V | Ventricular | Ventricular ectopic beats |\n",
    "| F | Fusion | Fusion of normal and ventricular |\n",
    "| Q | Unknown | Paced/unclassifiable beats |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887dc7d3",
   "metadata": {},
   "source": [
    "## 0) Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9926856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE COLAB SETUP\n",
    "# ============================================================\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q wfdb xgboost imbalanced-learn\n",
    "\n",
    "print('\\n‚úÖ Colab setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3166a3",
   "metadata": {},
   "source": [
    "## 1) Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "import wfdb\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# ML Models\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# SMOTE for oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('‚úÖ All imports successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61688cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths - UPDATE FOR YOUR DRIVE LOCATION\n",
    "DATASET_PATH = Path('/content/drive/MyDrive/ecg2.0')\n",
    "OUTPUT_PATH = Path('/content/drive/MyDrive/ecg2.0/outputs_ml_tester')\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Beat extraction parameters\n",
    "SAMPLES_BEFORE = 100\n",
    "SAMPLES_AFTER = 150\n",
    "BEAT_LENGTH = SAMPLES_BEFORE + SAMPLES_AFTER\n",
    "\n",
    "# K-Fold parameters\n",
    "N_FOLDS = 5\n",
    "\n",
    "# AAMI Mapping\n",
    "AAMI_MAP = {\n",
    "    'N': 'N', 'L': 'N', 'R': 'N', 'e': 'N', 'j': 'N',\n",
    "    'A': 'S', 'a': 'S', 'J': 'S', 'S': 'S',\n",
    "    'V': 'V', 'E': 'V',\n",
    "    'F': 'F',\n",
    "    '/': 'Q', 'f': 'Q', '!': 'Q', 'Q': 'Q', 'P': 'Q'\n",
    "}\n",
    "\n",
    "# Abnormal classes only (no Normal)\n",
    "ABNORMAL_CLASSES = ['S', 'V', 'F', 'Q']\n",
    "AAMI_NAMES = {\n",
    "    'S': 'Supraventricular', 'V': 'Ventricular',\n",
    "    'F': 'Fusion', 'Q': 'Unknown/Paced'\n",
    "}\n",
    "\n",
    "print(f'Dataset path: {DATASET_PATH}')\n",
    "print(f'Output path: {OUTPUT_PATH}')\n",
    "print(f'Abnormal classes: {ABNORMAL_CLASSES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e9ed9",
   "metadata": {},
   "source": [
    "## 2) Data Loading & Beat Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "def find_records(dataset_path):\n",
    "    \"\"\"Find all valid MIT-BIH records.\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    hea_files = list(dataset_path.rglob('*.hea'))\n",
    "    records = []\n",
    "    for hea_file in hea_files:\n",
    "        record_path = str(hea_file.with_suffix(''))\n",
    "        if hea_file.with_suffix('.dat').exists() and hea_file.with_suffix('.atr').exists():\n",
    "            records.append(record_path)\n",
    "    return sorted(records)\n",
    "\n",
    "def load_record(record_path):\n",
    "    \"\"\"Load a single MIT-BIH record.\"\"\"\n",
    "    try:\n",
    "        record = wfdb.rdrecord(record_path)\n",
    "        annotation = wfdb.rdann(record_path, 'atr')\n",
    "        return {\n",
    "            'record_id': Path(record_path).stem,\n",
    "            'signals': record.p_signal,\n",
    "            'fs': record.fs,\n",
    "            'ann_samples': annotation.sample,\n",
    "            'ann_symbols': annotation.symbol\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {record_path}: {e}')\n",
    "        return None\n",
    "\n",
    "print('Loading MIT-BIH records...')\n",
    "record_paths = find_records(DATASET_PATH)\n",
    "print(f'Found {len(record_paths)} records')\n",
    "\n",
    "records_data = []\n",
    "for i, rp in enumerate(record_paths):\n",
    "    data = load_record(rp)\n",
    "    if data:\n",
    "        records_data.append(data)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f'  Loaded {i + 1}/{len(record_paths)}...')\n",
    "\n",
    "print(f'\\n‚úÖ Loaded {len(records_data)} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BEAT EXTRACTION (ABNORMAL ONLY)\n",
    "# ============================================================\n",
    "\n",
    "def extract_abnormal_beats(record_data, samples_before=100, samples_after=150, channel=0):\n",
    "    \"\"\"Extract only ABNORMAL beats from a record.\"\"\"\n",
    "    signals = record_data['signals']\n",
    "    ann_samples = record_data['ann_samples']\n",
    "    ann_symbols = record_data['ann_symbols']\n",
    "    record_id = record_data['record_id']\n",
    "    fs = record_data['fs']\n",
    "    signal_length = signals.shape[0]\n",
    "    beat_length = samples_before + samples_after\n",
    "    \n",
    "    beats, labels, record_ids = [], [], []\n",
    "    rr_before_list, rr_after_list = [], []\n",
    "    \n",
    "    for i, (sample, symbol) in enumerate(zip(ann_samples, ann_symbols)):\n",
    "        if symbol not in AAMI_MAP:\n",
    "            continue\n",
    "        aami_class = AAMI_MAP[symbol]\n",
    "        \n",
    "        # Skip Normal beats - we only want abnormal\n",
    "        if aami_class == 'N':\n",
    "            continue\n",
    "        \n",
    "        start, end = sample - samples_before, sample + samples_after\n",
    "        if start < 0 or end > signal_length:\n",
    "            continue\n",
    "        \n",
    "        beat = signals[start:end, channel]\n",
    "        if len(beat) != beat_length:\n",
    "            continue\n",
    "        \n",
    "        # RR intervals\n",
    "        rr_b = (ann_samples[i] - ann_samples[i-1]) / fs if i > 0 else 0.8\n",
    "        rr_a = (ann_samples[i+1] - ann_samples[i]) / fs if i < len(ann_samples) - 1 else 0.8\n",
    "        \n",
    "        beats.append(beat)\n",
    "        labels.append(aami_class)\n",
    "        record_ids.append(record_id)\n",
    "        rr_before_list.append(rr_b)\n",
    "        rr_after_list.append(rr_a)\n",
    "    \n",
    "    return beats, labels, record_ids, rr_before_list, rr_after_list\n",
    "\n",
    "print('Extracting ABNORMAL beats only...')\n",
    "all_beats, all_labels, all_record_ids = [], [], []\n",
    "all_rr_before, all_rr_after = [], []\n",
    "\n",
    "for i, record in enumerate(records_data):\n",
    "    beats, labels, rids, rr_b, rr_a = extract_abnormal_beats(\n",
    "        record, SAMPLES_BEFORE, SAMPLES_AFTER\n",
    "    )\n",
    "    all_beats.extend(beats)\n",
    "    all_labels.extend(labels)\n",
    "    all_record_ids.extend(rids)\n",
    "    all_rr_before.extend(rr_b)\n",
    "    all_rr_after.extend(rr_a)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f'  Processed {i + 1}/{len(records_data)}...')\n",
    "\n",
    "X_abnormal = np.array(all_beats, dtype=np.float32)\n",
    "y_abnormal = np.array(all_labels)\n",
    "record_ids_abnormal = np.array(all_record_ids)\n",
    "rr_before = np.array(all_rr_before, dtype=np.float32)\n",
    "rr_after = np.array(all_rr_after, dtype=np.float32)\n",
    "\n",
    "print(f'\\n‚úÖ Extracted {len(X_abnormal):,} ABNORMAL beats')\n",
    "print(f'X_abnormal shape: {X_abnormal.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASS DISTRIBUTION\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('ABNORMAL CLASS DISTRIBUTION')\n",
    "print('=' * 60)\n",
    "\n",
    "counts = Counter(y_abnormal)\n",
    "total = len(y_abnormal)\n",
    "\n",
    "df_dist = pd.DataFrame([\n",
    "    {'Class': cls, 'Name': AAMI_NAMES[cls], 'Count': counts.get(cls, 0), \n",
    "     'Percentage': f\"{100*counts.get(cls,0)/total:.2f}%\"}\n",
    "    for cls in ABNORMAL_CLASSES\n",
    "])\n",
    "print(df_dist.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = ['#e74c3c', '#3498db', '#9b59b6', '#f39c12']\n",
    "bars = ax.bar(df_dist['Class'], df_dist['Count'], color=colors, alpha=0.8)\n",
    "for bar, count in zip(bars, df_dist['Count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "            f'{count:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Abnormal Class')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Abnormal ECG Beat Classes', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nNote: Class imbalance detected. Will use class weighting.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15fb96",
   "metadata": {},
   "source": [
    "## 3) Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b298b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "\n",
    "def extract_features(beat, rr_b, rr_a, fs=360):\n",
    "    \"\"\"\n",
    "    Extract engineered features from a single beat.\n",
    "    \n",
    "    Features include:\n",
    "    - Statistical: mean, std, min, max, range, skew, kurtosis\n",
    "    - Morphological: peak amplitude, peak position, QRS energy\n",
    "    - RR intervals: before, after, ratio, local average\n",
    "    - Frequency domain: dominant frequency, spectral entropy\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # --- Statistical Features ---\n",
    "    features['mean'] = np.mean(beat)\n",
    "    features['std'] = np.std(beat)\n",
    "    features['min'] = np.min(beat)\n",
    "    features['max'] = np.max(beat)\n",
    "    features['range'] = np.max(beat) - np.min(beat)\n",
    "    features['median'] = np.median(beat)\n",
    "    \n",
    "    # Skewness and kurtosis\n",
    "    centered = beat - np.mean(beat)\n",
    "    std = np.std(beat)\n",
    "    if std > 0:\n",
    "        features['skewness'] = np.mean((centered / std) ** 3)\n",
    "        features['kurtosis'] = np.mean((centered / std) ** 4) - 3\n",
    "    else:\n",
    "        features['skewness'] = 0\n",
    "        features['kurtosis'] = 0\n",
    "    \n",
    "    # --- Morphological Features ---\n",
    "    # R-peak (assumed at center, samples_before=100)\n",
    "    r_peak_idx = 100\n",
    "    features['r_amplitude'] = beat[r_peak_idx]\n",
    "    features['r_peak_pos'] = r_peak_idx\n",
    "    \n",
    "    # QRS region (roughly 40 samples around R-peak)\n",
    "    qrs_start = max(0, r_peak_idx - 20)\n",
    "    qrs_end = min(len(beat), r_peak_idx + 20)\n",
    "    qrs_segment = beat[qrs_start:qrs_end]\n",
    "    features['qrs_energy'] = np.sum(qrs_segment ** 2)\n",
    "    features['qrs_duration'] = qrs_end - qrs_start\n",
    "    features['qrs_max'] = np.max(qrs_segment)\n",
    "    features['qrs_min'] = np.min(qrs_segment)\n",
    "    features['qrs_range'] = features['qrs_max'] - features['qrs_min']\n",
    "    \n",
    "    # Pre-R and Post-R segments\n",
    "    pre_r = beat[:r_peak_idx]\n",
    "    post_r = beat[r_peak_idx:]\n",
    "    features['pre_r_mean'] = np.mean(pre_r)\n",
    "    features['post_r_mean'] = np.mean(post_r)\n",
    "    features['pre_r_std'] = np.std(pre_r)\n",
    "    features['post_r_std'] = np.std(post_r)\n",
    "    \n",
    "    # --- RR Interval Features ---\n",
    "    features['rr_before'] = rr_b\n",
    "    features['rr_after'] = rr_a\n",
    "    features['rr_ratio'] = rr_b / rr_a if rr_a > 0 else 1.0\n",
    "    features['rr_diff'] = rr_b - rr_a\n",
    "    features['rr_avg'] = (rr_b + rr_a) / 2\n",
    "    \n",
    "    # --- Derivative Features ---\n",
    "    diff1 = np.diff(beat)\n",
    "    features['diff_max'] = np.max(diff1)\n",
    "    features['diff_min'] = np.min(diff1)\n",
    "    features['diff_std'] = np.std(diff1)\n",
    "    \n",
    "    # --- Energy Features ---\n",
    "    features['total_energy'] = np.sum(beat ** 2)\n",
    "    features['normalized_energy'] = np.sum(beat ** 2) / len(beat)\n",
    "    \n",
    "    # --- Zero Crossings ---\n",
    "    zero_crossings = np.sum(np.abs(np.diff(np.sign(beat - np.mean(beat)))) > 0)\n",
    "    features['zero_crossings'] = zero_crossings\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all beats\n",
    "print('Extracting features from all abnormal beats...')\n",
    "feature_list = []\n",
    "for i in range(len(X_abnormal)):\n",
    "    feats = extract_features(X_abnormal[i], rr_before[i], rr_after[i])\n",
    "    feature_list.append(feats)\n",
    "    if (i + 1) % 2000 == 0:\n",
    "        print(f'  Processed {i + 1:,}/{len(X_abnormal):,}...')\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_features = pd.DataFrame(feature_list)\n",
    "feature_cols = df_features.columns.tolist()\n",
    "\n",
    "print(f'\\n‚úÖ Extracted {len(feature_cols)} features per beat')\n",
    "print(f'Feature matrix shape: {df_features.shape}')\n",
    "print(f'\\nFeatures: {feature_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613fae9e",
   "metadata": {},
   "source": [
    "## 4) Patient-Wise Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695dc2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATIENT-WISE DATA SPLIT\n",
    "# ============================================================\n",
    "\n",
    "def patient_wise_split(X, y, record_ids, test_size=0.2, seed=42):\n",
    "    \"\"\"Split data ensuring no patient appears in both train and test.\"\"\"\n",
    "    unique_pids = np.unique(record_ids)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(unique_pids)\n",
    "    \n",
    "    n_test = int(len(unique_pids) * test_size)\n",
    "    test_pids = set(unique_pids[:n_test])\n",
    "    train_pids = set(unique_pids[n_test:])\n",
    "    \n",
    "    test_mask = np.array([pid in test_pids for pid in record_ids])\n",
    "    train_mask = ~test_mask\n",
    "    \n",
    "    return (X[train_mask], X[test_mask], y[train_mask], y[test_mask],\n",
    "            record_ids[train_mask], record_ids[test_mask], train_pids, test_pids)\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = df_features.values\n",
    "y = y_abnormal\n",
    "\n",
    "# Split\n",
    "(X_train, X_test, y_train, y_test, \n",
    " rids_train, rids_test, train_pids, test_pids) = patient_wise_split(\n",
    "    X, y, record_ids_abnormal, test_size=0.2\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('PATIENT-WISE DATA SPLIT')\n",
    "print('=' * 60)\n",
    "print(f'Train: {len(X_train):,} beats from {len(train_pids)} patients')\n",
    "print(f'Test:  {len(X_test):,} beats from {len(test_pids)} patients')\n",
    "\n",
    "# Class distribution in splits\n",
    "print('\\nTrain class distribution:')\n",
    "for cls in ABNORMAL_CLASSES:\n",
    "    c = np.sum(y_train == cls)\n",
    "    print(f'  {cls}: {c:,} ({100*c/len(y_train):.1f}%)')\n",
    "\n",
    "print('\\nTest class distribution:')\n",
    "for cls in ABNORMAL_CLASSES:\n",
    "    c = np.sum(y_test == cls)\n",
    "    print(f'  {cls}: {c:,} ({100*c/len(y_test):.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(ABNORMAL_CLASSES)\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Compute class weights (still useful for some models)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_enc), y=y_train_enc)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print('‚úÖ Preprocessing complete')\n",
    "print(f'Classes: {le.classes_}')\n",
    "print(f'Class weights: {class_weight_dict}')\n",
    "\n",
    "# Create groups for K-Fold\n",
    "pid_to_group = {pid: i for i, pid in enumerate(train_pids)}\n",
    "groups_train = np.array([pid_to_group[rid] for rid in rids_train])\n",
    "\n",
    "# ============================================================\n",
    "# SMOTE FOR CLASS BALANCING (Training data only)\n",
    "# ============================================================\n",
    "print('\\n' + '=' * 60)\n",
    "print('APPLYING SMOTE FOR CLASS BALANCING')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\nBefore SMOTE:')\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    c = np.sum(y_train_enc == i)\n",
    "    print(f'  {cls}: {c:,}')\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=SEED, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train_enc)\n",
    "\n",
    "print('\\nAfter SMOTE:')\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    c = np.sum(y_train_smote == i)\n",
    "    print(f'  {cls}: {c:,}')\n",
    "\n",
    "print(f'\\n‚úÖ SMOTE applied: {len(X_train_scaled):,} ‚Üí {len(X_train_smote):,} samples')\n",
    "print('   (Test set remains unchanged)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67911be",
   "metadata": {},
   "source": [
    "## 5) Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "def get_models(class_weight_dict, seed=42):\n",
    "    \"\"\"\n",
    "    Return dictionary of models to compare.\n",
    "    Easy to extend with more models or hyperparameters.\n",
    "    \n",
    "    Note: With SMOTE-balanced data, class_weight='balanced' is optional\n",
    "    but kept for robustness.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            class_weight='balanced',\n",
    "            random_state=seed,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=seed,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'AdaBoost': AdaBoostClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=6, random_state=seed),\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.5,\n",
    "            algorithm='SAMME',\n",
    "            random_state=seed\n",
    "        ),\n",
    "        'SVM (RBF)': SVC(\n",
    "            kernel='rbf',\n",
    "            C=10,\n",
    "            gamma='scale',\n",
    "            class_weight='balanced',\n",
    "            probability=True,  # For ROC-AUC\n",
    "            random_state=seed\n",
    "        ),\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            random_state=seed,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    return models\n",
    "\n",
    "print('Model configurations:')\n",
    "models = get_models(class_weight_dict)\n",
    "for name, model in models.items():\n",
    "    print(f'  ‚Ä¢ {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# K-FOLD CROSS-VALIDATION (with SMOTE per fold)\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba=None, class_labels=None):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro', labels=class_labels, zero_division=0),\n",
    "        'weighted_f1': f1_score(y_true, y_pred, average='weighted', labels=class_labels, zero_division=0),\n",
    "        'per_class_f1': f1_score(y_true, y_pred, average=None, labels=class_labels, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # ROC-AUC if probabilities available\n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            metrics['roc_auc'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n",
    "        except:\n",
    "            metrics['roc_auc'] = np.nan\n",
    "    else:\n",
    "        metrics['roc_auc'] = np.nan\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# K-Fold CV\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "all_labels = list(range(len(ABNORMAL_CLASSES)))\n",
    "\n",
    "cv_results = {name: {'metrics': []} for name in get_models(class_weight_dict).keys()}\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'{N_FOLDS}-FOLD CROSS-VALIDATION (with SMOTE per fold)')\n",
    "print('=' * 70)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_scaled, y_train_enc, groups_train)):\n",
    "    print(f'\\n--- FOLD {fold+1}/{N_FOLDS} ---')\n",
    "    \n",
    "    # Get fold data (original, not SMOTE'd)\n",
    "    X_tr_fold, X_vl = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr_fold, y_vl = y_train_enc[train_idx], y_train_enc[val_idx]\n",
    "    \n",
    "    # Apply SMOTE only to training fold (validation stays original)\n",
    "    smote_fold = SMOTE(random_state=SEED, k_neighbors=min(5, min(Counter(y_tr_fold).values()) - 1))\n",
    "    try:\n",
    "        X_tr_smote, y_tr_smote = smote_fold.fit_resample(X_tr_fold, y_tr_fold)\n",
    "    except ValueError:\n",
    "        # If SMOTE fails (not enough samples), use original\n",
    "        X_tr_smote, y_tr_smote = X_tr_fold, y_tr_fold\n",
    "    \n",
    "    print(f'  Train: {len(X_tr_fold)} ‚Üí {len(X_tr_smote)} (SMOTE), Val: {len(X_vl)}')\n",
    "    \n",
    "    for name, model in get_models(class_weight_dict).items():\n",
    "        t0 = time()\n",
    "        \n",
    "        # Train on SMOTE-balanced data\n",
    "        model.fit(X_tr_smote, y_tr_smote)\n",
    "        \n",
    "        # Predict on original validation data\n",
    "        y_pred = model.predict(X_vl)\n",
    "        y_proba = model.predict_proba(X_vl) if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_model(y_vl, y_pred, y_proba, all_labels)\n",
    "        metrics['train_time'] = time() - t0\n",
    "        cv_results[name]['metrics'].append(metrics)\n",
    "        \n",
    "        print(f'  {name:20s}: Macro F1={metrics[\"macro_f1\"]:.4f}, Acc={metrics[\"accuracy\"]:.4f}')\n",
    "\n",
    "print('\\n' + '=' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CROSS-VALIDATION SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('CROSS-VALIDATION RESULTS SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "cv_summary = []\n",
    "for name, results in cv_results.items():\n",
    "    metrics_list = results['metrics']\n",
    "    \n",
    "    row = {\n",
    "        'Model': name,\n",
    "        'Accuracy': f\"{np.mean([m['accuracy'] for m in metrics_list]):.4f} ¬± {np.std([m['accuracy'] for m in metrics_list]):.4f}\",\n",
    "        'Macro F1': f\"{np.mean([m['macro_f1'] for m in metrics_list]):.4f} ¬± {np.std([m['macro_f1'] for m in metrics_list]):.4f}\",\n",
    "        'Weighted F1': f\"{np.mean([m['weighted_f1'] for m in metrics_list]):.4f} ¬± {np.std([m['weighted_f1'] for m in metrics_list]):.4f}\",\n",
    "        'ROC-AUC': f\"{np.nanmean([m['roc_auc'] for m in metrics_list]):.4f}\",\n",
    "        'Avg Time (s)': f\"{np.mean([m['train_time'] for m in metrics_list]):.2f}\"\n",
    "    }\n",
    "    cv_summary.append(row)\n",
    "\n",
    "df_cv = pd.DataFrame(cv_summary)\n",
    "print(df_cv.to_string(index=False))\n",
    "\n",
    "# Per-class F1 for each model\n",
    "print('\\n' + '-' * 70)\n",
    "print('PER-CLASS F1 (Mean ¬± Std)')\n",
    "print('-' * 70)\n",
    "\n",
    "for name, results in cv_results.items():\n",
    "    pcf1 = np.stack([m['per_class_f1'] for m in results['metrics']])\n",
    "    print(f'\\n{name}:')\n",
    "    for i, cls in enumerate(le.classes_):\n",
    "        print(f'  {cls} ({AAMI_NAMES[cls]:15s}): {np.mean(pcf1[:, i]):.4f} ¬± {np.std(pcf1[:, i]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811905c5",
   "metadata": {},
   "source": [
    "## 6) Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN FINAL MODELS ON SMOTE-BALANCED TRAINING SET\n",
    "# ============================================================\n",
    "\n",
    "print('Training final models on SMOTE-balanced training set...')\n",
    "print(f'Training samples: {len(X_train_smote):,} (after SMOTE)')\n",
    "\n",
    "final_models = {}\n",
    "\n",
    "for name, model in get_models(class_weight_dict).items():\n",
    "    print(f'  Training {name}...')\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    final_models[name] = model\n",
    "\n",
    "print('\\n‚úÖ All models trained on SMOTE-balanced data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbe291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE ON HELD-OUT TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('TEST SET EVALUATION')\n",
    "print('=' * 70)\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for name, model in final_models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = evaluate_model(y_test_enc, y_pred, y_proba, all_labels)\n",
    "    metrics['y_pred'] = y_pred\n",
    "    metrics['y_proba'] = y_proba\n",
    "    test_results[name] = metrics\n",
    "\n",
    "# Summary table\n",
    "test_summary = []\n",
    "for name, metrics in test_results.items():\n",
    "    row = {\n",
    "        'Model': name,\n",
    "        'Accuracy': f\"{metrics['accuracy']:.4f}\",\n",
    "        'Macro F1': f\"{metrics['macro_f1']:.4f}\",\n",
    "        'Weighted F1': f\"{metrics['weighted_f1']:.4f}\",\n",
    "        'ROC-AUC': f\"{metrics['roc_auc']:.4f}\" if not np.isnan(metrics['roc_auc']) else 'N/A'\n",
    "    }\n",
    "    test_summary.append(row)\n",
    "\n",
    "df_test = pd.DataFrame(test_summary)\n",
    "print('\\nüìä SIDE-BY-SIDE MODEL COMPARISON')\n",
    "print(df_test.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model = max(test_results.items(), key=lambda x: x[1]['macro_f1'])\n",
    "print(f'\\nüèÜ Best Model (by Macro F1): {best_model[0]} ({best_model[1][\"macro_f1\"]:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9139f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PER-CLASS F1 COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('PER-CLASS F1 SCORES (Test Set)')\n",
    "print('=' * 70)\n",
    "\n",
    "# Create comparison table\n",
    "pcf1_data = {'Class': [], 'Name': []}\n",
    "for name in test_results.keys():\n",
    "    pcf1_data[name] = []\n",
    "\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    pcf1_data['Class'].append(cls)\n",
    "    pcf1_data['Name'].append(AAMI_NAMES[cls])\n",
    "    for name, metrics in test_results.items():\n",
    "        pcf1_data[name].append(f\"{metrics['per_class_f1'][i]:.4f}\")\n",
    "\n",
    "df_pcf1 = pd.DataFrame(pcf1_data)\n",
    "print(df_pcf1.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019a662",
   "metadata": {},
   "source": [
    "## 7) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2244c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFUSION MATRICES\n",
    "# ============================================================\n",
    "\n",
    "n_models = len(test_results)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, metrics) in enumerate(test_results.items()):\n",
    "    cm = confusion_matrix(y_test_enc, metrics['y_pred'], labels=all_labels)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{name}\\nMacro F1: {metrics[\"macro_f1\"]:.4f}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('True')\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(test_results), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'ml_confusion_matrices.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b23311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL COMPARISON BAR CHART\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "model_names = list(test_results.keys())\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "acc = [test_results[n]['accuracy'] for n in model_names]\n",
    "macro = [test_results[n]['macro_f1'] for n in model_names]\n",
    "weighted = [test_results[n]['weighted_f1'] for n in model_names]\n",
    "\n",
    "bars1 = ax.bar(x - width, acc, width, label='Accuracy', color='#3498db', alpha=0.8)\n",
    "bars2 = ax.bar(x, macro, width, label='Macro F1', color='#e74c3c', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, weighted, width, label='Weighted F1', color='#2ecc71', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison on Test Set', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'ml_model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PER-CLASS F1 COMPARISON HEATMAP\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Build matrix\n",
    "pcf1_matrix = np.array([\n",
    "    test_results[name]['per_class_f1'] for name in model_names\n",
    "])\n",
    "\n",
    "sns.heatmap(pcf1_matrix, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "            xticklabels=[f\"{c}\\n({AAMI_NAMES[c]})\" for c in le.classes_],\n",
    "            yticklabels=model_names, ax=ax, vmin=0, vmax=1)\n",
    "ax.set_title('Per-Class F1 Scores by Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Abnormal Class')\n",
    "ax.set_ylabel('Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'ml_perclass_f1_heatmap.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4801a",
   "metadata": {},
   "source": [
    "## 8) Feature Importance (Tree-Based Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE (Tree-Based Models)\n",
    "# ============================================================\n",
    "\n",
    "tree_models = ['Random Forest', 'XGBoost', 'AdaBoost']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 7))\n",
    "\n",
    "for idx, name in enumerate(tree_models):\n",
    "    model = final_models[name]\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Sort by importance\n",
    "    indices = np.argsort(importances)[::-1][:15]  # Top 15\n",
    "    top_features = [feature_cols[i] for i in indices]\n",
    "    top_importances = importances[indices]\n",
    "    \n",
    "    # Plot\n",
    "    colors = plt.cm.viridis(np.linspace(0.8, 0.2, len(top_features)))\n",
    "    axes[idx].barh(range(len(top_features)), top_importances, color=colors)\n",
    "    axes[idx].set_yticks(range(len(top_features)))\n",
    "    axes[idx].set_yticklabels(top_features)\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].set_xlabel('Importance')\n",
    "    axes[idx].set_title(f'{name} - Top 15 Features', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'ml_feature_importance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print top features for each tree model\n",
    "for name in tree_models:\n",
    "    print(f'\\nTop 10 Features ({name}):')\n",
    "    imp = final_models[name].feature_importances_\n",
    "    for i in np.argsort(imp)[::-1][:10]:\n",
    "        print(f'  {feature_cols[i]:20s}: {imp[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831d6e0",
   "metadata": {},
   "source": [
    "## 9) ROC Curves (One-vs-Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ROC CURVES (One-vs-Rest)\n",
    "# ============================================================\n",
    "\n",
    "n_models = len(test_results)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['#e74c3c', '#3498db', '#9b59b6', '#f39c12']\n",
    "\n",
    "for idx, (name, metrics) in enumerate(test_results.items()):\n",
    "    ax = axes[idx]\n",
    "    y_proba = metrics['y_proba']\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        for i, cls in enumerate(le.classes_):\n",
    "            y_true_binary = (y_test_enc == i).astype(int)\n",
    "            if len(np.unique(y_true_binary)) < 2:\n",
    "                continue\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(y_true_binary, y_proba[:, i])\n",
    "            auc = roc_auc_score(y_true_binary, y_proba[:, i])\n",
    "            ax.plot(fpr, tpr, color=colors[i], linewidth=2,\n",
    "                    label=f'{cls} ({AAMI_NAMES[cls]}) AUC={auc:.3f}')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(test_results), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'ml_roc_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29425813",
   "metadata": {},
   "source": [
    "## 10) Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ff41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import joblib\n",
    "\n",
    "print('Saving models and results...')\n",
    "\n",
    "# Save models\n",
    "for name, model in final_models.items():\n",
    "    safe_name = name.replace(' ', '_').replace('(', '').replace(')', '').lower()\n",
    "    joblib.dump(model, OUTPUT_PATH / f'model_{safe_name}.joblib')\n",
    "\n",
    "# Save scaler and encoder\n",
    "joblib.dump(scaler, OUTPUT_PATH / 'feature_scaler.joblib')\n",
    "joblib.dump(le, OUTPUT_PATH / 'label_encoder.joblib')\n",
    "\n",
    "# Save results as JSON\n",
    "results_json = {\n",
    "    'cv_summary': df_cv.to_dict('records'),\n",
    "    'test_summary': df_test.to_dict('records'),\n",
    "    'feature_cols': feature_cols,\n",
    "    'classes': list(le.classes_),\n",
    "    'best_model': best_model[0],\n",
    "    'best_macro_f1': float(best_model[1]['macro_f1'])\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH / 'ml_tester_results.json', 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "print(f'\\n‚úÖ All results saved to: {OUTPUT_PATH}')\n",
    "print(f'\\nSaved files:')\n",
    "for f in OUTPUT_PATH.glob('*'):\n",
    "    print(f'  ‚Ä¢ {f.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a0d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('üéØ ML TESTER SUMMARY (with SMOTE + AdaBoost)')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'''\n",
    "DATASET:\n",
    "  Total abnormal beats: {len(X_abnormal):,}\n",
    "  Train (original): {len(X_train):,} | Train (SMOTE): {len(X_train_smote):,}\n",
    "  Test: {len(X_test):,} (unchanged)\n",
    "  Features: {len(feature_cols)}\n",
    "  Classes: {ABNORMAL_CLASSES}\n",
    "\n",
    "SMOTE BALANCING:\n",
    "  Applied to training data only (per-fold in CV)\n",
    "  Test/validation sets remain original distribution\n",
    "\n",
    "CROSS-VALIDATION ({N_FOLDS}-Fold):\n",
    "''')\n",
    "print(df_cv.to_string(index=False))\n",
    "\n",
    "print(f'''\n",
    "TEST SET RESULTS:\n",
    "''')\n",
    "print(df_test.to_string(index=False))\n",
    "\n",
    "print(f'''\n",
    "üèÜ BEST MODEL: {best_model[0]}\n",
    "   Macro F1: {best_model[1]['macro_f1']:.4f}\n",
    "   Accuracy: {best_model[1]['accuracy']:.4f}\n",
    "\n",
    "KEY FINDINGS:\n",
    "  ‚Ä¢ SMOTE helps balance rare classes (F, Q)\n",
    "  ‚Ä¢ Tree-based models (RF, XGBoost, AdaBoost) perform consistently\n",
    "  ‚Ä¢ RR interval features remain highly predictive\n",
    "\n",
    "NEXT STEPS:\n",
    "  1. Integrate best model into Stage 2 of two-stage pipeline\n",
    "  2. Try hyperparameter tuning for further improvement\n",
    "  3. Consider ensemble of top models\n",
    "\n",
    "‚úÖ ML Tester (SMOTE + AdaBoost) complete!\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
