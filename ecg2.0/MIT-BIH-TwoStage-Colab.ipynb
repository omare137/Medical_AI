{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# MIT-BIH ECG Classification: Two-Stage Deep Learning Pipeline\n",
    "\n",
    "**Google Colab Version**\n",
    "\n",
    "This notebook implements a **two-stage hierarchical deep learning pipeline** for ECG beat classification:\n",
    "\n",
    "## Pipeline Architecture:\n",
    "```\n",
    "Stage 1: Binary Classification (Normal vs Abnormal)\n",
    "    ‚Üì (beats classified as Abnormal)\n",
    "Stage 2: Multiclass Classification (S, V, F, Q)\n",
    "```\n",
    "\n",
    "## Key Features:\n",
    "- **Dual-input CNN** (ECG waveform + RR intervals)\n",
    "- **Patient-wise K-Fold** cross-validation (no beat leakage)\n",
    "- **Stage 1**: Optimized for **high recall** of abnormal beats\n",
    "- **Stage 2**: Optimized for **Macro F1** across abnormal classes\n",
    "- **Class weighting** for imbalanced data\n",
    "\n",
    "## AAMI Superclasses:\n",
    "| Class | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| N | Normal | Normal beats |\n",
    "| S | Abnormal | Supraventricular ectopic |\n",
    "| V | Abnormal | Ventricular ectopic |\n",
    "| F | Abnormal | Fusion beats |\n",
    "| Q | Abnormal | Unknown/Paced |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-header",
   "metadata": {},
   "source": [
    "## 0) Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE COLAB SETUP\n",
    "# ============================================================\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q wfdb\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'GPU available: {tf.config.list_physical_devices(\"GPU\")}')\n",
    "\n",
    "# Set memory growth for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f'Memory growth enabled for {len(gpus)} GPU(s)')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print('\\n‚úÖ Colab setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1) Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seeds\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import pandas as pd\n",
    "\n",
    "import wfdb\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "print('\\n‚úÖ All imports successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Paths - UPDATE FOR YOUR DRIVE LOCATION\n",
    "DATASET_PATH = Path('/content/drive/MyDrive/ecg2.0')\n",
    "OUTPUT_PATH = Path('/content/drive/MyDrive/ecg2.0/outputs_twostage')\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Beat extraction parameters\n",
    "SAMPLES_BEFORE = 100\n",
    "SAMPLES_AFTER = 150\n",
    "BEAT_LENGTH = SAMPLES_BEFORE + SAMPLES_AFTER\n",
    "\n",
    "# Training parameters\n",
    "N_FOLDS = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25\n",
    "PATIENCE = 8\n",
    "\n",
    "# AAMI Superclass Mapping\n",
    "AAMI_MAP = {\n",
    "    'N': 'N', 'L': 'N', 'R': 'N', 'e': 'N', 'j': 'N',\n",
    "    'A': 'S', 'a': 'S', 'J': 'S', 'S': 'S',\n",
    "    'V': 'V', 'E': 'V',\n",
    "    'F': 'F',\n",
    "    '/': 'Q', 'f': 'Q', '!': 'Q', 'Q': 'Q', 'P': 'Q'\n",
    "}\n",
    "\n",
    "AAMI_CLASSES = ['N', 'S', 'V', 'F', 'Q']\n",
    "ABNORMAL_CLASSES = ['S', 'V', 'F', 'Q']  # Stage 2 classes\n",
    "AAMI_NAMES = {\n",
    "    'N': 'Normal', 'S': 'Supraventricular', 'V': 'Ventricular',\n",
    "    'F': 'Fusion', 'Q': 'Unknown/Paced'\n",
    "}\n",
    "\n",
    "print(f'Dataset path: {DATASET_PATH}')\n",
    "print(f'Output path: {OUTPUT_PATH}')\n",
    "print(f'Beat length: {BEAT_LENGTH}')\n",
    "print(f'K-Folds: {N_FOLDS}')\n",
    "print(f'Epochs: {EPOCHS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2) Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "def find_records(dataset_path):\n",
    "    dataset_path = Path(dataset_path)\n",
    "    hea_files = list(dataset_path.rglob('*.hea'))\n",
    "    records = []\n",
    "    for hea_file in hea_files:\n",
    "        record_path = str(hea_file.with_suffix(''))\n",
    "        if hea_file.with_suffix('.dat').exists() and hea_file.with_suffix('.atr').exists():\n",
    "            records.append(record_path)\n",
    "    return sorted(records)\n",
    "\n",
    "def load_record(record_path):\n",
    "    try:\n",
    "        record = wfdb.rdrecord(record_path)\n",
    "        annotation = wfdb.rdann(record_path, 'atr')\n",
    "        return {\n",
    "            'record_id': Path(record_path).stem,\n",
    "            'signals': record.p_signal,\n",
    "            'fs': record.fs,\n",
    "            'ann_samples': annotation.sample,\n",
    "            'ann_symbols': annotation.symbol\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None\n",
    "\n",
    "print('Loading MIT-BIH records...')\n",
    "record_paths = find_records(DATASET_PATH)\n",
    "print(f'Found {len(record_paths)} records')\n",
    "\n",
    "records_data = []\n",
    "for i, rp in enumerate(record_paths):\n",
    "    data = load_record(rp)\n",
    "    if data:\n",
    "        records_data.append(data)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f'  Loaded {i + 1}/{len(record_paths)}...')\n",
    "\n",
    "print(f'\\n‚úÖ Loaded {len(records_data)} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beat-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BEAT EXTRACTION\n",
    "# ============================================================\n",
    "\n",
    "def extract_beats(record_data, samples_before=100, samples_after=150, channel=0):\n",
    "    signals = record_data['signals']\n",
    "    ann_samples = record_data['ann_samples']\n",
    "    ann_symbols = record_data['ann_symbols']\n",
    "    record_id = record_data['record_id']\n",
    "    fs = record_data['fs']\n",
    "    signal_length = signals.shape[0]\n",
    "    beat_length = samples_before + samples_after\n",
    "    \n",
    "    beats, aami_labels, record_ids = [], [], []\n",
    "    rr_before_list, rr_after_list = [], []\n",
    "    \n",
    "    for i, (sample, symbol) in enumerate(zip(ann_samples, ann_symbols)):\n",
    "        if symbol not in AAMI_MAP:\n",
    "            continue\n",
    "        start, end = sample - samples_before, sample + samples_after\n",
    "        if start < 0 or end > signal_length:\n",
    "            continue\n",
    "        beat = signals[start:end, channel]\n",
    "        if len(beat) != beat_length:\n",
    "            continue\n",
    "        \n",
    "        rr_b = (ann_samples[i] - ann_samples[i-1]) / fs if i > 0 else 0.8\n",
    "        rr_a = (ann_samples[i+1] - ann_samples[i]) / fs if i < len(ann_samples) - 1 else 0.8\n",
    "        \n",
    "        beats.append(beat)\n",
    "        aami_labels.append(AAMI_MAP[symbol])\n",
    "        record_ids.append(record_id)\n",
    "        rr_before_list.append(rr_b)\n",
    "        rr_after_list.append(rr_a)\n",
    "    \n",
    "    return beats, aami_labels, record_ids, rr_before_list, rr_after_list\n",
    "\n",
    "print('Extracting beats...')\n",
    "all_beats, all_labels, all_record_ids = [], [], []\n",
    "all_rr_before, all_rr_after = [], []\n",
    "\n",
    "for i, record in enumerate(records_data):\n",
    "    beats, labels, rids, rr_b, rr_a = extract_beats(record, SAMPLES_BEFORE, SAMPLES_AFTER)\n",
    "    all_beats.extend(beats)\n",
    "    all_labels.extend(labels)\n",
    "    all_record_ids.extend(rids)\n",
    "    all_rr_before.extend(rr_b)\n",
    "    all_rr_after.extend(rr_a)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f'  Processed {i + 1}/{len(records_data)}...')\n",
    "\n",
    "X_beats = np.array(all_beats, dtype=np.float32)\n",
    "y = np.array(all_labels)\n",
    "record_ids = np.array(all_record_ids)\n",
    "rr_before = np.array(all_rr_before, dtype=np.float32)\n",
    "rr_after = np.array(all_rr_after, dtype=np.float32)\n",
    "\n",
    "print(f'\\n‚úÖ Extracted {len(X_beats):,} beats')\n",
    "print(f'X_beats: {X_beats.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "class-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASS DISTRIBUTION & BINARY LABELS\n",
    "# ============================================================\n",
    "\n",
    "# Original distribution\n",
    "print('AAMI Class Distribution:')\n",
    "print('-' * 50)\n",
    "counts = Counter(y)\n",
    "for cls in AAMI_CLASSES:\n",
    "    c = counts.get(cls, 0)\n",
    "    print(f'  {cls} ({AAMI_NAMES[cls]:15s}): {c:6,} ({100*c/len(y):.2f}%)')\n",
    "\n",
    "# Create binary labels (Normal=0, Abnormal=1)\n",
    "y_binary = np.array(['Abnormal' if label != 'N' else 'Normal' for label in y])\n",
    "\n",
    "print(f'\\nBinary Distribution (Stage 1):')\n",
    "print('-' * 50)\n",
    "binary_counts = Counter(y_binary)\n",
    "for cls in ['Normal', 'Abnormal']:\n",
    "    c = binary_counts[cls]\n",
    "    print(f'  {cls:10s}: {c:6,} ({100*c/len(y):.2f}%)')\n",
    "\n",
    "# Abnormal-only distribution (Stage 2)\n",
    "abnormal_mask = y != 'N'\n",
    "y_abnormal = y[abnormal_mask]\n",
    "print(f'\\nAbnormal Classes Distribution (Stage 2):')\n",
    "print('-' * 50)\n",
    "abn_counts = Counter(y_abnormal)\n",
    "for cls in ABNORMAL_CLASSES:\n",
    "    c = abn_counts.get(cls, 0)\n",
    "    print(f'  {cls} ({AAMI_NAMES[cls]:15s}): {c:6,} ({100*c/len(y_abnormal):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATIENT-WISE DATA SPLIT\n",
    "# ============================================================\n",
    "\n",
    "def holdout_test_set(X, rr_b, rr_a, y, y_bin, rids, test_size=0.15, seed=42):\n",
    "    unique_ids = np.unique(rids)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(unique_ids)\n",
    "    \n",
    "    n_test = int(len(unique_ids) * test_size)\n",
    "    test_ids = set(unique_ids[:n_test])\n",
    "    trainval_ids = set(unique_ids[n_test:])\n",
    "    \n",
    "    test_mask = np.array([rid in test_ids for rid in rids])\n",
    "    trainval_mask = ~test_mask\n",
    "    \n",
    "    return (\n",
    "        X[trainval_mask], rr_b[trainval_mask], rr_a[trainval_mask],\n",
    "        y[trainval_mask], y_bin[trainval_mask], rids[trainval_mask],\n",
    "        X[test_mask], rr_b[test_mask], rr_a[test_mask],\n",
    "        y[test_mask], y_bin[test_mask], rids[test_mask],\n",
    "        trainval_ids, test_ids\n",
    "    )\n",
    "\n",
    "(X_tv, rr_b_tv, rr_a_tv, y_tv, y_bin_tv, rids_tv,\n",
    " X_test, rr_b_test, rr_a_test, y_test, y_bin_test, rids_test,\n",
    " trainval_pids, test_pids) = holdout_test_set(\n",
    "    X_beats, rr_before, rr_after, y, y_binary, record_ids\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('PATIENT-WISE DATA SPLIT')\n",
    "print('=' * 60)\n",
    "print(f'Train+Val: {len(X_tv):,} beats from {len(trainval_pids)} patients')\n",
    "print(f'Test:      {len(X_test):,} beats from {len(test_pids)} patients')\n",
    "\n",
    "# Create patient groups for K-Fold\n",
    "pid_to_group = {pid: i for i, pid in enumerate(trainval_pids)}\n",
    "groups_tv = np.array([pid_to_group[rid] for rid in rids_tv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "print('Preprocessing data...')\n",
    "\n",
    "# Reshape ECG beats to (N, T, 1)\n",
    "X_tv_ecg = X_tv.reshape(-1, BEAT_LENGTH, 1)\n",
    "X_test_ecg = X_test.reshape(-1, BEAT_LENGTH, 1)\n",
    "\n",
    "# Stack RR features (N, 2)\n",
    "rr_tv = np.column_stack([rr_b_tv, rr_a_tv])\n",
    "rr_test = np.column_stack([rr_b_test, rr_a_test])\n",
    "\n",
    "# Normalize ECG (per-beat z-score)\n",
    "def normalize_beats(X):\n",
    "    X_norm = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "        beat = X[i, :, 0]\n",
    "        m, s = np.mean(beat), np.std(beat)\n",
    "        X_norm[i, :, 0] = (beat - m) / s if s > 0 else beat - m\n",
    "    return X_norm\n",
    "\n",
    "X_tv_ecg = normalize_beats(X_tv_ecg)\n",
    "X_test_ecg = normalize_beats(X_test_ecg)\n",
    "\n",
    "# Standardize RR features\n",
    "rr_scaler = StandardScaler()\n",
    "rr_tv_scaled = rr_scaler.fit_transform(rr_tv)\n",
    "rr_test_scaled = rr_scaler.transform(rr_test)\n",
    "\n",
    "# Encode binary labels (Stage 1)\n",
    "le_binary = LabelEncoder()\n",
    "le_binary.fit(['Normal', 'Abnormal'])\n",
    "y_tv_bin_enc = le_binary.transform(y_bin_tv)\n",
    "y_test_bin_enc = le_binary.transform(y_bin_test)\n",
    "\n",
    "# Encode multiclass labels (Stage 2)\n",
    "le_multi = LabelEncoder()\n",
    "le_multi.fit(ABNORMAL_CLASSES)\n",
    "NUM_ABN_CLASSES = len(ABNORMAL_CLASSES)\n",
    "\n",
    "print(f'\\n‚úÖ Preprocessing complete!')\n",
    "print(f'X_tv_ecg: {X_tv_ecg.shape}')\n",
    "print(f'rr_tv_scaled: {rr_tv_scaled.shape}')\n",
    "print(f'Binary classes: {le_binary.classes_}')\n",
    "print(f'Abnormal classes: {le_multi.classes_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage1-header",
   "metadata": {},
   "source": [
    "## 3) Stage 1: Normal vs Abnormal (Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage1-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 MODEL DEFINITION\n",
    "# ============================================================\n",
    "\n",
    "def build_stage1_model(ecg_length, rr_features):\n",
    "    \"\"\"\n",
    "    Binary classifier: Normal vs Abnormal\n",
    "    Optimized for HIGH RECALL of abnormal beats.\n",
    "    \"\"\"\n",
    "    # ECG Branch\n",
    "    ecg_input = Input(shape=(ecg_length, 1), name='ecg_input')\n",
    "    x = layers.Conv1D(32, 5, padding='same')(ecg_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(64, 5, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # RR Branch\n",
    "    rr_input = Input(shape=(rr_features,), name='rr_input')\n",
    "    y = layers.Dense(32)(rr_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    y = layers.Dropout(0.2)(y)\n",
    "    y = layers.Dense(32)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    \n",
    "    # Concatenate\n",
    "    combined = layers.Concatenate()([x, y])\n",
    "    z = layers.Dense(64)(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Activation('relu')(z)\n",
    "    z = layers.Dropout(0.3)(z)\n",
    "    \n",
    "    # Output (sigmoid for binary)\n",
    "    output = layers.Dense(1, activation='sigmoid', name='output')(z)\n",
    "    \n",
    "    model = Model(inputs=[ecg_input, rr_input], outputs=output, name='Stage1_Binary')\n",
    "    return model\n",
    "\n",
    "def compile_stage1(model, lr=0.001):\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build and show\n",
    "model_s1 = build_stage1_model(BEAT_LENGTH, 2)\n",
    "model_s1 = compile_stage1(model_s1)\n",
    "model_s1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage1-kfold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 1: K-FOLD CROSS-VALIDATION\n",
    "# ============================================================\n",
    "\n",
    "def clear_session():\n",
    "    keras.backend.clear_session()\n",
    "    tf.random.set_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "s1_results = {'metrics': [], 'histories': []}\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'STAGE 1: BINARY CLASSIFICATION ({N_FOLDS}-FOLD CV)')\n",
    "print('=' * 70)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_tv_ecg, y_tv_bin_enc, groups_tv)):\n",
    "    print(f'\\n--- FOLD {fold+1}/{N_FOLDS} ---')\n",
    "    clear_session()\n",
    "    \n",
    "    # Split\n",
    "    X_tr_ecg, X_vl_ecg = X_tv_ecg[train_idx], X_tv_ecg[val_idx]\n",
    "    rr_tr, rr_vl = rr_tv_scaled[train_idx], rr_tv_scaled[val_idx]\n",
    "    y_tr, y_vl = y_tv_bin_enc[train_idx], y_tv_bin_enc[val_idx]\n",
    "    \n",
    "    # Class weights (favor abnormal recall)\n",
    "    cw = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
    "    # Increase abnormal weight for higher recall\n",
    "    cw_dict = {0: cw[0], 1: cw[1] * 1.5}  # Boost abnormal class\n",
    "    \n",
    "    # Build model\n",
    "    model = build_stage1_model(BEAT_LENGTH, 2)\n",
    "    model = compile_stage1(model)\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        [X_tr_ecg, rr_tr], y_tr,\n",
    "        validation_data=([X_vl_ecg, rr_vl], y_vl),\n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        class_weight=cw_dict,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_prob = model.predict([X_vl_ecg, rr_vl], verbose=0).flatten()\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_vl, y_pred),\n",
    "        'precision': precision_score(y_vl, y_pred, pos_label=1),\n",
    "        'recall': recall_score(y_vl, y_pred, pos_label=1),  # Abnormal recall\n",
    "        'f1': f1_score(y_vl, y_pred, pos_label=1),\n",
    "        'roc_auc': roc_auc_score(y_vl, y_pred_prob)\n",
    "    }\n",
    "    s1_results['metrics'].append(metrics)\n",
    "    s1_results['histories'].append(history.history)\n",
    "    \n",
    "    print(f'  Acc: {metrics[\"accuracy\"]:.4f} | Recall(Abn): {metrics[\"recall\"]:.4f} | F1: {metrics[\"f1\"]:.4f} | AUC: {metrics[\"roc_auc\"]:.4f}')\n",
    "\n",
    "# Summary\n",
    "print('\\n' + '=' * 70)\n",
    "print('STAGE 1 CV SUMMARY')\n",
    "print('=' * 70)\n",
    "for m in ['accuracy', 'recall', 'f1', 'roc_auc']:\n",
    "    vals = [r[m] for r in s1_results['metrics']]\n",
    "    print(f'  {m:12s}: {np.mean(vals):.4f} ¬± {np.std(vals):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage2-header",
   "metadata": {},
   "source": [
    "## 4) Stage 2: Abnormal Classification (Multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage2-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 MODEL DEFINITION\n",
    "# ============================================================\n",
    "\n",
    "def build_stage2_model(ecg_length, rr_features, num_classes):\n",
    "    \"\"\"\n",
    "    Multiclass classifier for abnormal classes (S, V, F, Q)\n",
    "    Optimized for MACRO F1.\n",
    "    \"\"\"\n",
    "    # ECG Branch\n",
    "    ecg_input = Input(shape=(ecg_length, 1), name='ecg_input')\n",
    "    x = layers.Conv1D(32, 5, padding='same')(ecg_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(64, 5, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv1D(256, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # RR Branch\n",
    "    rr_input = Input(shape=(rr_features,), name='rr_input')\n",
    "    y = layers.Dense(32)(rr_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    y = layers.Dropout(0.2)(y)\n",
    "    y = layers.Dense(64)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    \n",
    "    # Concatenate\n",
    "    combined = layers.Concatenate()([x, y])\n",
    "    z = layers.Dense(128)(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Activation('relu')(z)\n",
    "    z = layers.Dropout(0.4)(z)\n",
    "    z = layers.Dense(64)(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Activation('relu')(z)\n",
    "    z = layers.Dropout(0.3)(z)\n",
    "    \n",
    "    # Output (softmax for multiclass)\n",
    "    output = layers.Dense(num_classes, activation='softmax', name='output')(z)\n",
    "    \n",
    "    model = Model(inputs=[ecg_input, rr_input], outputs=output, name='Stage2_Multiclass')\n",
    "    return model\n",
    "\n",
    "def compile_stage2(model, lr=0.001):\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_s2 = build_stage2_model(BEAT_LENGTH, 2, NUM_ABN_CLASSES)\n",
    "model_s2 = compile_stage2(model_s2)\n",
    "model_s2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stage2-kfold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 2: K-FOLD CROSS-VALIDATION (Abnormal beats only)\n",
    "# ============================================================\n",
    "\n",
    "# Filter to abnormal beats only\n",
    "abn_mask_tv = y_tv != 'N'\n",
    "X_tv_abn_ecg = X_tv_ecg[abn_mask_tv]\n",
    "rr_tv_abn = rr_tv_scaled[abn_mask_tv]\n",
    "y_tv_abn = y_tv[abn_mask_tv]\n",
    "groups_tv_abn = groups_tv[abn_mask_tv]\n",
    "\n",
    "# Encode abnormal labels\n",
    "y_tv_abn_enc = le_multi.transform(y_tv_abn)\n",
    "y_tv_abn_onehot = to_categorical(y_tv_abn_enc, num_classes=NUM_ABN_CLASSES)\n",
    "\n",
    "print(f'Stage 2 training data: {len(X_tv_abn_ecg):,} abnormal beats')\n",
    "print(f'Classes: {le_multi.classes_}')\n",
    "\n",
    "s2_results = {'metrics': [], 'histories': []}\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print(f'STAGE 2: ABNORMAL CLASSIFICATION ({N_FOLDS}-FOLD CV)')\n",
    "print('=' * 70)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_tv_abn_ecg, y_tv_abn_enc, groups_tv_abn)):\n",
    "    print(f'\\n--- FOLD {fold+1}/{N_FOLDS} ---')\n",
    "    clear_session()\n",
    "    \n",
    "    # Split\n",
    "    X_tr, X_vl = X_tv_abn_ecg[train_idx], X_tv_abn_ecg[val_idx]\n",
    "    rr_tr, rr_vl = rr_tv_abn[train_idx], rr_tv_abn[val_idx]\n",
    "    y_tr, y_vl = y_tv_abn_onehot[train_idx], y_tv_abn_onehot[val_idx]\n",
    "    y_vl_enc = y_tv_abn_enc[val_idx]\n",
    "    y_tr_enc = y_tv_abn_enc[train_idx]\n",
    "    \n",
    "    # Class weights - handle missing classes\n",
    "    unique_classes = np.unique(y_tr_enc)\n",
    "    cw = compute_class_weight('balanced', classes=unique_classes, y=y_tr_enc)\n",
    "    cw_dict = {c: w for c, w in zip(unique_classes, cw)}\n",
    "    # Fill missing classes with weight 1.0\n",
    "    for c in range(NUM_ABN_CLASSES):\n",
    "        if c not in cw_dict:\n",
    "            cw_dict[c] = 1.0\n",
    "    \n",
    "    # Build model\n",
    "    model = build_stage2_model(BEAT_LENGTH, 2, NUM_ABN_CLASSES)\n",
    "    model = compile_stage2(model)\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        [X_tr, rr_tr], y_tr,\n",
    "        validation_data=([X_vl, rr_vl], y_vl),\n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        class_weight=cw_dict,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate - use explicit labels to ensure consistent array sizes\n",
    "    y_pred_prob = model.predict([X_vl, rr_vl], verbose=0)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    all_labels = list(range(NUM_ABN_CLASSES))  # [0, 1, 2, 3] for S, V, F, Q\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_vl_enc, y_pred),\n",
    "        'macro_f1': f1_score(y_vl_enc, y_pred, average='macro', labels=all_labels, zero_division=0),\n",
    "        'weighted_f1': f1_score(y_vl_enc, y_pred, average='weighted', labels=all_labels, zero_division=0),\n",
    "        'per_class_f1': f1_score(y_vl_enc, y_pred, average=None, labels=all_labels, zero_division=0)\n",
    "    }\n",
    "    s2_results['metrics'].append(metrics)\n",
    "    s2_results['histories'].append(history.history)\n",
    "    \n",
    "    print(f'  Acc: {metrics[\"accuracy\"]:.4f} | Macro F1: {metrics[\"macro_f1\"]:.4f} | Weighted F1: {metrics[\"weighted_f1\"]:.4f}')\n",
    "\n",
    "# Summary\n",
    "print('\\n' + '=' * 70)\n",
    "print('STAGE 2 CV SUMMARY')\n",
    "print('=' * 70)\n",
    "for m in ['accuracy', 'macro_f1', 'weighted_f1']:\n",
    "    vals = [r[m] for r in s2_results['metrics']]\n",
    "    print(f'  {m:12s}: {np.mean(vals):.4f} ¬± {np.std(vals):.4f}')\n",
    "\n",
    "print('\\nPer-class F1 (mean):')\n",
    "pcf1 = np.stack([r['per_class_f1'] for r in s2_results['metrics']])\n",
    "for i, cls in enumerate(le_multi.classes_):\n",
    "    print(f'  {cls} ({AAMI_NAMES[cls]:15s}): {np.mean(pcf1[:, i]):.4f} ¬± {np.std(pcf1[:, i]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-header",
   "metadata": {},
   "source": [
    "## 5) Final Model Training & End-to-End Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('FINAL MODEL TRAINING')\n",
    "print('=' * 70)\n",
    "\n",
    "# --- STAGE 1: Train on full Train+Val ---\n",
    "print('\\n--- Training Final Stage 1 Model ---')\n",
    "clear_session()\n",
    "\n",
    "cw_s1 = compute_class_weight('balanced', classes=np.unique(y_tv_bin_enc), y=y_tv_bin_enc)\n",
    "cw_s1_dict = {0: cw_s1[0], 1: cw_s1[1] * 1.5}\n",
    "\n",
    "final_s1 = build_stage1_model(BEAT_LENGTH, 2)\n",
    "final_s1 = compile_stage1(final_s1)\n",
    "\n",
    "h1 = final_s1.fit(\n",
    "    [X_tv_ecg, rr_tv_scaled], y_tv_bin_enc,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    class_weight=cw_s1_dict,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- STAGE 2: Train on full abnormal subset ---\n",
    "print('\\n--- Training Final Stage 2 Model ---')\n",
    "\n",
    "cw_s2 = compute_class_weight('balanced', classes=np.unique(y_tv_abn_enc), y=y_tv_abn_enc)\n",
    "cw_s2_dict = dict(enumerate(cw_s2))\n",
    "\n",
    "final_s2 = build_stage2_model(BEAT_LENGTH, 2, NUM_ABN_CLASSES)\n",
    "final_s2 = compile_stage2(final_s2)\n",
    "\n",
    "h2 = final_s2.fit(\n",
    "    [X_tv_abn_ecg, rr_tv_abn], y_tv_abn_onehot,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    class_weight=cw_s2_dict,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\n‚úÖ Final models trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# END-TO-END EVALUATION ON HELD-OUT TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('END-TO-END EVALUATION ON TEST SET')\n",
    "print('=' * 70)\n",
    "\n",
    "# Stage 1: Predict Normal vs Abnormal\n",
    "print('\\n--- Stage 1: Binary Classification ---')\n",
    "y_test_s1_prob = final_s1.predict([X_test_ecg, rr_test_scaled], verbose=0).flatten()\n",
    "y_test_s1_pred = (y_test_s1_prob >= 0.5).astype(int)\n",
    "\n",
    "s1_acc = accuracy_score(y_test_bin_enc, y_test_s1_pred)\n",
    "s1_recall = recall_score(y_test_bin_enc, y_test_s1_pred, pos_label=1)\n",
    "s1_prec = precision_score(y_test_bin_enc, y_test_s1_pred, pos_label=1)\n",
    "s1_f1 = f1_score(y_test_bin_enc, y_test_s1_pred, pos_label=1)\n",
    "s1_auc = roc_auc_score(y_test_bin_enc, y_test_s1_prob)\n",
    "\n",
    "print(f'  Accuracy:       {s1_acc:.4f}')\n",
    "print(f'  Recall (Abn):   {s1_recall:.4f}  <- KEY METRIC')\n",
    "print(f'  Precision (Abn):{s1_prec:.4f}')\n",
    "print(f'  F1 (Abn):       {s1_f1:.4f}')\n",
    "print(f'  ROC-AUC:        {s1_auc:.4f}')\n",
    "\n",
    "# Stage 2: Classify abnormal beats\n",
    "print('\\n--- Stage 2: Abnormal Classification ---')\n",
    "\n",
    "# Get beats predicted as abnormal by Stage 1\n",
    "abn_pred_mask = y_test_s1_pred == 1\n",
    "X_test_abn_ecg = X_test_ecg[abn_pred_mask]\n",
    "rr_test_abn = rr_test_scaled[abn_pred_mask]\n",
    "\n",
    "# Get true labels for these beats\n",
    "y_test_true_for_abn = y_test[abn_pred_mask]\n",
    "\n",
    "# Predict Stage 2\n",
    "if len(X_test_abn_ecg) > 0:\n",
    "    y_test_s2_prob = final_s2.predict([X_test_abn_ecg, rr_test_abn], verbose=0)\n",
    "    y_test_s2_pred = np.argmax(y_test_s2_prob, axis=1)\n",
    "    y_test_s2_labels = le_multi.inverse_transform(y_test_s2_pred)\n",
    "else:\n",
    "    y_test_s2_labels = np.array([])\n",
    "\n",
    "# Build final predictions (combining both stages)\n",
    "y_final_pred = np.array(['N'] * len(y_test))\n",
    "y_final_pred[abn_pred_mask] = y_test_s2_labels\n",
    "\n",
    "# Final evaluation\n",
    "print('\\n--- End-to-End Results (All 5 Classes) ---')\n",
    "final_acc = accuracy_score(y_test, y_final_pred)\n",
    "final_macro_f1 = f1_score(y_test, y_final_pred, average='macro', labels=AAMI_CLASSES, zero_division=0)\n",
    "final_weighted_f1 = f1_score(y_test, y_final_pred, average='weighted', labels=AAMI_CLASSES, zero_division=0)\n",
    "final_per_class_f1 = f1_score(y_test, y_final_pred, average=None, labels=AAMI_CLASSES, zero_division=0)\n",
    "\n",
    "print(f'\\n  Accuracy:     {final_acc:.4f}')\n",
    "print(f'  Macro F1:     {final_macro_f1:.4f}  <- PRIMARY METRIC')\n",
    "print(f'  Weighted F1:  {final_weighted_f1:.4f}')\n",
    "\n",
    "print('\\n  Per-Class F1:')\n",
    "for i, cls in enumerate(AAMI_CLASSES):\n",
    "    print(f'    {cls} ({AAMI_NAMES[cls]:15s}): {final_per_class_f1[i]:.4f}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('CLASSIFICATION REPORT')\n",
    "print('=' * 70)\n",
    "print(classification_report(y_test, y_final_pred, labels=AAMI_CLASSES, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 6) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING CURVES\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Stage 1 Loss\n",
    "axes[0, 0].plot(h1.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(h1.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0, 0].set_title('Stage 1: Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Stage 1 Accuracy\n",
    "axes[0, 1].plot(h1.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(h1.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[0, 1].set_title('Stage 1: Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Stage 2 Loss\n",
    "axes[1, 0].plot(h2.history['loss'], label='Train', linewidth=2)\n",
    "axes[1, 0].plot(h2.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[1, 0].set_title('Stage 2: Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Stage 2 Accuracy\n",
    "axes[1, 1].plot(h2.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[1, 1].plot(h2.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[1, 1].set_title('Stage 2: Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'twostage_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFUSION MATRICES\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Stage 1 CM\n",
    "cm1 = confusion_matrix(y_test_bin_enc, y_test_s1_pred)\n",
    "sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Normal', 'Abnormal'], yticklabels=['Normal', 'Abnormal'], ax=axes[0])\n",
    "axes[0].set_title(f'Stage 1: Normal vs Abnormal\\nRecall(Abn): {s1_recall:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Stage 2 CM (only beats predicted as abnormal by Stage 1)\n",
    "# Filter to true abnormal beats that were correctly flagged\n",
    "true_abn_mask = y_test_true_for_abn != 'N'\n",
    "if np.sum(true_abn_mask) > 0:\n",
    "    y_true_s2 = le_multi.transform(y_test_true_for_abn[true_abn_mask])\n",
    "    y_pred_s2 = y_test_s2_pred[true_abn_mask]\n",
    "    cm2 = confusion_matrix(y_true_s2, y_pred_s2, labels=range(NUM_ABN_CLASSES))\n",
    "    sns.heatmap(cm2, annot=True, fmt='d', cmap='Oranges',\n",
    "                xticklabels=le_multi.classes_, yticklabels=le_multi.classes_, ax=axes[1])\n",
    "    axes[1].set_title('Stage 2: Abnormal Classes Only', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('True')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No abnormal beats', ha='center', va='center')\n",
    "    axes[1].set_title('Stage 2')\n",
    "\n",
    "# End-to-End CM\n",
    "le_all = LabelEncoder()\n",
    "le_all.fit(AAMI_CLASSES)\n",
    "y_true_enc = le_all.transform(y_test)\n",
    "y_pred_enc = le_all.transform(y_final_pred)\n",
    "cm_all = confusion_matrix(y_true_enc, y_pred_enc, labels=range(5))\n",
    "cm_all_norm = cm_all.astype('float') / cm_all.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_all_norm, annot=True, fmt='.2%', cmap='Greens',\n",
    "            xticklabels=AAMI_CLASSES, yticklabels=AAMI_CLASSES, ax=axes[2])\n",
    "axes[2].set_title(f'End-to-End (Normalized)\\nMacro F1: {final_macro_f1:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'twostage_confusion_matrices.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-roc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ROC CURVE (Stage 1)\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_bin_enc, y_test_s1_prob)\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'Stage 1 (AUC = {s1_auc:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('Stage 1: ROC Curve (Normal vs Abnormal)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'twostage_roc_curve.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PER-CLASS F1 BAR CHART\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(AAMI_CLASSES))\n",
    "colors = ['#2ecc71' if cls == 'N' else '#e74c3c' for cls in AAMI_CLASSES]\n",
    "\n",
    "bars = ax.bar(x, final_per_class_f1, color=colors, alpha=0.8)\n",
    "\n",
    "for bar, f1 in zip(bars, final_per_class_f1):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{f1:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{c}\\n({AAMI_NAMES[c]})' for c in AAMI_CLASSES])\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title(f'End-to-End Per-Class F1 Scores\\nMacro F1: {final_macro_f1:.4f}', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(y=final_macro_f1, color='black', linestyle='--', linewidth=2, label=f'Macro F1 = {final_macro_f1:.3f}')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'twostage_per_class_f1.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 7) Save Models & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE ARTIFACTS\n",
    "# ============================================================\n",
    "\n",
    "import joblib\n",
    "\n",
    "print('Saving models and artifacts...')\n",
    "\n",
    "final_s1.save(OUTPUT_PATH / 'stage1_model.keras')\n",
    "final_s2.save(OUTPUT_PATH / 'stage2_model.keras')\n",
    "joblib.dump(rr_scaler, OUTPUT_PATH / 'rr_scaler.joblib')\n",
    "joblib.dump(le_binary, OUTPUT_PATH / 'le_binary.joblib')\n",
    "joblib.dump(le_multi, OUTPUT_PATH / 'le_multi.joblib')\n",
    "\n",
    "# Save metrics report\n",
    "report = {\n",
    "    'stage1_cv': {\n",
    "        'accuracy': float(np.mean([r['accuracy'] for r in s1_results['metrics']])),\n",
    "        'recall_abnormal': float(np.mean([r['recall'] for r in s1_results['metrics']])),\n",
    "        'roc_auc': float(np.mean([r['roc_auc'] for r in s1_results['metrics']]))\n",
    "    },\n",
    "    'stage2_cv': {\n",
    "        'accuracy': float(np.mean([r['accuracy'] for r in s2_results['metrics']])),\n",
    "        'macro_f1': float(np.mean([r['macro_f1'] for r in s2_results['metrics']]))\n",
    "    },\n",
    "    'test_results': {\n",
    "        'stage1_recall': float(s1_recall),\n",
    "        'stage1_auc': float(s1_auc),\n",
    "        'final_accuracy': float(final_acc),\n",
    "        'final_macro_f1': float(final_macro_f1),\n",
    "        'final_weighted_f1': float(final_weighted_f1),\n",
    "        'per_class_f1': {c: float(f) for c, f in zip(AAMI_CLASSES, final_per_class_f1)}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH / 'twostage_metrics.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print('\\n‚úÖ All artifacts saved!')\n",
    "print(f'   Location: {OUTPUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('üéØ TWO-STAGE PIPELINE SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'''\n",
    "ARCHITECTURE:\n",
    "  Stage 1: Binary (Normal vs Abnormal) ‚Üí Sigmoid\n",
    "  Stage 2: Multiclass (S, V, F, Q) ‚Üí Softmax\n",
    "  Input: Dual-input CNN (ECG waveform + RR intervals)\n",
    "\n",
    "CROSS-VALIDATION RESULTS ({N_FOLDS}-Fold):\n",
    "  Stage 1 (Binary):\n",
    "    Recall (Abnormal): {np.mean([r[\"recall\"] for r in s1_results[\"metrics\"]]):.4f} ¬± {np.std([r[\"recall\"] for r in s1_results[\"metrics\"]]):.4f}\n",
    "    ROC-AUC:           {np.mean([r[\"roc_auc\"] for r in s1_results[\"metrics\"]]):.4f} ¬± {np.std([r[\"roc_auc\"] for r in s1_results[\"metrics\"]]):.4f}\n",
    "  \n",
    "  Stage 2 (Multiclass):\n",
    "    Macro F1:          {np.mean([r[\"macro_f1\"] for r in s2_results[\"metrics\"]]):.4f} ¬± {np.std([r[\"macro_f1\"] for r in s2_results[\"metrics\"]]):.4f}\n",
    "\n",
    "TEST SET RESULTS:\n",
    "  Stage 1:\n",
    "    Recall (Abnormal): {s1_recall:.4f}  ‚Üê Catches abnormal beats\n",
    "    ROC-AUC:           {s1_auc:.4f}\n",
    "  \n",
    "  End-to-End (All 5 Classes):\n",
    "    Accuracy:          {final_acc:.4f}\n",
    "    Macro F1:          {final_macro_f1:.4f}  ‚Üê PRIMARY METRIC\n",
    "    Weighted F1:       {final_weighted_f1:.4f}\n",
    "\n",
    "PER-CLASS F1 SCORES:''')\n",
    "\n",
    "for i, cls in enumerate(AAMI_CLASSES):\n",
    "    marker = '‚Üê Normal' if cls == 'N' else ''\n",
    "    print(f'    {cls} ({AAMI_NAMES[cls]:15s}): {final_per_class_f1[i]:.4f} {marker}')\n",
    "\n",
    "print(f'''\n",
    "OUTPUT FILES:\n",
    "    {OUTPUT_PATH / 'stage1_model.keras'}\n",
    "    {OUTPUT_PATH / 'stage2_model.keras'}\n",
    "    {OUTPUT_PATH / 'twostage_metrics.json'}\n",
    "    {OUTPUT_PATH / 'twostage_*.png'}\n",
    "\n",
    "‚úÖ Two-stage pipeline complete!\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
