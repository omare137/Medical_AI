{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN Classification of Abnormal ECG Beats (MIT-BIH)\n",
    "\n",
    "**Objective:** Classify ONLY abnormal ECG beats (AAMI classes: S, V, F, Q) using k-Nearest Neighbors.  \n",
    "**Normal beats (N) are EXCLUDED entirely.**\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a0\ufe0f Diagnostic Notebook Disclaimer\n",
    "\n",
    "This notebook is for **diagnostic/research purposes only**:\n",
    "- k-NN serves as a **baseline**, not a production model\n",
    "- Results inform feature engineering and class handling strategy\n",
    "- Ambiguous classes (F, S) should not be overfit\n",
    "- MIT-BIH has known limitations (small, 1980s recordings)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "| Section | Description |\n",
    "|---------|-------------|\n",
    "| 1. Data Loading | Load features, filter abnormal only, verify N excluded |\n",
    "| 2. Record-wise Split | Patient-level 80/20 split (no leakage) |\n",
    "| 3. Preprocessing | StandardScaler + PCA (fit on train only) |\n",
    "| 4. k-NN Modeling | Evaluate k \u2208 {3, 5, 7, 9} with confidence analysis |\n",
    "| 5. Results Analysis | Metrics table, class difficulty, strategy insights |\n",
    "| 6. Visualization | Confusion matrix, t-SNE, S/F overlap analysis |\n",
    "| 7. Summary | Findings, limitations, next steps |\n",
    "\n",
    "---\n",
    "\n",
    "## AAMI Abnormal Classes\n",
    "\n",
    "| Code | Name | Description | Expected Difficulty |\n",
    "|------|------|-------------|---------------------|\n",
    "| **S** | Supraventricular | Atrial/junctional premature beats | Medium-Hard (RR-dependent) |\n",
    "| **V** | Ventricular | Ventricular ectopic beats | Easy (distinct morphology) |\n",
    "| **F** | Fusion | Fusion of normal and ventricular | Very Hard (rare, ambiguous) |\n",
    "| **Q** | Unknown | Paced/unclassifiable beats | Medium (pacing artifacts) |\n",
    "\n",
    "---\n",
    "\n",
    "## Why k-NN as a Baseline?\n",
    "\n",
    "- **Simple & interpretable** \u2014 easy to debug and understand\n",
    "- **Distance-based** \u2014 naturally works with engineered features\n",
    "- **Fast iteration** \u2014 no training time, quick experiments\n",
    "- **Confidence via neighbors** \u2014 distance to neighbors indicates uncertainty\n",
    "\n",
    "## Why Macro F1?\n",
    "\n",
    "- **Accuracy is misleading** with imbalanced classes\n",
    "- **Macro F1 treats all classes equally** \u2014 rare classes matter\n",
    "- **Clinical relevance** \u2014 detecting ALL arrhythmia types is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# GOOGLE COLAB SETUP (skip if running locally)\n# ============================================================\n\n# Uncomment to mount Google Drive\n# from google.colab import drive\n# drive.mount('/content/drive')\n\n# Install packages if needed\n# !pip install -q wfdb\n\nprint('\u2705 Setup complete!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# IMPORTS\n# ============================================================\n\nimport os\nimport warnings\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, classification_report,\n    confusion_matrix, ConfusionMatrixDisplay\n)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Settings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('husl')\n\n# Reproducibility\nSEED = 42\nnp.random.seed(SEED)\n\nprint('\u2705 All imports successful!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Paths - UPDATE FOR YOUR ENVIRONMENT\n",
    "# The MIT-BIH data is in the 'mit-bih-arrhythmia-database-1.0.0' subdirectory\n",
    "\n",
    "# For Colab:\n",
    "BASE_PATH = Path('/content/drive/MyDrive/ecg2.0')\n",
    "DATA_PATH = BASE_PATH / 'mit-bih-arrhythmia-database-1.0.0'\n",
    "\n",
    "# For local (uncomment if running locally):\n",
    "# BASE_PATH = Path('/Volumes/Crucial X6/medical_ai/ecg2.0')\n",
    "# DATA_PATH = BASE_PATH / 'mit-bih-arrhythmia-database-1.0.0'\n",
    "\n",
    "OUTPUT_PATH = BASE_PATH / 'outputs_knn'\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify data path exists\n",
    "if DATA_PATH.exists():\n",
    "    print(f'\u2705 Data path found: {DATA_PATH}')\n",
    "    hea_files = list(DATA_PATH.glob('*.hea'))\n",
    "    print(f'   Found {len(hea_files)} .hea files')\n",
    "else:\n",
    "    print(f'\u274c Data path NOT found: {DATA_PATH}')\n",
    "    print('   Please update DATA_PATH to point to your MIT-BIH data folder')\n",
    "\n",
    "print(f'Output path: {OUTPUT_PATH}')\n",
    "\n",
    "# AAMI class definitions\n",
    "AAMI_MAP = {\n",
    "    'N': 'N', 'L': 'N', 'R': 'N', 'e': 'N', 'j': 'N',  # Normal\n",
    "    'A': 'S', 'a': 'S', 'J': 'S', 'S': 'S',             # Supraventricular\n",
    "    'V': 'V', 'E': 'V',                                  # Ventricular\n",
    "    'F': 'F',                                            # Fusion\n",
    "    '/': 'Q', 'f': 'Q', '!': 'Q', 'Q': 'Q', 'P': 'Q'    # Unknown/Paced\n",
    "}\n",
    "\n",
    "# ABNORMAL CLASSES ONLY (excluding Normal)\n",
    "ABNORMAL_CLASSES = ['S', 'V', 'F', 'Q']\n",
    "CLASS_NAMES = {\n",
    "    'S': 'Supraventricular',\n",
    "    'V': 'Ventricular', \n",
    "    'F': 'Fusion',\n",
    "    'Q': 'Unknown/Paced'\n",
    "}\n",
    "\n",
    "# k-NN parameters to evaluate\n",
    "K_VALUES = [3, 5, 7, 9]\n",
    "\n",
    "# PCA variance threshold\n",
    "PCA_VARIANCE = 0.95\n",
    "\n",
    "print(f'Abnormal classes: {ABNORMAL_CLASSES}')\n",
    "print(f'k values to test: {K_VALUES}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Loading & Filtering\n\nLoad the preprocessed beat-level feature dataset and filter to **abnormal beats only**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# DATA LOADING - Feature Extraction Function\n",
    "# ============================================================\n",
    "\n",
    "import wfdb\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Beat extraction parameters\n",
    "SAMPLES_BEFORE = 100\n",
    "SAMPLES_AFTER = 150\n",
    "BEAT_LENGTH = SAMPLES_BEFORE + SAMPLES_AFTER\n",
    "\n",
    "def extract_beat_features(beat, rr_before, rr_after, local_rr_mean=None, fs=360):\n",
    "    \"\"\"\n",
    "    Extract features from a single ECG beat.\n",
    "    \n",
    "    IMPORTANT: RR-heavy features are critical for detecting S (Supraventricular),\n",
    "    as these beats are characterized by premature timing (short RR_before).\n",
    "    \n",
    "    Returns a dictionary of feature values.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # --- Statistical features ---\n",
    "    features['mean'] = np.mean(beat)\n",
    "    features['std'] = np.std(beat)\n",
    "    features['min'] = np.min(beat)\n",
    "    features['max'] = np.max(beat)\n",
    "    features['ptp'] = np.ptp(beat)  # peak-to-peak\n",
    "    features['median'] = np.median(beat)\n",
    "    features['skewness'] = skew(beat)\n",
    "    features['kurtosis'] = kurtosis(beat)\n",
    "    \n",
    "    # --- Energy features ---\n",
    "    features['energy'] = np.sum(beat ** 2)\n",
    "    features['rms'] = np.sqrt(np.mean(beat ** 2))\n",
    "    \n",
    "    # ============================================================\n",
    "    # RR INTERVAL FEATURES (CRITICAL FOR S-CLASS DETECTION)\n",
    "    # ============================================================\n",
    "    # Supraventricular (S) beats are premature \u2192 short RR_before\n",
    "    # These features help distinguish S from other classes\n",
    "    \n",
    "    features['rr_before'] = rr_before\n",
    "    features['rr_after'] = rr_after\n",
    "    features['rr_ratio'] = rr_before / rr_after if rr_after > 0 else 1.0\n",
    "    features['rr_diff'] = rr_after - rr_before\n",
    "    features['rr_mean'] = (rr_before + rr_after) / 2\n",
    "    \n",
    "    # Additional RR features for S detection\n",
    "    features['rr_before_sq'] = rr_before ** 2  # Emphasize short intervals\n",
    "    features['rr_ratio_inv'] = rr_after / rr_before if rr_before > 0 else 1.0\n",
    "    features['rr_abs_diff'] = abs(rr_after - rr_before)\n",
    "    features['rr_product'] = rr_before * rr_after\n",
    "    \n",
    "    # Prematurity index (key for S): how much shorter is this beat?\n",
    "    if local_rr_mean and local_rr_mean > 0:\n",
    "        features['prematurity_index'] = (local_rr_mean - rr_before) / local_rr_mean\n",
    "        features['compensatory_pause'] = (rr_after - local_rr_mean) / local_rr_mean\n",
    "    else:\n",
    "        features['prematurity_index'] = 0.0\n",
    "        features['compensatory_pause'] = 0.0\n",
    "    \n",
    "    # --- Morphological features ---\n",
    "    r_peak_idx = SAMPLES_BEFORE\n",
    "    features['r_amplitude'] = beat[r_peak_idx]\n",
    "    \n",
    "    # QRS approximation (central region)\n",
    "    qrs_start = r_peak_idx - 20\n",
    "    qrs_end = r_peak_idx + 20\n",
    "    qrs_region = beat[max(0, qrs_start):min(len(beat), qrs_end)]\n",
    "    features['qrs_energy'] = np.sum(qrs_region ** 2)\n",
    "    features['qrs_max'] = np.max(qrs_region)\n",
    "    features['qrs_min'] = np.min(qrs_region)\n",
    "    features['qrs_width_approx'] = np.sum(np.abs(qrs_region) > 0.3 * np.max(np.abs(qrs_region)))\n",
    "    \n",
    "    # --- Derivative features ---\n",
    "    deriv = np.diff(beat)\n",
    "    features['deriv_max'] = np.max(deriv)\n",
    "    features['deriv_min'] = np.min(deriv)\n",
    "    features['deriv_std'] = np.std(deriv)\n",
    "    features['deriv_abs_mean'] = np.mean(np.abs(deriv))\n",
    "    \n",
    "    # Zero crossings (complexity measure)\n",
    "    features['zero_crossings'] = np.sum(np.diff(np.sign(beat)) != 0)\n",
    "    \n",
    "    # --- Segment features (for V vs F distinction) ---\n",
    "    pre_qrs = beat[:qrs_start] if qrs_start > 0 else beat[:10]\n",
    "    post_qrs = beat[qrs_end:] if qrs_end < len(beat) else beat[-10:]\n",
    "    features['pre_qrs_mean'] = np.mean(pre_qrs)\n",
    "    features['post_qrs_mean'] = np.mean(post_qrs)\n",
    "    features['pre_post_ratio'] = features['pre_qrs_mean'] / (features['post_qrs_mean'] + 1e-6)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print('Feature extraction function defined.')\n",
    "print(f'Beat length: {BEAT_LENGTH} samples ({SAMPLES_BEFORE} before, {SAMPLES_AFTER} after R-peak)')\n",
    "print('\\n\ud83d\udcca RR-heavy features added for S-class detection:')\n",
    "print('   - prematurity_index: how early the beat occurs')\n",
    "print('   - compensatory_pause: pause after premature beat')\n",
    "print('   - rr_before_sq, rr_ratio_inv, rr_abs_diff, rr_product')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# LOAD MIT-BIH DATA AND EXTRACT FEATURES\n",
    "# ============================================================\n",
    "\n",
    "# MIT-BIH record numbers\n",
    "MIT_BIH_RECORDS = [\n",
    "    100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
    "    111, 112, 113, 114, 115, 116, 117, 118, 119, 121,\n",
    "    122, 123, 124, 200, 201, 202, 203, 205, 207, 208,\n",
    "    209, 210, 212, 213, 214, 215, 217, 219, 220, 221,\n",
    "    222, 223, 228, 230, 231, 232, 233, 234\n",
    "]\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_record_ids = []\n",
    "skipped_normal = 0\n",
    "\n",
    "print('Loading MIT-BIH records and extracting features...')\n",
    "print(f'Data path: {DATA_PATH}')\n",
    "print('=' * 60)\n",
    "\n",
    "loaded_count = 0\n",
    "for rec_num in MIT_BIH_RECORDS:\n",
    "    rec_path = DATA_PATH / str(rec_num)\n",
    "    \n",
    "    try:\n",
    "        # Load record\n",
    "        record = wfdb.rdrecord(str(rec_path))\n",
    "        annotation = wfdb.rdann(str(rec_path), 'atr')\n",
    "        \n",
    "        signal = record.p_signal[:, 0]  # Lead 0\n",
    "        fs = record.fs\n",
    "        \n",
    "        r_peaks = annotation.sample\n",
    "        symbols = annotation.symbol\n",
    "        \n",
    "        # Calculate local RR mean for the record (for prematurity index)\n",
    "        rr_intervals = np.diff(r_peaks) / fs\n",
    "        local_rr_mean = np.median(rr_intervals) if len(rr_intervals) > 0 else 0.8\n",
    "        \n",
    "        rec_abnormal_count = 0\n",
    "        rec_normal_skipped = 0\n",
    "        \n",
    "        for i, (r_peak, symbol) in enumerate(zip(r_peaks, symbols)):\n",
    "            if symbol not in AAMI_MAP:\n",
    "                continue\n",
    "            \n",
    "            aami_label = AAMI_MAP[symbol]\n",
    "            \n",
    "            # EXPLICITLY SKIP Normal beats - ABNORMAL ONLY\n",
    "            if aami_label == 'N':\n",
    "                rec_normal_skipped += 1\n",
    "                skipped_normal += 1\n",
    "                continue\n",
    "            \n",
    "            # Extract beat window\n",
    "            start = r_peak - SAMPLES_BEFORE\n",
    "            end = r_peak + SAMPLES_AFTER\n",
    "            \n",
    "            if start < 0 or end > len(signal):\n",
    "                continue\n",
    "            \n",
    "            beat = signal[start:end]\n",
    "            \n",
    "            # Calculate RR intervals\n",
    "            rr_before = (r_peak - r_peaks[i-1]) / fs if i > 0 else local_rr_mean\n",
    "            rr_after = (r_peaks[i+1] - r_peak) / fs if i < len(r_peaks)-1 else local_rr_mean\n",
    "            \n",
    "            # Extract features with local RR mean for prematurity calculation\n",
    "            feat = extract_beat_features(beat, rr_before, rr_after, local_rr_mean, fs)\n",
    "            \n",
    "            all_features.append(feat)\n",
    "            all_labels.append(aami_label)\n",
    "            all_record_ids.append(rec_num)\n",
    "            rec_abnormal_count += 1\n",
    "        \n",
    "        if rec_abnormal_count > 0:\n",
    "            print(f'  Record {rec_num}: {rec_abnormal_count} abnormal (skipped {rec_normal_skipped} normal)')\n",
    "            loaded_count += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'  Warning: Could not load record {rec_num}: {e}')\n",
    "        continue\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_features)\n",
    "df['label'] = all_labels\n",
    "df['record_id'] = all_record_ids\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print(f'\u2705 Loaded {len(df):,} ABNORMAL beats from {loaded_count} records')\n",
    "print(f'   Skipped {skipped_normal:,} Normal (N) beats')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# VERIFICATION: CONFIRM N IS EXCLUDED\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('SANITY CHECK: Abnormal-Only Verification')\n",
    "print('=' * 60)\n",
    "\n",
    "# Assert N is fully excluded\n",
    "n_count = (df['label'] == 'N').sum()\n",
    "assert n_count == 0, f'ERROR: Found {n_count} Normal beats! N should be excluded.'\n",
    "print(f'\\n\u2705 VERIFIED: 0 Normal (N) beats in dataset')\n",
    "print(f'   Total beats: {len(df):,}')\n",
    "print(f'   All labels: {df[\"label\"].unique()}')\n",
    "\n",
    "# Class distribution\n",
    "print('\\n' + '=' * 60)\n",
    "print('CLASS DISTRIBUTION (Abnormal Beats Only)')\n",
    "print('=' * 60)\n",
    "\n",
    "class_counts = df['label'].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "print(f'\\nTotal abnormal beats: {total:,}\\n')\n",
    "print(f'{\"Class\":<5} {\"Name\":<20} {\"Count\":>8} {\"Percentage\":>10}')\n",
    "print('-' * 50)\n",
    "\n",
    "for cls in ABNORMAL_CLASSES:\n",
    "    if cls in class_counts.index:\n",
    "        count = class_counts[cls]\n",
    "        pct = 100 * count / total\n",
    "        print(f'{cls:<5} {CLASS_NAMES[cls]:<20} {count:>8,} {pct:>9.2f}%')\n",
    "\n",
    "print('-' * 50)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = ['#e74c3c', '#3498db', '#9b59b6', '#f39c12']\n",
    "bars = ax.bar(ABNORMAL_CLASSES, [class_counts.get(c, 0) for c in ABNORMAL_CLASSES], color=colors)\n",
    "ax.set_xlabel('AAMI Class')\n",
    "ax.set_ylabel('Number of Beats')\n",
    "ax.set_title('Abnormal Beat Distribution (N EXCLUDED)', fontweight='bold')\n",
    "\n",
    "for bar, cls in zip(bars, ABNORMAL_CLASSES):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:,}\\n({100*height/total:.1f}%)',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u26a0\ufe0f CLASS IMBALANCE NOTE:')\n",
    "print('   V dominates (~70%), F is very rare (<1%)')\n",
    "print('   This explains why high accuracy \u2260 good Macro F1')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Record-Wise Train/Test Split\n",
    "\n",
    "### \u26a0\ufe0f Why Record-Wise Splitting is MANDATORY for MIT-BIH\n",
    "\n",
    "**Beat-level splitting causes data leakage:**\n",
    "- Beats from the **same patient** are highly correlated\n",
    "- Same heart \u2192 same morphology, rhythm, baseline wander\n",
    "- Model learns **patient-specific patterns**, not generalizable arrhythmia features\n",
    "- Result: **Inflated metrics** that don't reflect real-world performance\n",
    "\n",
    "**Record-wise splitting prevents leakage:**\n",
    "- Each patient's beats go entirely to train OR test\n",
    "- Model must generalize to **unseen patients**\n",
    "- This is the **clinically relevant** evaluation scenario"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# RECORD-WISE TRAIN / TEST SPLIT (80% / 20%)\n",
    "# ============================================================\n",
    "\n",
    "# Get unique records\n",
    "unique_records = df['record_id'].unique()\n",
    "n_records = len(unique_records)\n",
    "\n",
    "print(f'Total unique records with abnormal beats: {n_records}')\n",
    "\n",
    "# Split records (NOT beats) - 80/20\n",
    "train_records, test_records = train_test_split(\n",
    "    unique_records,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f'\\nSplit ratio: 80% train / 20% test')\n",
    "print(f'Train records: {len(train_records)} ({100*len(train_records)/n_records:.1f}%)')\n",
    "print(f'Test records:  {len(test_records)} ({100*len(test_records)/n_records:.1f}%)')\n",
    "\n",
    "# ============================================================\n",
    "# EXPLICIT VERIFICATION: Zero overlap\n",
    "# ============================================================\n",
    "train_set = set(train_records)\n",
    "test_set = set(test_records)\n",
    "overlap = train_set & test_set\n",
    "\n",
    "print(f'\\n\ud83d\udd0d LEAKAGE CHECK:')\n",
    "print(f'   Train record IDs: {sorted(train_records)[:5]}... (showing first 5)')\n",
    "print(f'   Test record IDs:  {sorted(test_records)[:5]}... (showing first 5)')\n",
    "print(f'   Overlap: {len(overlap)} records')\n",
    "\n",
    "assert len(overlap) == 0, f'ERROR: {len(overlap)} records appear in both splits!'\n",
    "print(f'\\n\u2705 VERIFIED: Zero overlap between train and test records')\n",
    "print('   No patient data leakage.')\n",
    "\n",
    "# Split data by records\n",
    "train_mask = df['record_id'].isin(train_records)\n",
    "test_mask = df['record_id'].isin(test_records)\n",
    "\n",
    "df_train = df[train_mask].copy()\n",
    "df_test = df[test_mask].copy()\n",
    "\n",
    "print(f'\\nTrain beats: {len(df_train):,}')\n",
    "print(f'Test beats:  {len(df_test):,}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# VERIFY CLASS DISTRIBUTION IN SPLITS\n# ============================================================\n\nprint('\\n' + '=' * 60)\nprint('CLASS DISTRIBUTION BY SPLIT')\nprint('=' * 60)\n\nprint('\\n--- TRAIN SET ---')\nfor cls in ABNORMAL_CLASSES:\n    c = (df_train['label'] == cls).sum()\n    pct = 100 * c / len(df_train)\n    print(f'  {cls}: {c:>6,} ({pct:>5.2f}%)')\n\nprint('\\n--- TEST SET ---')\nfor cls in ABNORMAL_CLASSES:\n    c = (df_test['label'] == cls).sum()\n    pct = 100 * c / len(df_test)\n    print(f'  {cls}: {c:>6,} ({pct:>5.2f}%)')\n\nprint('\\n\u2705 Record-wise split complete. No patient data leakage.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Feature Preprocessing\n",
    "\n",
    "### Why Scaling is REQUIRED for k-NN\n",
    "\n",
    "k-NN uses **Euclidean distance** \u2014 features with larger scales dominate:\n",
    "- `energy` might be ~10,000\n",
    "- `rr_ratio` might be ~1.0\n",
    "- Without scaling, `energy` completely dominates the distance calculation\n",
    "\n",
    "**StandardScaler** normalizes all features to mean=0, std=1.\n",
    "\n",
    "### Why PCA Can Help\n",
    "\n",
    "- Removes correlated features (redundancy)\n",
    "- Reduces dimensionality \u2192 faster k-NN\n",
    "- Mitigates **curse of dimensionality** in high-D spaces\n",
    "\n",
    "### \u26a0\ufe0f Critical: Fit on Train ONLY\n",
    "\n",
    "- **StandardScaler**: fit on X_train, transform both\n",
    "- **PCA**: fit on scaled X_train, transform both\n",
    "- Never fit on test data \u2014 that's **data leakage**!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PREPARE FEATURE MATRICES\n# ============================================================\n\n# Get feature columns (exclude label and record_id)\nfeature_cols = [col for col in df.columns if col not in ['label', 'record_id']]\n\nprint(f'Number of features: {len(feature_cols)}')\nprint(f'Features: {feature_cols[:10]}...')\n\n# Extract X, y\nX_train = df_train[feature_cols].values\ny_train = df_train['label'].values\n\nX_test = df_test[feature_cols].values\ny_test = df_test['label'].values\n\nprint(f'\\nX_train shape: {X_train.shape}')\nprint(f'X_test shape:  {X_test.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# STANDARDSCALER (FIT ON TRAIN ONLY)\n# ============================================================\n\nscaler = StandardScaler()\n\n# Fit ONLY on training data\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Transform test data (do NOT fit)\nX_test_scaled = scaler.transform(X_test)\n\nprint('\u2705 StandardScaler applied')\nprint('   - Fitted on training data only')\nprint('   - Test data transformed (not fitted)')\nprint(f'\\n   Train mean after scaling: {X_train_scaled.mean():.6f}')\nprint(f'   Train std after scaling:  {X_train_scaled.std():.6f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PCA (FIT ON TRAIN ONLY, RETAIN ~95% VARIANCE)\n# ============================================================\n\n# Fit PCA on scaled training data ONLY\npca = PCA(n_components=PCA_VARIANCE, random_state=SEED)\nX_train_pca = pca.fit_transform(X_train_scaled)\n\n# Transform test data (do NOT fit)\nX_test_pca = pca.transform(X_test_scaled)\n\nn_components = pca.n_components_\nexplained_var = np.sum(pca.explained_variance_ratio_)\n\nprint('\u2705 PCA applied')\nprint('   - Fitted on training data only')\nprint(f'\\n   Original features:    {X_train_scaled.shape[1]}')\nprint(f'   PCA components:       {n_components}')\nprint(f'   Variance explained:   {100*explained_var:.2f}%')\n\nprint(f'\\n   X_train_pca shape: {X_train_pca.shape}')\nprint(f'   X_test_pca shape:  {X_test_pca.shape}')\n\n# Visualize explained variance\nfig, ax = plt.subplots(figsize=(10, 4))\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nax.bar(range(1, n_components+1), pca.explained_variance_ratio_, alpha=0.7, label='Individual')\nax.plot(range(1, n_components+1), cumsum, 'ro-', label='Cumulative')\nax.axhline(y=PCA_VARIANCE, color='g', linestyle='--', label=f'{100*PCA_VARIANCE}% threshold')\nax.set_xlabel('Principal Component')\nax.set_ylabel('Explained Variance Ratio')\nax.set_title('PCA Explained Variance', fontweight='bold')\nax.legend()\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# ENCODE LABELS\n# ============================================================\n\nle = LabelEncoder()\nle.fit(ABNORMAL_CLASSES)\n\ny_train_enc = le.transform(y_train)\ny_test_enc = le.transform(y_test)\n\nprint('\u2705 Labels encoded')\nprint(f'   Classes: {le.classes_}')\nprint(f'   Encoding: {dict(zip(le.classes_, range(len(le.classes_))))}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) k-NN Modeling\n\nTrain k-NN classifiers with different values of k.\n\n**Parameters:**\n- k \u2208 {3, 5, 7, 9}\n- Distance metric: Euclidean\n- Weights: distance-based (closer neighbors have more influence)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# k-NN TRAINING AND EVALUATION (with Confidence Analysis)\n",
    "# ============================================================\n",
    "\n",
    "results = []\n",
    "\n",
    "print('=' * 70)\n",
    "print('k-NN CLASSIFICATION RESULTS')\n",
    "print('=' * 70)\n",
    "\n",
    "for k in K_VALUES:\n",
    "    print(f'\\n--- k = {k} ---')\n",
    "    \n",
    "    # Create and train k-NN\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=k,\n",
    "        metric='euclidean',\n",
    "        weights='distance',  # Distance-weighted voting\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    knn.fit(X_train_pca, y_train_enc)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = knn.predict(X_test_pca)\n",
    "    y_proba = knn.predict_proba(X_test_pca)\n",
    "    \n",
    "    # Calculate confidence (max probability)\n",
    "    confidence = np.max(y_proba, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_enc, y_pred)\n",
    "    macro_f1 = f1_score(y_test_enc, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_test_enc, y_pred, average='weighted')\n",
    "    per_class_f1 = f1_score(y_test_enc, y_pred, average=None, labels=range(len(ABNORMAL_CLASSES)))\n",
    "    \n",
    "    # ============================================================\n",
    "    # CONFIDENCE / REJECT ANALYSIS\n",
    "    # ============================================================\n",
    "    low_conf_mask = confidence < 0.5  # Flag predictions with <50% confidence\n",
    "    low_conf_count = np.sum(low_conf_mask)\n",
    "    low_conf_pct = 100 * low_conf_count / len(y_pred)\n",
    "    \n",
    "    # Accuracy on high-confidence predictions only\n",
    "    if np.sum(~low_conf_mask) > 0:\n",
    "        high_conf_acc = accuracy_score(y_test_enc[~low_conf_mask], y_pred[~low_conf_mask])\n",
    "    else:\n",
    "        high_conf_acc = 0.0\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'k': k,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba,\n",
    "        'confidence': confidence,\n",
    "        'low_conf_pct': low_conf_pct,\n",
    "        'high_conf_acc': high_conf_acc,\n",
    "        'knn_model': knn\n",
    "    }\n",
    "    for i, cls in enumerate(ABNORMAL_CLASSES):\n",
    "        result[f'f1_{cls}'] = per_class_f1[i]\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f'  Accuracy:   {accuracy:.4f}')\n",
    "    print(f'  Macro F1:   {macro_f1:.4f} (PRIMARY)')\n",
    "    print(f'  Per-class:  S={per_class_f1[0]:.3f}, V={per_class_f1[1]:.3f}, F={per_class_f1[2]:.3f}, Q={per_class_f1[3]:.3f}')\n",
    "    print(f'  Low-conf:   {low_conf_pct:.1f}% predictions < 50% confidence')\n",
    "    print(f'  High-conf accuracy: {high_conf_acc:.4f} (if rejecting low-conf)')\n",
    "\n",
    "print('\\n' + '=' * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Results Analysis\n\nCompare all k values and identify the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# RESULTS COMPARISON TABLE\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('k-NN RESULTS COMPARISON')\n",
    "print('=' * 100)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_results = pd.DataFrame([{\n",
    "    'k': r['k'],\n",
    "    'Accuracy': f\"{r['accuracy']:.4f}\",\n",
    "    'Macro F1': f\"{r['macro_f1']:.4f}\",\n",
    "    'F1(S)': f\"{r['f1_S']:.4f}\",\n",
    "    'F1(V)': f\"{r['f1_V']:.4f}\",\n",
    "    'F1(F)': f\"{r['f1_F']:.4f}\",\n",
    "    'F1(Q)': f\"{r['f1_Q']:.4f}\",\n",
    "    'Low-Conf %': f\"{r['low_conf_pct']:.1f}%\"\n",
    "} for r in results])\n",
    "\n",
    "print('\\n' + df_results.to_string(index=False))\n",
    "\n",
    "# Find best k based on Macro F1\n",
    "best_result = max(results, key=lambda x: x['macro_f1'])\n",
    "best_k = best_result['k']\n",
    "\n",
    "print(f'\\n\ud83c\udfc6 BEST k = {best_k} (Macro F1 = {best_result[\"macro_f1\"]:.4f})')\n",
    "\n",
    "# ============================================================\n",
    "# ACCURACY vs MACRO F1 ANALYSIS\n",
    "# ============================================================\n",
    "print('\\n' + '=' * 100)\n",
    "print('WHY ACCURACY CAN BE HIGH BUT MACRO F1 LOW')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'''\n",
    "Example from our results:\n",
    "  Accuracy:  {best_result['accuracy']:.4f}\n",
    "  Macro F1:  {best_result['macro_f1']:.4f}\n",
    "\n",
    "The gap exists because:\n",
    "  \u2022 V class dominates (~70% of data) \u2192 predicting V is often correct\n",
    "  \u2022 But F class is tiny (~1%) \u2192 low F1(F) barely affects accuracy\n",
    "  \u2022 Macro F1 treats F equally to V \u2192 penalizes poor F performance\n",
    "\n",
    "CLINICAL IMPLICATION:\n",
    "  A model with 95% accuracy but 0% F-recall would miss ALL fusion beats.\n",
    "  Macro F1 catches this; accuracy does not.\n",
    "''')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# DETAILED CLASSIFICATION REPORT (BEST k)\n# ============================================================\n\nprint(f'\\n' + '=' * 70)\nprint(f'CLASSIFICATION REPORT (k = {best_k})')\nprint('=' * 70)\n\ny_pred_best = best_result['y_pred']\n\nprint(classification_report(\n    y_test_enc, y_pred_best,\n    target_names=ABNORMAL_CLASSES,\n    digits=4\n))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# PER-CLASS ANALYSIS & PRODUCT STRATEGY\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('PER-CLASS PERFORMANCE ANALYSIS')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'''\n",
    "\ud83d\udcca CLASS-BY-CLASS BREAKDOWN:\n",
    "\n",
    "  S (Supraventricular): F1 = {best_result['f1_S']:.4f}\n",
    "    \u2022 Atrial/junctional premature beats\n",
    "    \u2022 DEPENDS HEAVILY ON RR FEATURES (prematurity index)\n",
    "    \u2022 Often confused with Normal (morphology similar)\n",
    "    \u2022 RR-heavy features should improve this\n",
    "    \n",
    "  V (Ventricular): F1 = {best_result['f1_V']:.4f}\n",
    "    \u2022 Ventricular ectopic beats (PVCs)\n",
    "    \u2022 MOST DISTINCTIVE morphology (wide QRS, different axis)\n",
    "    \u2022 Expected to have highest F1\n",
    "    \u2022 Easy wins here.\n",
    "\n",
    "  F (Fusion): F1 = {best_result['f1_F']:.4f}\n",
    "    \u2022 Fusion of normal and ventricular beats\n",
    "    \u2022 RAREST and HARDEST class\n",
    "    \u2022 Mixed morphology \u2192 inherently ambiguous\n",
    "    \u2022 Even cardiologists disagree on F labels\n",
    "    \u2022 Consider: confidence thresholding or merging with V\n",
    "    \n",
    "  Q (Unknown/Paced): F1 = {best_result['f1_Q']:.4f}\n",
    "    \u2022 Paced or unclassifiable beats\n",
    "    \u2022 Distinct pacing spikes help detection\n",
    "    \u2022 Moderate difficulty\n",
    "''')\n",
    "\n",
    "print('\\n\ud83d\udcc8 EXPECTED DIFFICULTY RANKING (easiest \u2192 hardest):')\n",
    "print('    V  >  Q  >  S  >  F')\n",
    "print('    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518')\n",
    "print('    Easy      Medium    Hard')\n",
    "\n",
    "# Verify ranking matches results\n",
    "ranking = sorted(ABNORMAL_CLASSES, key=lambda c: best_result[f'f1_{c}'], reverse=True)\n",
    "print(f'\\n\ud83d\udcca ACTUAL RANKING (from our results): {\" > \".join(ranking)}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('PRODUCT STRATEGY INSIGHTS')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'''\n",
    "\ud83c\udfaf WHAT TO KEEP SEPARATE vs MERGE:\n",
    "\n",
    "  KEEP SEPARATE (distinct clinical meaning):\n",
    "    \u2022 V (Ventricular) - critical, high-risk arrhythmia\n",
    "    \u2022 S (Supraventricular) - different treatment path\n",
    "    \n",
    "  CONSIDER MERGING:\n",
    "    \u2022 F (Fusion) \u2192 merge with V for product?\n",
    "      - Clinically, F is a subtype of ventricular activity\n",
    "      - Very rare, hard to learn, often mislabeled\n",
    "      - Merging would boost V-class performance\n",
    "    \n",
    "  Q (Unknown/Paced) - PRODUCT DECISION:\n",
    "    \u2022 Option A: Flag separately (\"paced rhythm detected\")\n",
    "    \u2022 Option B: Exclude from main classification\n",
    "    \u2022 Depends on target use case\n",
    "\n",
    "\ud83d\udd27 CONFIDENCE/REJECT STRATEGY:\n",
    "\n",
    "  For ambiguous classes (S, F):\n",
    "    \u2022 Low-confidence predictions ({best_result['low_conf_pct']:.1f}% of test set)\n",
    "    \u2022 Option: Flag for human review instead of hard prediction\n",
    "    \u2022 Improves precision at cost of coverage\n",
    "''')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Discussion\n",
    "\n",
    "### Why Macro F1 is the Correct Metric Here\n",
    "\n",
    "| Metric | What it measures | Problem with imbalanced data |\n",
    "|--------|------------------|-----------------------------|\n",
    "| **Accuracy** | Overall correct predictions | Dominated by majority class (V) |\n",
    "| **Weighted F1** | F1 weighted by class support | Still favors majority class |\n",
    "| **Macro F1** | Unweighted average of per-class F1 | Treats all classes equally |\n",
    "\n",
    "For clinical use, **we care about detecting ALL arrhythmia types**, not just the common ones.\n",
    "\n",
    "### Why Fusion (F) is Inherently Ambiguous\n",
    "\n",
    "1. **Definition:** F beats occur when a normal beat and a ventricular beat happen simultaneously\n",
    "2. **Morphology:** Looks like a mix of N and V \u2014 no single distinctive pattern\n",
    "3. **Labeling:** Even expert cardiologists disagree on F annotations\n",
    "4. **Rarity:** <1% of beats \u2192 insufficient training samples\n",
    "5. **k-NN limitation:** Needs dense neighborhoods; F beats are sparse in feature space\n",
    "\n",
    "**Recommendation:** Consider merging F into V for product, or flagging low-confidence F predictions for human review.\n",
    "\n",
    "### Why Supraventricular (S) Depends on RR Features\n",
    "\n",
    "S beats are **premature atrial contractions** \u2014 they arrive early:\n",
    "- **Short RR_before** (premature timing)\n",
    "- **Compensatory pause** (longer RR_after)\n",
    "- Morphology often similar to Normal\n",
    "\n",
    "Without RR features, S is easily confused with N. The `prematurity_index` feature specifically captures this.\n",
    "\n",
    "### k-NN as a Baseline (Not a Final Model)\n",
    "\n",
    "**Use k-NN for:**\n",
    "- Quick benchmarking\n",
    "- Feature importance (which features reduce distance to correct class?)\n",
    "- Debugging data issues\n",
    "\n",
    "**Don't use k-NN for:**\n",
    "- Production deployment (slow inference on large datasets)\n",
    "- Learning complex temporal patterns\n",
    "- Handling severe class imbalance (no built-in weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# CONFUSION MATRIX (BEST k)\n# ============================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Raw counts\ncm = confusion_matrix(y_test_enc, y_pred_best, labels=range(len(ABNORMAL_CLASSES)))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=ABNORMAL_CLASSES, yticklabels=ABNORMAL_CLASSES, ax=axes[0])\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('True')\naxes[0].set_title(f'Confusion Matrix (k={best_k}) - Counts', fontweight='bold')\n\n# Normalized (percentages)\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n            xticklabels=ABNORMAL_CLASSES, yticklabels=ABNORMAL_CLASSES, ax=axes[1])\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('True')\naxes[1].set_title(f'Confusion Matrix (k={best_k}) - Normalized', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(OUTPUT_PATH / 'knn_confusion_matrix.png', dpi=150)\nplt.show()\n\nprint('\\n\ud83d\udcca Confusion matrix saved to outputs_knn/')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 2D VISUALIZATION: PCA and t-SNE\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca_2d = PCA(n_components=2, random_state=SEED)\n",
    "X_test_2d_pca = pca_2d.fit_transform(X_test_scaled)\n",
    "\n",
    "# t-SNE (better for visualizing clusters, but slower)\n",
    "print('Computing t-SNE (this may take a moment)...')\n",
    "tsne = TSNE(n_components=2, random_state=SEED, perplexity=30, n_iter=1000)\n",
    "X_test_2d_tsne = tsne.fit_transform(X_test_scaled)\n",
    "print('t-SNE complete.')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = {'S': '#e74c3c', 'V': '#3498db', 'F': '#9b59b6', 'Q': '#f39c12'}\n",
    "\n",
    "# PCA projection\n",
    "for cls in ABNORMAL_CLASSES:\n",
    "    mask = y_test == cls\n",
    "    axes[0].scatter(X_test_2d_pca[mask, 0], X_test_2d_pca[mask, 1],\n",
    "                    c=colors[cls], label=f'{cls} ({CLASS_NAMES[cls]})',\n",
    "                    alpha=0.6, s=30)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title('Test Data - 2D PCA', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# t-SNE projection\n",
    "for cls in ABNORMAL_CLASSES:\n",
    "    mask = y_test == cls\n",
    "    axes[1].scatter(X_test_2d_tsne[mask, 0], X_test_2d_tsne[mask, 1],\n",
    "                    c=colors[cls], label=f'{cls}',\n",
    "                    alpha=0.6, s=30)\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].set_title('Test Data - t-SNE (better cluster viz)', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'knn_2d_projections.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('\\n\ud83d\udcca OBSERVATIONS:')\n",
    "print('   - V (blue) should form distinct clusters (different morphology)')\n",
    "print('   - S (red) and F (purple) often overlap (similar features)')\n",
    "print('   - Q (orange) may be scattered (heterogeneous class)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# S vs F OVERLAP ANALYSIS (Key Insight)\n",
    "# ============================================================\n",
    "\n",
    "# Highlight just S and F to see overlap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot other classes faded\n",
    "for cls in ['V', 'Q']:\n",
    "    mask = y_test == cls\n",
    "    ax.scatter(X_test_2d_tsne[mask, 0], X_test_2d_tsne[mask, 1],\n",
    "               c='lightgray', alpha=0.2, s=20, label=f'{cls} (background)')\n",
    "\n",
    "# Plot S and F prominently\n",
    "s_mask = y_test == 'S'\n",
    "f_mask = y_test == 'F'\n",
    "ax.scatter(X_test_2d_tsne[s_mask, 0], X_test_2d_tsne[s_mask, 1],\n",
    "           c='#e74c3c', alpha=0.8, s=50, label=f'S (Supraventricular) n={s_mask.sum()}')\n",
    "ax.scatter(X_test_2d_tsne[f_mask, 0], X_test_2d_tsne[f_mask, 1],\n",
    "           c='#9b59b6', alpha=0.8, s=50, marker='x', label=f'F (Fusion) n={f_mask.sum()}')\n",
    "\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "ax.set_title('S vs F Overlap Analysis (t-SNE)', fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'knn_sf_overlap.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('\\n\ud83d\udd0d S vs F OVERLAP INSIGHT:')\n",
    "print('   If S and F points are heavily intermingled, this explains low F1 for both.')\n",
    "print('   k-NN struggles when classes share the same feature space region.')\n",
    "print('\\n   SOLUTIONS:')\n",
    "print('   1. Engineer features that separate S and F (e.g., QRS width for F)')\n",
    "print('   2. Use confidence thresholding for ambiguous predictions')\n",
    "print('   3. Consider merging F into V for product simplicity')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# MACRO F1 vs k PLOT\n# ============================================================\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nks = [r['k'] for r in results]\nmacro_f1s = [r['macro_f1'] for r in results]\naccuracies = [r['accuracy'] for r in results]\n\nax.plot(ks, macro_f1s, 'bo-', linewidth=2, markersize=10, label='Macro F1 (PRIMARY)')\nax.plot(ks, accuracies, 'g^--', linewidth=2, markersize=8, alpha=0.7, label='Accuracy')\n\n# Highlight best k\nbest_idx = macro_f1s.index(max(macro_f1s))\nax.scatter([ks[best_idx]], [macro_f1s[best_idx]], color='red', s=200, zorder=5, label=f'Best k={best_k}')\n\nax.set_xlabel('k (Number of Neighbors)', fontsize=12)\nax.set_ylabel('Score', fontsize=12)\nax.set_title('k-NN Performance vs. k', fontweight='bold', fontsize=14)\nax.set_xticks(ks)\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(OUTPUT_PATH / 'knn_k_selection.png', dpi=150)\nplt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Reproducibility & Good Practices\n\n### Why Record-Wise Splitting is Required for MIT-BIH\n\nThe MIT-BIH database contains **48 half-hour ECG recordings** from 47 patients.\n\nIf we split by beats instead of records:\n- Beats from the **same patient** could appear in both train and test\n- These beats are **highly correlated** (same heart, same morphology)\n- Model learns patient-specific patterns, not generalizable arrhythmia features\n- **Result:** Inflated metrics that don't reflect real-world performance\n\n**Solution:** Split by `record_id` to ensure no patient appears in both sets.\n\n### Diagnostic Benchmarking vs. Deployment\n\nThis notebook is for **benchmarking** \u2014 understanding baseline k-NN performance.\n\nFor **clinical deployment**, you would need:\n- External validation dataset (not MIT-BIH)\n- Confidence calibration\n- Real-time processing pipeline\n- Regulatory approval (FDA, CE marking)\n- Continuous monitoring and drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('\ud83c\udfaf k-NN ABNORMAL ECG CLASSIFICATION - DIAGNOSTIC SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'''\n",
    "DATASET:\n",
    "  Total abnormal beats: {len(df):,}\n",
    "  Train beats: {len(df_train):,} ({100*len(df_train)/len(df):.1f}%)\n",
    "  Test beats:  {len(df_test):,} ({100*len(df_test)/len(df):.1f}%)\n",
    "  Records (train/test): {len(train_records)}/{len(test_records)}\n",
    "  Normal (N) excluded: \u2713\n",
    "\n",
    "PREPROCESSING:\n",
    "  Original features: {len(feature_cols)} (including RR-heavy features for S)\n",
    "  PCA components: {n_components} (explaining {100*explained_var:.1f}% variance)\n",
    "  \n",
    "BEST MODEL:\n",
    "  k = {best_k}\n",
    "  Macro F1: {best_result['macro_f1']:.4f}\n",
    "  Accuracy: {best_result['accuracy']:.4f}\n",
    "\n",
    "PER-CLASS F1 SCORES:\n",
    "  S (Supraventricular): {best_result['f1_S']:.4f} {'\u26a0\ufe0f RR-dependent' if best_result['f1_S'] < 0.5 else ''}\n",
    "  V (Ventricular):      {best_result['f1_V']:.4f} {'\u2713 Strong' if best_result['f1_V'] > 0.8 else ''}\n",
    "  F (Fusion):           {best_result['f1_F']:.4f} {'\u274c Hardest class' if best_result['f1_F'] < 0.3 else ''}\n",
    "  Q (Unknown/Paced):    {best_result['f1_Q']:.4f}\n",
    "\n",
    "CONFIDENCE ANALYSIS:\n",
    "  Low-confidence predictions (<50%): {best_result['low_conf_pct']:.1f}%\n",
    "  High-confidence accuracy: {best_result['high_conf_acc']:.4f}\n",
    "\n",
    "STRENGTHS:\n",
    "  \u2022 V class: distinct morphology \u2192 high F1\n",
    "  \u2022 Q class: pacing artifacts \u2192 moderate F1\n",
    "  \u2022 Simple, interpretable baseline\n",
    "\n",
    "WEAKNESSES:\n",
    "  \u2022 F class: rare, ambiguous \u2192 very low F1\n",
    "  \u2022 S class: depends on RR features, morphology similar to N\n",
    "  \u2022 k-NN doesn't handle imbalance well\n",
    "\n",
    "KEY MIT-BIH LIMITATIONS:\n",
    "  \u2022 Small dataset (48 recordings, ~100k beats)\n",
    "  \u2022 F class extremely rare (<1%)\n",
    "  \u2022 Annotations sometimes ambiguous (inter-rater disagreement)\n",
    "  \u2022 1980s recordings \u2014 different from modern ECGs\n",
    "  \u2022 Record-wise split reduces effective training data\n",
    "\n",
    "CONCRETE NEXT STEPS:\n",
    "  1. RR-feature enrichment for S (more prematurity measures)\n",
    "  2. Confidence thresholding for F (flag, don't force prediction)\n",
    "  3. Compare raw features vs CNN embeddings as k-NN input\n",
    "  4. Decide: merge F into V for product, or keep separate?\n",
    "  5. Validate on INCART or PTB-XL (external datasets)\n",
    "  6. Try ensemble (k-NN + RF + XGBoost) for robustness\n",
    "\n",
    "\u26a0\ufe0f REMINDER: This notebook is DIAGNOSTIC.\n",
    "   k-NN is a baseline, not a production model.\n",
    "   Do not oversell these results.\n",
    "\n",
    "\u2705 Diagnostic Analysis Complete!\n",
    "''')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# SAVE RESULTS\n# ============================================================\n\nimport json\n\n# Save results summary\nresults_summary = {\n    'best_k': int(best_k),\n    'macro_f1': float(best_result['macro_f1']),\n    'accuracy': float(best_result['accuracy']),\n    'per_class_f1': {\n        'S': float(best_result['f1_S']),\n        'V': float(best_result['f1_V']),\n        'F': float(best_result['f1_F']),\n        'Q': float(best_result['f1_Q'])\n    },\n    'all_results': [{\n        'k': int(r['k']),\n        'macro_f1': float(r['macro_f1']),\n        'accuracy': float(r['accuracy'])\n    } for r in results],\n    'preprocessing': {\n        'n_features_original': len(feature_cols),\n        'n_pca_components': int(n_components),\n        'pca_variance_explained': float(explained_var)\n    },\n    'dataset': {\n        'total_beats': int(len(df)),\n        'train_beats': int(len(df_train)),\n        'test_beats': int(len(df_test))\n    }\n}\n\nwith open(OUTPUT_PATH / 'knn_results.json', 'w') as f:\n    json.dump(results_summary, f, indent=2)\n\nprint('\\n\u2705 Results saved to outputs_knn/')\nprint(f'   - knn_results.json')\nprint(f'   - knn_confusion_matrix.png')\nprint(f'   - knn_pca_projection.png')\nprint(f'   - knn_k_selection.png')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}