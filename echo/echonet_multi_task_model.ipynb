{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EchoNet-Dynamic Multi-Task Model (EF, EDV, ESV)\n",
        "\n",
        "**Google Colab Version** ‚Äî Optimized for fast training with local SSD storage.\n",
        "\n",
        "This notebook implements a complete pipeline for training a 3D CNN model to predict:\n",
        "- **EF** (Ejection Fraction)\n",
        "- **EDV** (End-Diastolic Volume)\n",
        "- **ESV** (End-Systolic Volume)\n",
        "\n",
        "from echocardiogram videos.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup Flow (Each Session)\n",
        "1. Mount Google Drive (where raw videos are stored)\n",
        "2. Preprocess videos ‚Üí save directly to local SSD (~1-2 hrs)\n",
        "3. Train model (fast I/O from local disk)\n",
        "4. Model saved to Google Drive (persistent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GOOGLE COLAB SETUP\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "%pip install -q imageio imageio-ffmpeg\n",
        "\n",
        "print('\\n‚úÖ Drive mounted and dependencies installed!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1) IMPORTS\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import video reading libraries\n",
        "try:\n",
        "    from torchvision.io import read_video\n",
        "    USE_TORCHVISION = True\n",
        "except:\n",
        "    USE_TORCHVISION = False\n",
        "\n",
        "try:\n",
        "    import imageio\n",
        "    USE_IMAGEIO = True\n",
        "except ImportError:\n",
        "    USE_IMAGEIO = False\n",
        "    print(\"Warning: imageio not installed. Install with: pip install imageio imageio-ffmpeg\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading labels from: /Volumes/Crucial X6/medical_ai_extra/EchoNet-Dynamic/FileList.csv\n",
            "Dataset shape: (10030, 9)\n",
            "\n",
            "Columns: ['FileName', 'EF', 'ESV', 'EDV', 'FrameHeight', 'FrameWidth', 'FPS', 'NumberOfFrames', 'Split']\n",
            "\n",
            "First few rows:\n",
            "             FileName         EF         ESV         EDV  FrameHeight  \\\n",
            "0  0X100009310A3BD7FC  78.498406   14.881368   69.210534          112   \n",
            "1  0X1002E8FBACD08477  59.101988   40.383876   98.742884          112   \n",
            "2  0X1005D03EED19C65B  62.363798   14.267784   37.909734          112   \n",
            "3  0X10075961BC11C88E  54.545097   33.143084   72.914210          112   \n",
            "4  0X10094BA0A028EAC3  24.887742  127.581945  169.855024          112   \n",
            "\n",
            "   FrameWidth  FPS  NumberOfFrames  Split  \n",
            "0         112   50             174    VAL  \n",
            "1         112   50             215  TRAIN  \n",
            "2         112   50             104  TRAIN  \n",
            "3         112   55             122  TRAIN  \n",
            "4         112   52             207    VAL  \n",
            "\n",
            "Data types:\n",
            "FileName           object\n",
            "EF                float64\n",
            "ESV               float64\n",
            "EDV               float64\n",
            "FrameHeight         int64\n",
            "FrameWidth          int64\n",
            "FPS                 int64\n",
            "NumberOfFrames      int64\n",
            "Split              object\n",
            "dtype: object\n",
            "\n",
            "Missing values:\n",
            "FileName          0\n",
            "EF                0\n",
            "ESV               0\n",
            "EDV               0\n",
            "FrameHeight       0\n",
            "FrameWidth        0\n",
            "FPS               0\n",
            "NumberOfFrames    0\n",
            "Split             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2) PATH CONFIGURATION (GOOGLE DRIVE)\n",
        "# ============================================================\n",
        "\n",
        "# Google Drive paths (persistent storage)\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/EchoNet-Dynamic\"\n",
        "VIDEOS_DIR = os.path.join(DRIVE_BASE, \"Videos\")\n",
        "LABELS_CSV = os.path.join(DRIVE_BASE, \"FileList.csv\")\n",
        "PREPROCESSED_DIR_DRIVE = os.path.join(DRIVE_BASE, \"PreprocessedVideos\")\n",
        "\n",
        "# Local SSD paths (fast I/O, cleared each session)\n",
        "LOCAL_BASE = \"/content/echonet_local\"\n",
        "PREPROCESSED_DIR_LOCAL = os.path.join(LOCAL_BASE, \"PreprocessedVideos\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(PREPROCESSED_DIR_DRIVE, exist_ok=True)\n",
        "os.makedirs(LOCAL_BASE, exist_ok=True)\n",
        "\n",
        "# Verify data exists\n",
        "print(\"üìÅ Checking Google Drive paths...\")\n",
        "print(f\"   DRIVE_BASE: {DRIVE_BASE}\")\n",
        "print(f\"   Videos: {'‚úÖ' if os.path.exists(VIDEOS_DIR) else '‚ùå'} {VIDEOS_DIR}\")\n",
        "print(f\"   Labels: {'‚úÖ' if os.path.exists(LABELS_CSV) else '‚ùå'} {LABELS_CSV}\")\n",
        "\n",
        "# Load labels CSV\n",
        "print(f\"\\nüìä Loading labels...\")\n",
        "df = pd.read_csv(LABELS_CSV)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5) Preprocess All Videos (Saves to Local SSD)\n",
        "\n",
        "This cell preprocesses all videos and saves them as `.pt` files **to local SSD** (fast I/O).\n",
        "- Runs **each Colab session** (local storage is cleared when session ends)\n",
        "- Takes ~1-2 hours\n",
        "- Training is MUCH faster from local SSD than from Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2.5) PREPROCESS ALL VIDEOS (SAVES TO LOCAL SSD)\n",
        "# ============================================================\n",
        "# This converts raw .avi videos to .pt tensors for faster loading\n",
        "# Saves to LOCAL SSD for fast training I/O\n",
        "# Needs to run each Colab session (local storage is ephemeral)\n",
        "\n",
        "import shutil\n",
        "import gc\n",
        "\n",
        "# Save preprocessed videos to LOCAL SSD (fast I/O for training)\n",
        "PREPROCESSED_DIR = PREPROCESSED_DIR_LOCAL\n",
        "os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n",
        "\n",
        "def preprocess_single_video(video_path, num_frames=32, target_size=(112, 112)):\n",
        "    \"\"\"\n",
        "    Memory-efficient video preprocessing.\n",
        "    Uses get_data() to load ONLY the specific frames we need.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        reader = imageio.get_reader(video_path)\n",
        "        total_frames = reader.count_frames()\n",
        "        \n",
        "        # Calculate which frame indices to sample\n",
        "        if total_frames >= num_frames:\n",
        "            indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "        else:\n",
        "            indices = list(range(total_frames)) + [total_frames - 1] * (num_frames - total_frames)\n",
        "            indices = np.array(indices[:num_frames])\n",
        "        \n",
        "        # Pre-allocate output tensor (C, T, H, W)\n",
        "        video_tensor = np.zeros((3, num_frames, target_size[0], target_size[1]), dtype=np.float32)\n",
        "        \n",
        "        # Load ONLY the frames we need using get_data() - much more memory efficient!\n",
        "        for out_idx, frame_idx in enumerate(indices):\n",
        "            frame = reader.get_data(int(frame_idx))  # Load single frame\n",
        "            \n",
        "            # Convert HWC -> CHW, resize, normalize in one go\n",
        "            frame_chw = np.transpose(frame, (2, 0, 1))\n",
        "            frame_t = torch.from_numpy(frame_chw).float()\n",
        "            frame_resized = resize(frame_t, target_size, antialias=True).numpy()\n",
        "            video_tensor[:, out_idx, :, :] = frame_resized / 255.0\n",
        "            \n",
        "            # Free intermediate memory\n",
        "            del frame, frame_chw, frame_t, frame_resized\n",
        "        \n",
        "        reader.close()\n",
        "        return torch.from_numpy(video_tensor)\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {video_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Check how many are already preprocessed\n",
        "existing = set(f.replace('.pt', '') for f in os.listdir(PREPROCESSED_DIR) if f.endswith('.pt'))\n",
        "to_process = df[~df['FileName'].isin(existing)]\n",
        "\n",
        "print(f\"üìä Preprocessing Status:\")\n",
        "print(f\"   Already preprocessed: {len(existing):,}\")\n",
        "print(f\"   Remaining to process: {len(to_process):,}\")\n",
        "print(f\"   Total videos: {len(df):,}\")\n",
        "\n",
        "if len(to_process) > 0:\n",
        "    print(f\"\\nüîÑ Starting preprocessing...\")\n",
        "    \n",
        "    failed = []\n",
        "    for idx, row in tqdm(to_process.iterrows(), total=len(to_process), desc=\"Preprocessing\"):\n",
        "        filename = row['FileName']\n",
        "        video_path = os.path.join(VIDEOS_DIR, filename + '.avi')\n",
        "        output_path = os.path.join(PREPROCESSED_DIR, f\"{filename}.pt\")\n",
        "        \n",
        "        if os.path.exists(output_path):\n",
        "            continue\n",
        "        \n",
        "        video_tensor = preprocess_single_video(video_path)\n",
        "        \n",
        "        if video_tensor is not None:\n",
        "            torch.save(video_tensor, output_path)\n",
        "            del video_tensor  # Free immediately after saving\n",
        "        else:\n",
        "            failed.append(filename)\n",
        "        \n",
        "        # Aggressive memory cleanup every 10 videos\n",
        "        if idx % 10 == 0:\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
        "    print(f\"   Successful: {len(to_process) - len(failed):,}\")\n",
        "    print(f\"   Failed: {len(failed)}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All videos already preprocessed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6) Verify Preprocessing Complete\n",
        "\n",
        "Quick check that all videos were preprocessed successfully.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2.6) VERIFY PREPROCESSING COMPLETE\n",
        "# ============================================================\n",
        "# Quick check that preprocessing completed successfully\n",
        "\n",
        "local_pt_files = [f for f in os.listdir(PREPROCESSED_DIR) if f.endswith('.pt')]\n",
        "\n",
        "print(f\"üìä Preprocessing Status:\")\n",
        "print(f\"   Preprocessed videos: {len(local_pt_files):,}\")\n",
        "print(f\"   Total in dataset: {len(df):,}\")\n",
        "print(f\"   Location: {PREPROCESSED_DIR} (local SSD)\")\n",
        "\n",
        "if len(local_pt_files) >= len(df) * 0.95:  # Allow 5% failure\n",
        "    print(f\"\\n‚úÖ Preprocessing complete! Ready to train.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Only {len(local_pt_files)}/{len(df)} videos preprocessed.\")\n",
        "    print(\"   Run the preprocessing cell above to complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 3) PREPROCESSING\n",
        "# ============================================================\n",
        "\n",
        "def preprocess_video(video_path, num_frames=32, target_size=(112, 112)):\n",
        "    \"\"\"\n",
        "    Preprocess video: sample frames, resize, normalize.\n",
        "    \n",
        "    Args:\n",
        "        video_path: Path to video file\n",
        "        num_frames: Number of frames to sample (default: 32)\n",
        "        target_size: Target frame size (H, W) (default: (112, 112))\n",
        "    \n",
        "    Returns:\n",
        "        tensor: Shape (C=3, T=32, H=112, W=112)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to read video using torchvision first, fallback to imageio\n",
        "        if USE_TORCHVISION:\n",
        "            try:\n",
        "                video, audio, info = read_video(video_path, output_format=\"TCHW\")\n",
        "                # video shape: (T, C, H, W)\n",
        "                video_np = video.numpy()\n",
        "            except Exception as e:\n",
        "                # Fallback to imageio if torchvision fails\n",
        "                if USE_IMAGEIO:\n",
        "                    reader = imageio.get_reader(video_path)\n",
        "                    frames = []\n",
        "                    for frame in reader:\n",
        "                        # frame is (H, W, C), convert to (C, H, W)\n",
        "                        frame_chw = np.transpose(frame, (2, 0, 1))\n",
        "                        frames.append(frame_chw)\n",
        "                    reader.close()\n",
        "                    # Stack frames: (T, C, H, W)\n",
        "                    video_np = np.stack(frames, axis=0)\n",
        "                else:\n",
        "                    raise e\n",
        "        elif USE_IMAGEIO:\n",
        "            # Use imageio directly\n",
        "            reader = imageio.get_reader(video_path)\n",
        "            frames = []\n",
        "            for frame in reader:\n",
        "                # frame is (H, W, C), convert to (C, H, W)\n",
        "                frame_chw = np.transpose(frame, (2, 0, 1))\n",
        "                frames.append(frame_chw)\n",
        "            reader.close()\n",
        "            # Stack frames: (T, C, H, W)\n",
        "            video_np = np.stack(frames, axis=0)\n",
        "        else:\n",
        "            raise ImportError(\"Neither torchvision nor imageio available for video reading\")\n",
        "        \n",
        "        T, C, H, W = video_np.shape\n",
        "        \n",
        "        # Sample exactly num_frames uniformly\n",
        "        if T >= num_frames:\n",
        "            indices = np.linspace(0, T - 1, num_frames, dtype=int)\n",
        "        else:\n",
        "            # If video has fewer frames, repeat last frame\n",
        "            indices = list(range(T)) + [T - 1] * (num_frames - T)\n",
        "            indices = indices[:num_frames]\n",
        "        \n",
        "        sampled_frames = video_np[indices]  # (num_frames, C, H, W)\n",
        "        \n",
        "        # Resize frames to target_size using torchvision\n",
        "        resized_frames = []\n",
        "        for frame in sampled_frames:\n",
        "            # frame is (C, H, W) - convert to torch tensor\n",
        "            frame_tensor = torch.from_numpy(frame).float()\n",
        "            # Resize using torchvision (expects (C, H, W) format)\n",
        "            frame_resized = resize(frame_tensor, target_size, antialias=True)\n",
        "            resized_frames.append(frame_resized.numpy())\n",
        "        \n",
        "        # Stack frames: (num_frames, C, H, W) -> (C, num_frames, H, W)\n",
        "        video_tensor = np.stack(resized_frames, axis=1)\n",
        "        \n",
        "        # Normalize to [0, 1]\n",
        "        video_tensor = video_tensor.astype(np.float32) / 255.0\n",
        "        \n",
        "        return torch.from_numpy(video_tensor)\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {video_path}: {e}\")\n",
        "        # Return zero tensor as fallback\n",
        "        return torch.zeros((3, num_frames, target_size[0], target_size[1]), dtype=torch.float32)\n",
        "\n",
        "# Test preprocessing function\n",
        "print(\"Preprocessing function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EchoDataset class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 4) BUILD PYTORCH DATASET\n",
        "# ============================================================\n",
        "\n",
        "class EchoDataset(Dataset):\n",
        "    def __init__(self, dataframe, videos_dir, num_frames=32, target_size=(112, 112), \n",
        "                 preprocessed_dir=None, use_preprocessed=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe: DataFrame with columns: FileName, EF, EDV, ESV\n",
        "            videos_dir: Base directory containing video files\n",
        "            num_frames: Number of frames to sample per video\n",
        "            target_size: Target frame size (H, W)\n",
        "            preprocessed_dir: Directory with preprocessed .pt files (optional)\n",
        "            use_preprocessed: If True, use preprocessed files if available\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe.reset_index(drop=True)\n",
        "        self.videos_dir = videos_dir\n",
        "        self.preprocessed_dir = preprocessed_dir\n",
        "        self.use_preprocessed = use_preprocessed\n",
        "        self.num_frames = num_frames\n",
        "        self.target_size = target_size\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        \n",
        "        # Try to load preprocessed video first\n",
        "        if self.use_preprocessed and self.preprocessed_dir:\n",
        "            preprocessed_path = os.path.join(self.preprocessed_dir, f\"{row['FileName']}.pt\")\n",
        "            if os.path.exists(preprocessed_path):\n",
        "                video_tensor = torch.load(preprocessed_path)\n",
        "                # Ensure correct shape\n",
        "                if video_tensor.shape != (3, self.num_frames, self.target_size[0], self.target_size[1]):\n",
        "                    # If shape doesn't match, fall back to preprocessing\n",
        "                    video_tensor = None\n",
        "            else:\n",
        "                video_tensor = None\n",
        "        else:\n",
        "            video_tensor = None\n",
        "        \n",
        "        # If preprocessed not available, process on-the-fly\n",
        "        if video_tensor is None:\n",
        "            filename = row['FileName']\n",
        "            if not filename.endswith('.avi'):\n",
        "                filename = filename + '.avi'\n",
        "            video_path = os.path.join(self.videos_dir, filename)\n",
        "            video_tensor = preprocess_video(video_path, self.num_frames, self.target_size)\n",
        "        \n",
        "        # Get labels\n",
        "        EF = float(row['EF'])\n",
        "        EDV = float(row['EDV'])\n",
        "        ESV = float(row['ESV'])\n",
        "        \n",
        "        labels = torch.tensor([EF, EDV, ESV], dtype=torch.float32)\n",
        "        \n",
        "        return video_tensor, labels\n",
        "\n",
        "print(\"EchoDataset class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Train/Val/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing Split column from CSV\n",
            "Train: 7465 samples\n",
            "Val: 1288 samples\n",
            "Test: 1277 samples\n",
            "‚úÖ Using preprocessed videos from: /Volumes/Crucial X6/medical_ai_extra/EchoNet-Dynamic/PreprocessedVideos\n",
            "   Found 10030 preprocessed files\n",
            "\n",
            "Datasets created successfully!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 5) SPLITTING\n",
        "# ============================================================\n",
        "\n",
        "# Check if Split column exists\n",
        "if 'Split' in df.columns:\n",
        "    print(\"Using existing Split column from CSV\")\n",
        "    # Handle both uppercase (TRAIN/VAL/TEST) and lowercase (train/val/test) values\n",
        "    df['Split'] = df['Split'].str.upper()\n",
        "    train_df = df[df['Split'].isin(['TRAIN', 'TRAINING'])].copy()\n",
        "    val_df = df[df['Split'].isin(['VAL', 'VALIDATION'])].copy()\n",
        "    test_df = df[df['Split'].isin(['TEST', 'TESTING'])].copy()\n",
        "    \n",
        "    print(f\"Train: {len(train_df)} samples\")\n",
        "    print(f\"Val: {len(val_df)} samples\")\n",
        "    print(f\"Test: {len(test_df)} samples\")\n",
        "else:\n",
        "    print(\"Split column not found. Creating train/val/test splits...\")\n",
        "    # First split: train (70%) and temp (30%)\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df, test_size=0.3, random_state=42, shuffle=True\n",
        "    )\n",
        "    # Second split: val (15%) and test (15%) from temp\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df, test_size=0.5, random_state=42, shuffle=True\n",
        "    )\n",
        "    \n",
        "    print(f\"Train: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"Val: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"Test: {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Create datasets (PREPROCESSED_DIR set in cell 2.6 to local SSD for fast I/O)\n",
        "# If PREPROCESSED_DIR is None, fall back to on-the-fly processing (slow)\n",
        "use_preprocessed = PREPROCESSED_DIR is not None and os.path.exists(PREPROCESSED_DIR) and len(os.listdir(PREPROCESSED_DIR)) > 0\n",
        "\n",
        "if use_preprocessed:\n",
        "    print(f\"‚úÖ Using preprocessed videos from: {PREPROCESSED_DIR}\")\n",
        "    print(f\"   Found {len(os.listdir(PREPROCESSED_DIR)):,} preprocessed files (local SSD - fast!)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No preprocessed videos found. Videos will be processed on-the-fly (slow).\")\n",
        "    print(\"   Run the preprocessing cells above first!\")\n",
        "\n",
        "train_dataset = EchoDataset(train_df, VIDEOS_DIR, preprocessed_dir=PREPROCESSED_DIR, \n",
        "                           use_preprocessed=use_preprocessed)\n",
        "val_dataset = EchoDataset(val_df, VIDEOS_DIR, preprocessed_dir=PREPROCESSED_DIR,\n",
        "                          use_preprocessed=use_preprocessed)\n",
        "test_dataset = EchoDataset(test_df, VIDEOS_DIR, preprocessed_dir=PREPROCESSED_DIR,\n",
        "                           use_preprocessed=use_preprocessed)\n",
        "\n",
        "print(\"\\nDatasets created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) 3D CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created. Total parameters: 4,883,331\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test forward pass successful!\n",
            "  EF output shape: torch.Size([2])\n",
            "  EDV output shape: torch.Size([2])\n",
            "  ESV output shape: torch.Size([2])\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 6) MODEL (3D CNN)\n",
        "# ============================================================\n",
        "\n",
        "class EchoNet3DCNN(nn.Module):\n",
        "    def __init__(self, feature_dim=256):\n",
        "        super(EchoNet3DCNN, self).__init__()\n",
        "        \n",
        "        # 3D CNN Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
        "            \n",
        "            # Block 2\n",
        "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
        "            \n",
        "            # Block 3\n",
        "            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
        "            nn.BatchNorm3d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
        "            \n",
        "            # Block 4\n",
        "            nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
        "            nn.BatchNorm3d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Adaptive pooling to get fixed-size feature vector\n",
        "            nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        )\n",
        "        \n",
        "        # Flatten\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        # Feature projection to feature_dim\n",
        "        self.feature_proj = nn.Sequential(\n",
        "            nn.Linear(512, feature_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        \n",
        "        # Prediction heads\n",
        "        self.EF_head = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        \n",
        "        self.EDV_head = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        \n",
        "        self.ESV_head = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (B, C=3, T=32, H=112, W=112)\n",
        "        features = self.encoder(x)  # (B, 512, 1, 1, 1)\n",
        "        features = self.flatten(features)  # (B, 512)\n",
        "        features = self.feature_proj(features)  # (B, feature_dim)\n",
        "        \n",
        "        # Multi-task predictions\n",
        "        EF_pred = self.EF_head(features)  # (B, 1)\n",
        "        EDV_pred = self.EDV_head(features)  # (B, 1)\n",
        "        ESV_pred = self.ESV_head(features)  # (B, 1)\n",
        "        \n",
        "        return EF_pred.squeeze(-1), EDV_pred.squeeze(-1), ESV_pred.squeeze(-1)\n",
        "\n",
        "# Initialize model\n",
        "model = EchoNet3DCNN(feature_dim=256).to(device)\n",
        "print(f\"Model created. Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(2, 3, 32, 112, 112).to(device)\n",
        "EF_test, EDV_test, ESV_test = model(test_input)\n",
        "print(f\"Test forward pass successful!\")\n",
        "print(f\"  EF output shape: {EF_test.shape}\")\n",
        "print(f\"  EDV output shape: {EDV_test.shape}\")\n",
        "print(f\"  ESV output shape: {ESV_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training setup complete!\n",
            "  Device: cpu\n",
            "  Batch size: 4\n",
            "  Learning rate: 0.0001\n",
            "  Number of epochs: 40\n",
            "  Train batches: 1867\n",
            "  Val batches: 322\n",
            "  Workers: 0\n",
            "\n",
            "‚ö†Ô∏è  NOTE: Training on CPU with on-the-fly video preprocessing is very slow.\n",
            "    Estimated time per epoch: ~51.9 hours (at ~100s/iter)\n",
            "    Consider: 1) Using GPU, 2) Preprocessing videos ahead of time, or 3) Reducing epochs for testing\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 7) TRAINING SETUP\n",
        "# ============================================================\n",
        "\n",
        "# Hyperparameters\n",
        "# Update device if GPU becomes available (device already set in imports)\n",
        "if torch.backends.mps.is_available() and device.type == 'cpu':\n",
        "    device = torch.device('mps')\n",
        "    print(\"Switching to MPS (Apple Silicon GPU)\")\n",
        "elif torch.cuda.is_available() and device.type == 'cpu':\n",
        "    device = torch.device('cuda')\n",
        "    print(\"Switching to CUDA GPU\")\n",
        "\n",
        "BATCH_SIZE = 8 if device.type != 'cpu' else 4  # Larger batch on GPU\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 5  # Reduced for testing - change back to 40 for full training\n",
        "NUM_WORKERS = 0  # Use main process to avoid multiprocessing issues on Mac\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True, \n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False, \n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "# Loss function (MAE for each task)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "print(\"Training setup complete!\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Workers: {NUM_WORKERS}\")\n",
        "print(f\"\\n‚ö†Ô∏è  NOTE: Training on CPU with on-the-fly video preprocessing is very slow.\")\n",
        "print(f\"    Estimated time per epoch: ~{len(train_loader) * 100 / 3600:.1f} hours (at ~100s/iter)\")\n",
        "print(f\"    Consider: 1) Using GPU, 2) Preprocessing videos ahead of time, or 3) Reducing epochs for testing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training functions defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 7) TRAINING LOOP\n",
        "# ============================================================\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_EF_loss = 0.0\n",
        "    total_EDV_loss = 0.0\n",
        "    total_ESV_loss = 0.0\n",
        "    \n",
        "    for videos, labels in tqdm(loader, desc=\"Training\"):\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)  # (B, 3) [EF, EDV, ESV]\n",
        "        \n",
        "        # Forward pass\n",
        "        EF_pred, EDV_pred, ESV_pred = model(videos)\n",
        "        \n",
        "        # Compute losses\n",
        "        EF_loss = criterion(EF_pred, labels[:, 0])\n",
        "        EDV_loss = criterion(EDV_pred, labels[:, 1])\n",
        "        ESV_loss = criterion(ESV_pred, labels[:, 2])\n",
        "        \n",
        "        total_loss_batch = EF_loss + EDV_loss + ESV_loss\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        total_loss_batch.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Accumulate losses\n",
        "        total_loss += total_loss_batch.item()\n",
        "        total_EF_loss += EF_loss.item()\n",
        "        total_EDV_loss += EDV_loss.item()\n",
        "        total_ESV_loss += ESV_loss.item()\n",
        "    \n",
        "    return {\n",
        "        'total_loss': total_loss / len(loader),\n",
        "        'EF_loss': total_EF_loss / len(loader),\n",
        "        'EDV_loss': total_EDV_loss / len(loader),\n",
        "        'ESV_loss': total_ESV_loss / len(loader)\n",
        "    }\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_EF_loss = 0.0\n",
        "    total_EDV_loss = 0.0\n",
        "    total_ESV_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for videos, labels in tqdm(loader, desc=\"Validating\"):\n",
        "            videos = videos.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            EF_pred, EDV_pred, ESV_pred = model(videos)\n",
        "            \n",
        "            # Compute losses\n",
        "            EF_loss = criterion(EF_pred, labels[:, 0])\n",
        "            EDV_loss = criterion(EDV_pred, labels[:, 1])\n",
        "            ESV_loss = criterion(ESV_pred, labels[:, 2])\n",
        "            \n",
        "            total_loss_batch = EF_loss + EDV_loss + ESV_loss\n",
        "            \n",
        "            # Accumulate losses\n",
        "            total_loss += total_loss_batch.item()\n",
        "            total_EF_loss += EF_loss.item()\n",
        "            total_EDV_loss += EDV_loss.item()\n",
        "            total_ESV_loss += ESV_loss.item()\n",
        "    \n",
        "    return {\n",
        "        'total_loss': total_loss / len(loader),\n",
        "        'EF_loss': total_EF_loss / len(loader),\n",
        "        'EDV_loss': total_EDV_loss / len(loader),\n",
        "        'ESV_loss': total_ESV_loss / len(loader)\n",
        "    }\n",
        "\n",
        "print(\"Training functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: /Volumes/Crucial X6/medical_ai_extra/echonet_outputs\n",
            "Model will be saved to: /Volumes/Crucial X6/medical_ai_extra/echonet_outputs/echonet_multi_task.pth\n",
            "Starting training...\n",
            "============================================================\n",
            "\n",
            "Epoch 1/40\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/1867 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1867/1867 [7:26:46<00:00, 14.36s/it]  \n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 322/322 [26:32<00:00,  4.95s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss - Total: 71.5037, EF: 14.1937, EDV: 35.6947, ESV: 21.6153\n",
            "Val Loss   - Total: 59.1177, EF: 9.9926, EDV: 29.4616, ESV: 19.6634\n",
            "‚úì Saved best model (val_loss: 59.1177)\n",
            "\n",
            "Epoch 2/40\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 664/1867 [2:34:24<4:39:45, 13.95s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m train_metrics = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m train_history[\u001b[33m'\u001b[39m\u001b[33mtotal_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_metrics[\u001b[33m'\u001b[39m\u001b[33mtotal_loss\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     34\u001b[39m train_history[\u001b[33m'\u001b[39m\u001b[33mEF_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_metrics[\u001b[33m'\u001b[39m\u001b[33mEF_loss\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     14\u001b[39m labels = labels.to(device)  \u001b[38;5;66;03m# (B, 3) [EF, EDV, ESV]\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m EF_pred, EDV_pred, ESV_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[32m     20\u001b[39m EF_loss = criterion(EF_pred, labels[:, \u001b[32m0\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mEchoNet3DCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# x shape: (B, C=3, T=32, H=112, W=112)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, 512, 1, 1, 1)\u001b[39;00m\n\u001b[32m     73\u001b[39m     features = \u001b[38;5;28mself\u001b[39m.flatten(features)  \u001b[38;5;66;03m# (B, 512)\u001b[39;00m\n\u001b[32m     74\u001b[39m     features = \u001b[38;5;28mself\u001b[39m.feature_proj(features)  \u001b[38;5;66;03m# (B, feature_dim)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:610\u001b[39m, in \u001b[36mConv3d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:605\u001b[39m, in \u001b[36mConv3d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    594\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv3d(\n\u001b[32m    595\u001b[39m         F.pad(\n\u001b[32m    596\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    603\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    604\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Training history\n",
        "train_history = {\n",
        "    'total_loss': [],\n",
        "    'EF_loss': [],\n",
        "    'EDV_loss': [],\n",
        "    'ESV_loss': []\n",
        "}\n",
        "\n",
        "val_history = {\n",
        "    'total_loss': [],\n",
        "    'EF_loss': [],\n",
        "    'EDV_loss': [],\n",
        "    'ESV_loss': []\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "# Create output directory on Google Drive (persistent across sessions)\n",
        "OUTPUT_DIR = os.path.join(DRIVE_BASE, \"outputs\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "best_model_path = os.path.join(OUTPUT_DIR, \"echonet_multi_task.pth\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Model will be saved to: {best_model_path}\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Train\n",
        "    train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_history['total_loss'].append(train_metrics['total_loss'])\n",
        "    train_history['EF_loss'].append(train_metrics['EF_loss'])\n",
        "    train_history['EDV_loss'].append(train_metrics['EDV_loss'])\n",
        "    train_history['ESV_loss'].append(train_metrics['ESV_loss'])\n",
        "    \n",
        "    # Validate\n",
        "    val_metrics = validate_epoch(model, val_loader, criterion, device)\n",
        "    val_history['total_loss'].append(val_metrics['total_loss'])\n",
        "    val_history['EF_loss'].append(val_metrics['EF_loss'])\n",
        "    val_history['EDV_loss'].append(val_metrics['EDV_loss'])\n",
        "    val_history['ESV_loss'].append(val_metrics['ESV_loss'])\n",
        "    \n",
        "    # Print metrics\n",
        "    print(f\"\\nTrain Loss - Total: {train_metrics['total_loss']:.4f}, \"\n",
        "          f\"EF: {train_metrics['EF_loss']:.4f}, \"\n",
        "          f\"EDV: {train_metrics['EDV_loss']:.4f}, \"\n",
        "          f\"ESV: {train_metrics['ESV_loss']:.4f}\")\n",
        "    print(f\"Val Loss   - Total: {val_metrics['total_loss']:.4f}, \"\n",
        "          f\"EF: {val_metrics['EF_loss']:.4f}, \"\n",
        "          f\"EDV: {val_metrics['EDV_loss']:.4f}, \"\n",
        "          f\"ESV: {val_metrics['ESV_loss']:.4f}\")\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step(val_metrics['total_loss'])\n",
        "    \n",
        "    # Save best model\n",
        "    if val_metrics['total_loss'] < best_val_loss:\n",
        "        best_val_loss = val_metrics['total_loss']\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"‚úì Saved best model (val_loss: {best_val_loss:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Best model saved to: {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Testing/Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8) TESTING\n",
        "# ============================================================\n",
        "\n",
        "# Load best model\n",
        "print(f\"Loading best model from: {best_model_path}\")\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Create test loader\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False, \n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_EF_pred = []\n",
        "all_EDV_pred = []\n",
        "all_ESV_pred = []\n",
        "all_EF_true = []\n",
        "all_EDV_true = []\n",
        "all_ESV_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for videos, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        EF_pred, EDV_pred, ESV_pred = model(videos)\n",
        "        \n",
        "        # Store predictions and ground truth\n",
        "        all_EF_pred.extend(EF_pred.cpu().numpy())\n",
        "        all_EDV_pred.extend(EDV_pred.cpu().numpy())\n",
        "        all_ESV_pred.extend(ESV_pred.cpu().numpy())\n",
        "        all_EF_true.extend(labels[:, 0].cpu().numpy())\n",
        "        all_EDV_true.extend(labels[:, 1].cpu().numpy())\n",
        "        all_ESV_true.extend(labels[:, 2].cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_EF_pred = np.array(all_EF_pred)\n",
        "all_EDV_pred = np.array(all_EDV_pred)\n",
        "all_ESV_pred = np.array(all_ESV_pred)\n",
        "all_EF_true = np.array(all_EF_true)\n",
        "all_EDV_true = np.array(all_EDV_true)\n",
        "all_ESV_true = np.array(all_ESV_true)\n",
        "\n",
        "# Calculate MAE for each task\n",
        "EF_mae = np.mean(np.abs(all_EF_pred - all_EF_true))\n",
        "EDV_mae = np.mean(np.abs(all_EDV_pred - all_EDV_true))\n",
        "ESV_mae = np.mean(np.abs(all_ESV_pred - all_ESV_true))\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"EF  MAE: {EF_mae:.4f}\")\n",
        "print(f\"EDV MAE: {EDV_mae:.4f}\")\n",
        "print(f\"ESV MAE: {ESV_mae:.4f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Additional metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "EF_rmse = np.sqrt(mean_squared_error(all_EF_true, all_EF_pred))\n",
        "EDV_rmse = np.sqrt(mean_squared_error(all_EDV_true, all_EDV_pred))\n",
        "ESV_rmse = np.sqrt(mean_squared_error(all_ESV_true, all_ESV_pred))\n",
        "\n",
        "EF_r2 = r2_score(all_EF_true, all_EF_pred)\n",
        "EDV_r2 = r2_score(all_EDV_true, all_EDV_pred)\n",
        "ESV_r2 = r2_score(all_ESV_true, all_ESV_pred)\n",
        "\n",
        "print(\"\\nAdditional Metrics:\")\n",
        "print(f\"EF  - RMSE: {EF_rmse:.4f}, R¬≤: {EF_r2:.4f}\")\n",
        "print(f\"EDV - RMSE: {EDV_rmse:.4f}, R¬≤: {EDV_r2:.4f}\")\n",
        "print(f\"ESV - RMSE: {ESV_rmse:.4f}, R¬≤: {ESV_r2:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Plot training curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Total loss\n",
        "axes[0, 0].plot(train_history['total_loss'], label='Train')\n",
        "axes[0, 0].plot(val_history['total_loss'], label='Val')\n",
        "axes[0, 0].set_title('Total Loss')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# EF loss\n",
        "axes[0, 1].plot(train_history['EF_loss'], label='Train')\n",
        "axes[0, 1].plot(val_history['EF_loss'], label='Val')\n",
        "axes[0, 1].set_title('EF Loss')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Loss')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# EDV loss\n",
        "axes[1, 0].plot(train_history['EDV_loss'], label='Train')\n",
        "axes[1, 0].plot(val_history['EDV_loss'], label='Val')\n",
        "axes[1, 0].set_title('EDV Loss')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# ESV loss\n",
        "axes[1, 1].plot(train_history['ESV_loss'], label='Train')\n",
        "axes[1, 1].plot(val_history['ESV_loss'], label='Val')\n",
        "axes[1, 1].set_title('ESV Loss')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "training_curves_path = os.path.join(OUTPUT_DIR, 'training_curves.png')\n",
        "plt.savefig(training_curves_path, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training curves saved to '{training_curves_path}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Plot predictions vs ground truth\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# EF\n",
        "axes[0].scatter(all_EF_true, all_EF_pred, alpha=0.5)\n",
        "axes[0].plot([all_EF_true.min(), all_EF_true.max()], \n",
        "             [all_EF_true.min(), all_EF_true.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('True EF')\n",
        "axes[0].set_ylabel('Predicted EF')\n",
        "axes[0].set_title(f'EF (MAE: {EF_mae:.4f}, R¬≤: {EF_r2:.4f})')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# EDV\n",
        "axes[1].scatter(all_EDV_true, all_EDV_pred, alpha=0.5)\n",
        "axes[1].plot([all_EDV_true.min(), all_EDV_true.max()], \n",
        "             [all_EDV_true.min(), all_EDV_true.max()], 'r--', lw=2)\n",
        "axes[1].set_xlabel('True EDV')\n",
        "axes[1].set_ylabel('Predicted EDV')\n",
        "axes[1].set_title(f'EDV (MAE: {EDV_mae:.4f}, R¬≤: {EDV_r2:.4f})')\n",
        "axes[1].grid(True)\n",
        "\n",
        "# ESV\n",
        "axes[2].scatter(all_ESV_true, all_ESV_pred, alpha=0.5)\n",
        "axes[2].plot([all_ESV_true.min(), all_ESV_true.max()], \n",
        "             [all_ESV_true.min(), all_ESV_true.max()], 'r--', lw=2)\n",
        "axes[2].set_xlabel('True ESV')\n",
        "axes[2].set_ylabel('Predicted ESV')\n",
        "axes[2].set_title(f'ESV (MAE: {ESV_mae:.4f}, R¬≤: {ESV_r2:.4f})')\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "predictions_plot_path = os.path.join(OUTPUT_DIR, 'predictions_vs_ground_truth.png')\n",
        "plt.savefig(predictions_plot_path, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Predictions vs ground truth plots saved to '{predictions_plot_path}'\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
