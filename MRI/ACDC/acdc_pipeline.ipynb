{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ACDC MRI Disease Classification Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib  # noqa: F401 (used in visualization helpers)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from dataset import ACDCDataset\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "\n",
        "def get_processed_dir() -> Path:\n",
        "    external = Path(\"/Volumes/Crucial X6/medical_ai_extra/processed\")\n",
        "    if external.exists():\n",
        "        return external\n",
        "    project_root = Path.cwd()\n",
        "    if (project_root / \"database\" / \"processed\").exists():\n",
        "        return project_root / \"database\" / \"processed\"\n",
        "    raise FileNotFoundError(\"Could not locate processed directory.\")\n",
        "\n",
        "\n",
        "def seed_everything(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "seed_everything(42)\n",
        "\n",
        "PROCESSED_DIR = get_processed_dir()\n",
        "META_CSV = PROCESSED_DIR / \"meta.csv\"\n",
        "SPLITS_JSON = PROCESSED_DIR / \"splits.json\"\n",
        "BEST_MODEL_PATH = Path(\"best_model.pt\")\n",
        "EVAL_RESULTS_PATH = Path(\"evaluation_results.json\")\n",
        "CONF_MATRIX_PATH = Path(\"confusion_matrix.png\")\n",
        "LABEL_NAMES = {0: \"NOR\", 1: \"MINF\", 2: \"DCM\", 3: \"HCM\", 4: \"RV\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed directory: /Volumes/Crucial X6/medical_ai_extra/processed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patient_id</th>\n",
              "      <th>filepath</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>ED</th>\n",
              "      <th>ES</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>patient001</td>\n",
              "      <td>/Volumes/Crucial X6/medical_ai_extra/processed...</td>\n",
              "      <td>184.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>patient002</td>\n",
              "      <td>/Volumes/Crucial X6/medical_ai_extra/processed...</td>\n",
              "      <td>160.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>patient003</td>\n",
              "      <td>/Volumes/Crucial X6/medical_ai_extra/processed...</td>\n",
              "      <td>165.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>patient004</td>\n",
              "      <td>/Volumes/Crucial X6/medical_ai_extra/processed...</td>\n",
              "      <td>159.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>patient005</td>\n",
              "      <td>/Volumes/Crucial X6/medical_ai_extra/processed...</td>\n",
              "      <td>165.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   patient_id                                           filepath  height  \\\n",
              "0  patient001  /Volumes/Crucial X6/medical_ai_extra/processed...   184.0   \n",
              "1  patient002  /Volumes/Crucial X6/medical_ai_extra/processed...   160.0   \n",
              "2  patient003  /Volumes/Crucial X6/medical_ai_extra/processed...   165.0   \n",
              "3  patient004  /Volumes/Crucial X6/medical_ai_extra/processed...   159.0   \n",
              "4  patient005  /Volumes/Crucial X6/medical_ai_extra/processed...   165.0   \n",
              "\n",
              "   weight  ED  ES  label  \n",
              "0    95.0   1  12      2  \n",
              "1    70.0   1  12      2  \n",
              "2    77.0   1  15      2  \n",
              "3    46.0   1  15      2  \n",
              "4    77.0   1  13      2  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples per split:\n",
            "  train: 80\n",
            "  val: 20\n",
            "  test: 50\n"
          ]
        }
      ],
      "source": [
        "metadata_df = pd.read_csv(META_CSV)\n",
        "with SPLITS_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    splits = json.load(f)\n",
        "\n",
        "print(\"Processed directory:\", PROCESSED_DIR)\n",
        "display(metadata_df.head())\n",
        "\n",
        "split_counts = {split: len(ids) for split, ids in splits.items()}\n",
        "print(\"Samples per split:\")\n",
        "for split, count in split_counts.items():\n",
        "    print(f\"  {split}: {count}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ACDCNet(\n",
            "  (mri_encoder): MRIEncoder3D(\n",
            "    (layers): Sequential(\n",
            "      (0): Conv3DBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv3d(30, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): MaxPool3d(kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "      (2): Conv3DBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (3): MaxPool3d(kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Conv3DBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (5): MaxPool3d(kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "      (6): Conv3DBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (global_pool): AdaptiveAvgPool3d(output_size=1)\n",
            "  )\n",
            "  (meta_encoder): MetaEncoder(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=4, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Linear(in_features=32, out_features=64, bias=True)\n",
            "      (4): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): FusionClassifier(\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=320, out_features=256, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=5, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Total trainable parameters: 1,274,373\n"
          ]
        }
      ],
      "source": [
        "class Conv3DBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class MRIEncoder3D(nn.Module):\n",
        "    def __init__(self, in_channels: int = 30, base_channels: int = 32):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            Conv3DBlock(in_channels, base_channels),\n",
        "            nn.MaxPool3d((2, 2, 1)),\n",
        "            Conv3DBlock(base_channels, base_channels * 2),\n",
        "            nn.MaxPool3d((2, 2, 1)),\n",
        "            Conv3DBlock(base_channels * 2, base_channels * 4),\n",
        "            nn.MaxPool3d((2, 2, 1)),\n",
        "            Conv3DBlock(base_channels * 4, base_channels * 8),\n",
        "        )\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.layers(x)\n",
        "        x = self.global_pool(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "class MetaEncoder(nn.Module):\n",
        "    def __init__(self, input_dim: int = 4, hidden_dim: int = 32, output_dim: int = 64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class FusionClassifier(nn.Module):\n",
        "    def __init__(self, mri_dim: int, meta_dim: int, num_classes: int = 5, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        fusion_dim = mri_dim + meta_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, mri_feat: torch.Tensor, meta_feat: torch.Tensor) -> torch.Tensor:\n",
        "        fused = torch.cat([mri_feat, meta_feat], dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "\n",
        "class ACDCNet(nn.Module):\n",
        "    def __init__(self, num_classes: int = 5):\n",
        "        super().__init__()\n",
        "        self.mri_encoder = MRIEncoder3D()\n",
        "        self.meta_encoder = MetaEncoder()\n",
        "        self.classifier = FusionClassifier(mri_dim=256, meta_dim=64, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, image: torch.Tensor, metadata: torch.Tensor) -> torch.Tensor:\n",
        "        mri_feat = self.mri_encoder(image)\n",
        "        meta_feat = self.meta_encoder(metadata)\n",
        "        return self.classifier(mri_feat, meta_feat)\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "model = ACDCNet(num_classes=len(LABEL_NAMES)).to(device)\n",
        "print(model)\n",
        "print(f\"Total trainable parameters: {count_parameters(model):,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val/Test sizes: 80 / 20 / 50\n"
          ]
        }
      ],
      "source": [
        "TARGET_DEPTH = 216\n",
        "\n",
        "def adjust_depth(volume: torch.Tensor, target_depth: int = TARGET_DEPTH) -> torch.Tensor:\n",
        "    depth = volume.shape[0]\n",
        "    if depth == target_depth:\n",
        "        return volume\n",
        "    if depth > target_depth:\n",
        "        start = (depth - target_depth) // 2\n",
        "        end = start + target_depth\n",
        "        return volume[start:end, :, :, :]\n",
        "    pad_before = (target_depth - depth) // 2\n",
        "    pad_after = target_depth - depth - pad_before\n",
        "    padding = (0, 0, 0, 0, 0, 0, pad_before, pad_after)\n",
        "    return F.pad(volume, padding)\n",
        "\n",
        "\n",
        "def image_transform(volume: torch.Tensor) -> torch.Tensor:\n",
        "    if volume.ndim != 4:\n",
        "        raise ValueError(\"Expected 4D volume (D, H, W, T)\")\n",
        "    volume = adjust_depth(volume)\n",
        "    # Permute to (T, D, H, W) to align with Conv3d input ordering (C, D, H, W)\n",
        "    return volume.permute(3, 0, 1, 2)\n",
        "\n",
        "\n",
        "def create_dataset(split: str) -> ACDCDataset:\n",
        "    return ACDCDataset(\n",
        "        meta_csv=META_CSV,\n",
        "        splits_json=SPLITS_JSON,\n",
        "        subset=split,\n",
        "        image_transform=image_transform,\n",
        "    )\n",
        "\n",
        "\n",
        "TARGET_CHANNELS = 30\n",
        "\n",
        "\n",
        "def pad_trim_channels(image: torch.Tensor, target_channels: int = TARGET_CHANNELS) -> torch.Tensor:\n",
        "    c, d, h, w = image.shape\n",
        "    if c == target_channels:\n",
        "        return image\n",
        "    if c > target_channels:\n",
        "        start = (c - target_channels) // 2\n",
        "        end = start + target_channels\n",
        "        return image[start:end, :, :, :]\n",
        "    pad_before = (target_channels - c) // 2\n",
        "    pad_after = target_channels - c - pad_before\n",
        "    padding = (0, 0, 0, 0, 0, 0, pad_before, pad_after)\n",
        "    return F.pad(image, padding)\n",
        "\n",
        "\n",
        "def pad_image(image: torch.Tensor, target_shape: tuple[int, int, int, int]) -> torch.Tensor:\n",
        "    image = pad_trim_channels(image)\n",
        "    c, d, h, w = image.shape\n",
        "    tc, td, th, tw = target_shape\n",
        "    output = torch.zeros(target_shape, dtype=image.dtype)\n",
        "    output[:c, :d, :h, :w] = image[:tc, :td, :th, :tw]\n",
        "    return output\n",
        "\n",
        "\n",
        "def pad_collate(batch):\n",
        "    images = [item[0][0] for item in batch]\n",
        "    metadata = [item[0][1] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    max_dims = [\n",
        "        max(img.shape[dim] for img in images) for dim in range(4)\n",
        "    ]\n",
        "    max_dims[0] = TARGET_CHANNELS\n",
        "    padded_images = [pad_image(img, tuple(max_dims)) for img in images]\n",
        "\n",
        "    images_tensor = torch.stack(padded_images)\n",
        "    metadata_tensor = torch.stack(metadata).float()\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    return (images_tensor, metadata_tensor), labels_tensor\n",
        "\n",
        "\n",
        "train_dataset = create_dataset(\"train\")\n",
        "val_dataset = create_dataset(\"val\")\n",
        "test_dataset = create_dataset(\"test\")\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "# Use single-process loading inside notebooks to avoid fork/pickle issues.\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "def make_loader(dataset: ACDCDataset, shuffle: bool) -> DataLoader:\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=pad_collate,\n",
        "    )\n",
        "\n",
        "train_loader = make_loader(train_dataset, shuffle=True)\n",
        "val_loader = make_loader(val_dataset, shuffle=False)\n",
        "test_loader = make_loader(test_dataset, shuffle=False)\n",
        "\n",
        "print(\n",
        "    f\"Train/Val/Test sizes: {len(train_dataset)} / {len(val_dataset)} / {len(test_dataset)}\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç DIAGNOSTIC CHECKLIST - Run this before training to debug 0% validation accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üîç DIAGNOSTIC CHECKLIST\n",
            "============================================================\n",
            "\n",
            "1Ô∏è‚É£  LABEL DISTRIBUTION\n",
            "Train labels: Counter({2: 16, 3: 16, 1: 16, 0: 16, 4: 16})\n",
            "Val labels  : Counter({2: 4, 3: 4, 1: 4, 0: 4, 4: 4})\n",
            "Test labels : Counter({2: 10, 0: 10, 1: 10, 3: 10, 4: 10})\n",
            "‚úÖ Val contains all classes\n",
            "\n",
            "2Ô∏è‚É£  MODEL OUTPUT DIMENSION\n",
            "Final layer: Linear(in_features=320, out_features=256, bias=True)\n",
            "‚ùå Wrong output dimension: 256, expected 5\n",
            "Final layer: Linear(in_features=256, out_features=5, bias=True)\n",
            "‚úÖ Output dimension correct: 5\n",
            "\n",
            "3Ô∏è‚É£  DATASET OUTPUT SHAPES\n",
            "Image shape: torch.Size([30, 216, 256, 10])\n",
            "Meta shape : torch.Size([4])\n",
            "Label      : 2 (DCM)\n",
            "\n",
            "4Ô∏è‚É£  BATCH UNPACKING\n",
            "‚úÖ Batch shapes - Images: torch.Size([2, 30, 216, 256, 10]), Metas: torch.Size([2, 4]), Labels: torch.Size([2])\n",
            "\n",
            "5Ô∏è‚É£  NORMALIZATION CHECK\n",
            "Train mean: 0.0389, std: 0.9915\n",
            "Val   mean: -0.0000, std: 1.0000\n",
            "\n",
            "6Ô∏è‚É£  METADATA IN BATCHES\n",
            "‚úÖ Train batch meta shape: torch.Size([2, 4]), mean: 68.5000\n",
            "‚úÖ Val batch meta shape  : torch.Size([2, 4]), mean: 68.7500\n",
            "\n",
            "7Ô∏è‚É£  OVERFIT TEST (40 steps on single batch)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m40\u001b[39m):\n\u001b[32m     88\u001b[39m     optimizer_test.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     out = \u001b[43mmodel_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetas_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     loss = criterion_test(out, labels_test)\n\u001b[32m     91\u001b[39m     loss.backward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mACDCNet.forward\u001b[39m\u001b[34m(self, image, metadata)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: torch.Tensor, metadata: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     mri_feat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmri_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     meta_feat = \u001b[38;5;28mself\u001b[39m.meta_encoder(metadata)\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classifier(mri_feat, meta_feat)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mMRIEncoder3D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.global_pool(x)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.flatten(x, \u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mConv3DBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:610\u001b[39m, in \u001b[36mConv3d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:605\u001b[39m, in \u001b[36mConv3d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    594\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv3d(\n\u001b[32m    595\u001b[39m         F.pad(\n\u001b[32m    596\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    603\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    604\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üîç DIAGNOSTIC CHECKLIST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. CHECK LABEL DISTRIBUTION\n",
        "print(\"\\n1Ô∏è‚É£  LABEL DISTRIBUTION\")\n",
        "train_labels = [int(train_dataset[i][1]) for i in range(len(train_dataset))]\n",
        "val_labels = [int(val_dataset[i][1]) for i in range(len(val_dataset))]\n",
        "test_labels = [int(test_dataset[i][1]) for i in range(len(test_dataset))]\n",
        "\n",
        "print(\"Train labels:\", Counter(train_labels))\n",
        "print(\"Val labels  :\", Counter(val_labels))\n",
        "print(\"Test labels :\", Counter(test_labels))\n",
        "\n",
        "if len(set(val_labels)) < len(LABEL_NAMES):\n",
        "    print(\"‚ö†Ô∏è  WARNING: Val set missing some classes!\")\n",
        "else:\n",
        "    print(\"‚úÖ Val contains all classes\")\n",
        "\n",
        "# 2. CHECK MODEL OUTPUT DIMENSION\n",
        "print(\"\\n2Ô∏è‚É£  MODEL OUTPUT DIMENSION\")\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear) and name.startswith(\"classifier\"):\n",
        "        print(f\"Final layer: {module}\")\n",
        "        if module.out_features == len(LABEL_NAMES):\n",
        "            print(f\"‚úÖ Output dimension correct: {module.out_features}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Wrong output dimension: {module.out_features}, expected {len(LABEL_NAMES)}\")\n",
        "\n",
        "# 3. CHECK DATASET OUTPUT SHAPES\n",
        "print(\"\\n3Ô∏è‚É£  DATASET OUTPUT SHAPES\")\n",
        "(img, meta), label = train_dataset[0]\n",
        "print(f\"Image shape: {img.shape}\")\n",
        "print(f\"Meta shape : {meta.shape}\")\n",
        "print(f\"Label      : {label.item()} ({LABEL_NAMES[label.item()]})\")\n",
        "\n",
        "if img.ndim not in [4, 5]:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Image should be 4D or 5D, got {img.ndim}D\")\n",
        "if meta.shape[0] == 0:\n",
        "    print(\"‚ùå ERROR: Metadata is empty!\")\n",
        "\n",
        "# 4. CHECK BATCH UNPACKING\n",
        "print(\"\\n4Ô∏è‚É£  BATCH UNPACKING\")\n",
        "for batch in val_loader:\n",
        "    (images, metas), labels = batch\n",
        "    print(f\"‚úÖ Batch shapes - Images: {images.shape}, Metas: {metas.shape}, Labels: {labels.shape}\")\n",
        "    break\n",
        "\n",
        "# 5. CHECK NORMALIZATION\n",
        "print(\"\\n5Ô∏è‚É£  NORMALIZATION CHECK\")\n",
        "img_train = train_dataset[0][0][0]\n",
        "img_val = val_dataset[0][0][0] if len(val_dataset) > 0 else None\n",
        "\n",
        "print(f\"Train mean: {img_train.mean().item():.4f}, std: {img_train.std().item():.4f}\")\n",
        "if img_val is not None:\n",
        "    print(f\"Val   mean: {img_val.mean().item():.4f}, std: {img_val.std().item():.4f}\")\n",
        "    if abs(img_train.mean() - img_val.mean()) > 0.5:\n",
        "        print(\"‚ö†Ô∏è  WARNING: Train/val normalization differs significantly\")\n",
        "\n",
        "# 6. CHECK METADATA IN BATCHES\n",
        "print(\"\\n6Ô∏è‚É£  METADATA IN BATCHES\")\n",
        "for batch in train_loader:\n",
        "    (i, m), l = batch\n",
        "    print(f\"‚úÖ Train batch meta shape: {m.shape}, mean: {m.mean().item():.4f}\")\n",
        "    break\n",
        "\n",
        "for batch in val_loader:\n",
        "    (i, m), l = batch\n",
        "    print(f\"‚úÖ Val batch meta shape  : {m.shape}, mean: {m.mean().item():.4f}\")\n",
        "    break\n",
        "\n",
        "# 7. OVERFIT TEST\n",
        "print(\"\\n7Ô∏è‚É£  OVERFIT TEST (40 steps on single batch)\")\n",
        "model_copy = ACDCNet(num_classes=len(LABEL_NAMES)).to(device)\n",
        "optimizer_test = torch.optim.Adam(model_copy.parameters(), lr=1e-3)\n",
        "criterion_test = nn.CrossEntropyLoss()\n",
        "\n",
        "test_batch = next(iter(train_loader))\n",
        "(images_test, metas_test), labels_test = test_batch\n",
        "images_test = images_test.float().to(device)\n",
        "metas_test = metas_test.float().to(device)\n",
        "labels_test = labels_test.to(device)\n",
        "\n",
        "initial_loss = None\n",
        "for step in range(40):\n",
        "    optimizer_test.zero_grad()\n",
        "    out = model_copy(images_test, metas_test)\n",
        "    loss = criterion_test(out, labels_test)\n",
        "    loss.backward()\n",
        "    optimizer_test.step()\n",
        "    if step == 0:\n",
        "        initial_loss = loss.item()\n",
        "    if step % 10 == 0 or step == 39:\n",
        "        print(f\"  Step {step:2d}: loss = {loss.item():.4f}\")\n",
        "\n",
        "if initial_loss is not None and loss.item() < initial_loss * 0.5:\n",
        "    print(\"‚úÖ Loss decreased - architecture is correct\")\n",
        "else:\n",
        "    print(\"‚ùå Loss did not decrease significantly - check architecture!\")\n",
        "\n",
        "# 8. CHECK PATIENT IDS MATCH SPLITS\n",
        "print(\"\\n8Ô∏è‚É£  PATIENT IDS MATCH SPLITS\")\n",
        "meta_df = pd.read_csv(META_CSV)\n",
        "with SPLITS_JSON.open(\"r\") as f:\n",
        "    splits_check = json.load(f)\n",
        "\n",
        "train_ids_set = set(splits_check['train'])\n",
        "val_ids_set = set(splits_check['val'])\n",
        "meta_ids_set = set(meta_df['patient_id'])\n",
        "\n",
        "missing_in_meta = val_ids_set - meta_ids_set\n",
        "if missing_in_meta:\n",
        "    print(f\"‚ùå Val patients not in meta.csv: {missing_in_meta}\")\n",
        "else:\n",
        "    print(\"‚úÖ All val patients found in meta.csv\")\n",
        "\n",
        "duplicates = meta_df['patient_id'].value_counts()\n",
        "if (duplicates > 1).any():\n",
        "    print(f\"‚ö†Ô∏è  Duplicate patient IDs: {duplicates[duplicates > 1]}\")\n",
        "else:\n",
        "    print(\"‚úÖ No duplicate patient IDs\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Diagnostic complete!\")\n",
        "print(\"=\" * 60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/omarelsisi/Downloads/medical_ai/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01/15\n",
            "  Train -> loss: 1.6146, acc: 0.150\n",
            "  Val   -> loss: 1.5558, acc: 0.300 (new best)\n",
            "  LR: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02/15\n",
            "  Train -> loss: 1.5874, acc: 0.325\n",
            "  Val   -> loss: 1.5490, acc: 0.250\n",
            "  LR: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03/15\n",
            "  Train -> loss: 1.5539, acc: 0.325\n",
            "  Val   -> loss: 1.5497, acc: 0.250\n",
            "  LR: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04/15\n",
            "  Train -> loss: 1.5220, acc: 0.362\n",
            "  Val   -> loss: 1.5551, acc: 0.200\n",
            "  LR: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05/15\n",
            "  Train -> loss: 1.5507, acc: 0.312\n",
            "  Val   -> loss: 1.4949, acc: 0.200\n",
            "  LR: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06/15\n",
            "  Train -> loss: 1.4947, acc: 0.275\n",
            "  Val   -> loss: 1.5471, acc: 0.400 (new best)\n",
            "  LR: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07/15\n",
            "  Train -> loss: 1.5229, acc: 0.350\n",
            "  Val   -> loss: 1.5471, acc: 0.250\n",
            "  LR: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08/15\n",
            "  Train -> loss: 1.4577, acc: 0.425\n",
            "  Val   -> loss: 1.5071, acc: 0.250\n",
            "  LR: 0.000050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09/15\n",
            "  Train -> loss: 1.4524, acc: 0.375\n",
            "  Val   -> loss: 1.7247, acc: 0.200\n",
            "  LR: 0.000050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/15\n",
            "  Train -> loss: 1.4355, acc: 0.388\n",
            "  Val   -> loss: 1.5634, acc: 0.300\n",
            "  LR: 0.000050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Val 11:  30%|‚ñà‚ñà‚ñà       | 3/10 [00:32<01:13, 10.51s/it]   "
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "loss_fn = CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2, verbose=True)\n",
        "scaler = GradScaler(enabled=device.type == \"cuda\")\n",
        "\n",
        "NUM_EPOCHS = 15\n",
        "history: List[Dict] = []\n",
        "best_val_acc = 0.0\n",
        "\n",
        "\n",
        "def run_epoch(loader: DataLoader, train: bool, description: str) -> Dict[str, float]:\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    iterator = tqdm(loader, desc=description, leave=False)\n",
        "    for (images, metadata), labels in iterator:\n",
        "        images = images.float().to(device, non_blocking=True)\n",
        "        metadata = metadata.float().to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(enabled=device.type == \"cuda\"):\n",
        "            logits = model(images, metadata)\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return {\n",
        "        \"loss\": epoch_loss / max(total, 1),\n",
        "        \"acc\": correct / max(total, 1),\n",
        "    }\n",
        "\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_metrics = run_epoch(train_loader, train=True, description=f\"Train {epoch}\")\n",
        "    with torch.no_grad():\n",
        "        val_metrics = run_epoch(val_loader, train=False, description=f\"Val {epoch}\")\n",
        "\n",
        "    scheduler.step(val_metrics[\"loss\"])\n",
        "\n",
        "    history.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_metrics[\"loss\"],\n",
        "            \"train_acc\": train_metrics[\"acc\"],\n",
        "            \"val_loss\": val_metrics[\"loss\"],\n",
        "            \"val_acc\": val_metrics[\"acc\"],\n",
        "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if val_metrics[\"acc\"] > best_val_acc:\n",
        "        best_val_acc = val_metrics[\"acc\"]\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        best_tag = \" (new best)\"\n",
        "    else:\n",
        "        best_tag = \"\"\n",
        "\n",
        "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d}/{NUM_EPOCHS}\\n\"\n",
        "        f\"  Train -> loss: {train_metrics['loss']:.4f}, acc: {train_metrics['acc']:.3f}\\n\"\n",
        "        f\"  Val   -> loss: {val_metrics['loss']:.4f}, acc: {val_metrics['acc']:.3f}{best_tag}\\n\"\n",
        "        f\"  LR: {current_lr:.6f}\"\n",
        "    )\n",
        "\n",
        "metrics_df = pd.DataFrame(history)\n",
        "display(metrics_df.tail())\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "metrics_df.plot(x=\"epoch\", y=[\"train_loss\", \"val_loss\"], ax=axes[0])\n",
        "axes[0].set_title(\"Loss\")\n",
        "metrics_df.plot(x=\"epoch\", y=[\"train_acc\", \"val_acc\"], ax=axes[1])\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].set_title(\"Accuracy\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_best_model(model: nn.Module, path: Path) -> None:\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader):\n",
        "    all_preds: List[int] = []\n",
        "    all_labels: List[int] = []\n",
        "    metas: List[np.ndarray] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (images, metadata), labels in loader:\n",
        "            images = images.float().to(device)\n",
        "            metadata = metadata.float().to(device)\n",
        "            logits = model(images, metadata)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "            metas.extend(metadata.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels), metas\n",
        "\n",
        "\n",
        "load_best_model(model, BEST_MODEL_PATH)\n",
        "preds, labels_np, metas = evaluate(model, test_loader)\n",
        "\n",
        "acc = accuracy_score(labels_np, preds)\n",
        "macro_f1 = f1_score(labels_np, preds, average=\"macro\")\n",
        "micro_f1 = f1_score(labels_np, preds, average=\"micro\")\n",
        "\n",
        "cm = confusion_matrix(labels_np, preds, labels=list(LABEL_NAMES.keys()))\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "im = ax.imshow(cm, cmap=\"Blues\")\n",
        "ax.set_xticks(range(len(LABEL_NAMES)))\n",
        "ax.set_yticks(range(len(LABEL_NAMES)))\n",
        "ax.set_xticklabels(LABEL_NAMES.values())\n",
        "ax.set_yticklabels(LABEL_NAMES.values())\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
        "fig.colorbar(im)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(CONF_MATRIX_PATH)\n",
        "plt.show()\n",
        "\n",
        "per_class_acc = {}\n",
        "for label_idx, label_name in LABEL_NAMES.items():\n",
        "    mask = labels_np == label_idx\n",
        "    if mask.any():\n",
        "        per_class_acc[label_name] = float((preds[mask] == labels_np[mask]).mean())\n",
        "    else:\n",
        "        per_class_acc[label_name] = float(\"nan\")\n",
        "\n",
        "evaluation_results = {\n",
        "    \"accuracy\": float(acc),\n",
        "    \"macro_f1\": float(macro_f1),\n",
        "    \"micro_f1\": float(micro_f1),\n",
        "    \"per_class_accuracy\": per_class_acc,\n",
        "    \"best_model_path\": str(BEST_MODEL_PATH.resolve()),\n",
        "}\n",
        "\n",
        "with EVAL_RESULTS_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(evaluation_results, f, indent=2)\n",
        "\n",
        "print(json.dumps(evaluation_results, indent=2))\n",
        "\n",
        "print(\"\\nRandom sample predictions:\")\n",
        "sample_count = min(5, len(preds))\n",
        "if sample_count == 0:\n",
        "    print(\"No test samples available.\")\n",
        "else:\n",
        "    indices = random.sample(range(len(preds)), k=sample_count)\n",
        "    for idx in indices:\n",
        "        meta_vals = metas[idx]\n",
        "        print(\n",
        "            f\"Sample {idx}: metadata={meta_vals.tolist()} | \"\n",
        "            f\"pred={LABEL_NAMES[preds[idx]]} vs actual={LABEL_NAMES[labels_np[idx]]}\"\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_volume_slices(volume: np.ndarray, num_slices: int = 6) -> None:\n",
        "    if volume.ndim != 4:\n",
        "        raise ValueError(\"Volume must be 4D (D, H, W, T)\")\n",
        "    depth = volume.shape[0]\n",
        "    indices = np.linspace(0, depth - 1, num_slices, dtype=int)\n",
        "    fig, axes = plt.subplots(1, num_slices, figsize=(15, 3))\n",
        "    for ax, idx in zip(axes, indices):\n",
        "        ax.imshow(volume[idx, :, :, 0], cmap=\"gray\")\n",
        "        ax.set_title(f\"Slice {idx}\")\n",
        "        ax.axis(\"off\")\n",
        "    plt.suptitle(\"Volume slices (first time frame)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_single_slice(volume: np.ndarray, slice_index: int = 0, time_index: int = 0) -> None:\n",
        "    if volume.ndim != 4:\n",
        "        raise ValueError(\"Volume must be 4D (D, H, W, T)\")\n",
        "    slice_index = np.clip(slice_index, 0, volume.shape[0] - 1)\n",
        "    time_index = np.clip(time_index, 0, volume.shape[3] - 1)\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(volume[slice_index, :, :, time_index], cmap=\"gray\")\n",
        "    plt.title(f\"Slice {slice_index} | Time {time_index}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_metadata(meta_vector: np.ndarray) -> None:\n",
        "    labels = [\"Height\", \"Weight\", \"ED\", \"ES\"]\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    plt.bar(labels, meta_vector)\n",
        "    plt.title(\"Metadata values\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with EVAL_RESULTS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    final_metrics = json.load(f)\n",
        "\n",
        "print(\"Pipeline completed successfully.\")\n",
        "print(f\"Best model saved at: {BEST_MODEL_PATH.resolve()}\")\n",
        "print(\"Final test metrics:\")\n",
        "print(json.dumps(final_metrics, indent=2))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
