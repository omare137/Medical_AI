{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTB-XL Deep Learning Classification\n",
    "\n",
    "**Goal:** Beat the classical ML baseline (Macro F1 \u2248 0.69) using deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Target Superclasses\n",
    "\n",
    "| Code | Description |\n",
    "|------|-------------|\n",
    "| **NORM** | Normal ECG |\n",
    "| **MI** | Myocardial Infarction |\n",
    "| **STTC** | ST/T Changes |\n",
    "| **CD** | Conduction Disturbance |\n",
    "| **HYP** | Hypertrophy |\n",
    "\n",
    "---\n",
    "\n",
    "## Models\n",
    "\n",
    "| Model | Architecture | Description |\n",
    "|-------|--------------|-------------|\n",
    "| **Model A** | 1D CNN | Residual conv blocks + GAP |\n",
    "| **Model B** | CNN + BiLSTM | CNN features \u2192 BiLSTM temporal |\n",
    "| **Model C** | CNN + Attention | Channel/temporal attention |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences from Baseline\n",
    "\n",
    "- **100 Hz signals** \u2014 optimized for local training (5x faster than 500 Hz)\n",
    "- **Raw waveforms** \u2014 no hand-crafted features\n",
    "- **Deep learning** \u2014 learns hierarchical representations\n",
    "- **Official splits** \u2014 strat_fold 1-8/9/10 for train/val/test\n",
    "- **MPS Support** \u2014 Apple Silicon GPU acceleration when available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Local Setup\n",
    "\n",
    "Verify dependencies and check for GPU/MPS acceleration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u26a0\ufe0f No GPU detected - using CPU (training will be slower)\n",
      "\u2705 wfdb package available\n",
      "\n",
      "\u2705 Local setup complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOCAL SETUP\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check for hardware acceleration\n",
    "if torch.cuda.is_available():\n",
    "    print(f'\u2705 CUDA GPU available: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "elif torch.backends.mps.is_available():\n",
    "    print('\u2705 Apple MPS (Metal) acceleration available!')\n",
    "    print('   Using Apple Silicon GPU for faster training.')\n",
    "else:\n",
    "    print('\u26a0\ufe0f No GPU detected - using CPU (training will be slower)')\n",
    "\n",
    "# Verify wfdb is installed\n",
    "try:\n",
    "    import wfdb\n",
    "    print('\u2705 wfdb package available')\n",
    "except ImportError:\n",
    "    print('\u274c wfdb not installed. Run: pip install wfdb')\n",
    "\n",
    "print('\\n\u2705 Local setup complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial X6/medical_ai/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "\n",
    "from scipy import signal as scipy_signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print('All imports successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Data path found: /Volumes/Crucial X6/medical_ai/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\n",
      "Data path: /Volumes/Crucial X6/medical_ai/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\n",
      "Sampling rate: 100 Hz (using records100/)\n",
      "Sequence length: 1000 samples\n",
      "Batch size: 32\n",
      "Baseline Macro F1 to beat: 0.69\n",
      "\n",
      "\u26a1 Local optimization: Using 100 Hz signals for 5x faster training\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION (OPTIMIZED FOR LOCAL)\n",
    "# ============================================================\n",
    "\n",
    "# Paths (Local)\n",
    "DATA_PATH = Path('/Volumes/Crucial X6/medical_ai/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1')\n",
    "OUTPUT_PATH = Path('/Volumes/Crucial X6/medical_ai/ptb-xl/outputs_dl')\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify path exists\n",
    "if DATA_PATH.exists():\n",
    "    print(f'\u2705 Data path found: {DATA_PATH}')\n",
    "else:\n",
    "    print(f'\u274c Data path NOT found: {DATA_PATH}')\n",
    "    print('   Please check the path!')\n",
    "\n",
    "# Target superclasses\n",
    "SUPERCLASSES = ['NORM', 'MI', 'STTC', 'CD', 'HYP']\n",
    "N_CLASSES = len(SUPERCLASSES)\n",
    "\n",
    "# ECG parameters - OPTIMIZED FOR LOCAL (5x faster, 5x less memory)\n",
    "SAMPLING_RATE = 100  # Hz (using records100/ instead of records500/)\n",
    "DURATION = 10  # seconds\n",
    "SEQ_LEN = SAMPLING_RATE * DURATION  # 1000 samples (was 5000)\n",
    "N_LEADS = 12\n",
    "\n",
    "# Training parameters - OPTIMIZED FOR LOCAL\n",
    "BATCH_SIZE = 32  # Reduced for CPU/MPS memory (was 64)\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 10\n",
    "\n",
    "# Baseline to beat\n",
    "BASELINE_MACRO_F1 = 0.69\n",
    "\n",
    "print(f'Data path: {DATA_PATH}')\n",
    "print(f'Sampling rate: {SAMPLING_RATE} Hz (using records100/)')\n",
    "print(f'Sequence length: {SEQ_LEN} samples')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Baseline Macro F1 to beat: {BASELINE_MACRO_F1}')\n",
    "print(f'\\n\u26a1 Local optimization: Using 100 Hz signals for 5x faster training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Construction\n",
    "\n",
    "Using official PTB-XL splits:\n",
    "- **Train:** strat_fold 1-8\n",
    "- **Validation:** strat_fold 9\n",
    "- **Test:** strat_fold 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21,837 ECG records\n",
      "Diagnostic SCP codes: 44\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD METADATA & CREATE TARGETS\n",
    "# ============================================================\n",
    "\n",
    "# Load database\n",
    "df = pd.read_csv(DATA_PATH / 'ptbxl_database.csv')\n",
    "print(f'Loaded {len(df):,} ECG records')\n",
    "\n",
    "# Parse scp_codes\n",
    "def parse_scp_codes(scp_str):\n",
    "    try:\n",
    "        return ast.literal_eval(scp_str)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "df['scp_codes_dict'] = df['scp_codes'].apply(parse_scp_codes)\n",
    "\n",
    "# Load SCP statements\n",
    "scp_df = pd.read_csv(DATA_PATH / 'scp_statements.csv', index_col=0)\n",
    "scp_diagnostic = scp_df[scp_df['diagnostic'] == 1.0]\n",
    "scp_to_superclass = scp_diagnostic['diagnostic_class'].to_dict()\n",
    "\n",
    "print(f'Diagnostic SCP codes: {len(scp_to_superclass)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGs with diagnostic labels: 21,417\n",
      "Label matrix shape: (21417, 5)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CREATE MULTI-LABEL TARGETS\n",
    "# ============================================================\n",
    "\n",
    "def get_superclasses(scp_codes_dict):\n",
    "    active = set()\n",
    "    for scp_code, likelihood in scp_codes_dict.items():\n",
    "        if likelihood > 0 and scp_code in scp_to_superclass:\n",
    "            superclass = scp_to_superclass[scp_code]\n",
    "            if superclass in SUPERCLASSES:\n",
    "                active.add(superclass)\n",
    "    return list(active)\n",
    "\n",
    "df['superclasses'] = df['scp_codes_dict'].apply(get_superclasses)\n",
    "\n",
    "# Filter to ECGs with at least one diagnostic label\n",
    "df_filtered = df[df['superclasses'].apply(len) > 0].copy()\n",
    "print(f'ECGs with diagnostic labels: {len(df_filtered):,}')\n",
    "\n",
    "# Create binary label matrix\n",
    "mlb = MultiLabelBinarizer(classes=SUPERCLASSES)\n",
    "y_all = mlb.fit_transform(df_filtered['superclasses'])\n",
    "\n",
    "print(f'Label matrix shape: {y_all.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OFFICIAL PTB-XL SPLITS\n",
      "============================================================\n",
      "Train (folds 1-8): 17,100 samples\n",
      "Val   (fold 9):    2,155 samples\n",
      "Test  (fold 10):   2,162 samples\n",
      "\n",
      "Class distribution (Train):\n",
      "  NORM: 7,607 (44.5%)\n",
      "  MI: 4,389 (25.7%)\n",
      "  STTC: 4,094 (23.9%)\n",
      "  CD: 3,912 (22.9%)\n",
      "  HYP: 2,121 (12.4%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# OFFICIAL PTB-XL SPLITS\n",
    "# ============================================================\n",
    "\n",
    "# Split by strat_fold\n",
    "train_mask = df_filtered['strat_fold'].isin([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "val_mask = df_filtered['strat_fold'] == 9\n",
    "test_mask = df_filtered['strat_fold'] == 10\n",
    "\n",
    "df_train = df_filtered[train_mask].reset_index(drop=True)\n",
    "df_val = df_filtered[val_mask].reset_index(drop=True)\n",
    "df_test = df_filtered[test_mask].reset_index(drop=True)\n",
    "\n",
    "y_train = y_all[train_mask.values]\n",
    "y_val = y_all[val_mask.values]\n",
    "y_test = y_all[test_mask.values]\n",
    "\n",
    "print('=' * 60)\n",
    "print('OFFICIAL PTB-XL SPLITS')\n",
    "print('=' * 60)\n",
    "print(f'Train (folds 1-8): {len(df_train):,} samples')\n",
    "print(f'Val   (fold 9):    {len(df_val):,} samples')\n",
    "print(f'Test  (fold 10):   {len(df_test):,} samples')\n",
    "\n",
    "# Class distribution\n",
    "print('\\nClass distribution (Train):')\n",
    "for i, cls in enumerate(SUPERCLASSES):\n",
    "    count = y_train[:, i].sum()\n",
    "    pct = 100 * count / len(y_train)\n",
    "    print(f'  {cls}: {count:,} ({pct:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights for imbalance handling:\n",
      "  NORM: 0.450\n",
      "  MI: 0.779\n",
      "  STTC: 0.835\n",
      "  CD: 0.874\n",
      "  HYP: 1.612\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPUTE CLASS WEIGHTS\n",
    "# ============================================================\n",
    "\n",
    "# For handling class imbalance\n",
    "class_counts = y_train.sum(axis=0)\n",
    "total = len(y_train)\n",
    "class_weights = total / (N_CLASSES * class_counts)\n",
    "class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
    "\n",
    "print('Class weights for imbalance handling:')\n",
    "for cls, w in zip(SUPERCLASSES, class_weights.cpu().numpy()):\n",
    "    print(f'  {cls}: {w:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Signal Loading & Dataset\n",
    "\n",
    "**\u26a1 Optimized for Colab:** Pre-load ALL signals into memory once to avoid slow Google Drive I/O during training.\n",
    "\n",
    "- Load 500 Hz ECG signals (all at once)\n",
    "- Per-lead z-score normalization\n",
    "- Optional bandpass filter\n",
    "- In-memory dataset for fast batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading function defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PRE-LOAD ALL ECG SIGNALS INTO MEMORY\n",
    "# ============================================================\n",
    "# This is CRITICAL for Colab performance - loading from Google Drive\n",
    "# during training is extremely slow. Pre-loading takes a few minutes\n",
    "# but makes each epoch ~30x faster.\n",
    "\n",
    "def bandpass_filter(ecg, sampling_rate=500, lowcut=0.5, highcut=40):\n",
    "    \"\"\"Apply bandpass filter to ECG signal.\"\"\"\n",
    "    nyq = 0.5 * sampling_rate\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = scipy_signal.butter(3, [low, high], btype='band')\n",
    "    filtered = scipy_signal.filtfilt(b, a, ecg, axis=0)\n",
    "    return filtered\n",
    "\n",
    "def load_all_signals(df, data_path, sampling_rate=500, seq_len=5000, \n",
    "                     normalize=True, apply_bandpass=True):\n",
    "    \"\"\"\n",
    "    Pre-load ALL ECG signals into a single numpy array.\n",
    "    This avoids slow Google Drive I/O during training.\n",
    "    \"\"\"\n",
    "    signals = []\n",
    "    failed = 0\n",
    "    \n",
    "    # filename_hr/filename_lr already contains the full relative path (e.g., \"records100/00000/00001_lr\")\n",
    "    filename_col = 'filename_hr' if sampling_rate == 500 else 'filename_lr'\n",
    "    \n",
    "    for idx in tqdm(range(len(df)), desc=f\"Loading {len(df)} ECGs\"):\n",
    "        row = df.iloc[idx]\n",
    "        filepath = str(data_path / row[filename_col])\n",
    "        \n",
    "        try:\n",
    "            record = wfdb.rdrecord(filepath)\n",
    "            ecg = record.p_signal  # (time, 12)\n",
    "            \n",
    "            # Ensure correct length\n",
    "            if len(ecg) < seq_len:\n",
    "                ecg = np.pad(ecg, ((0, seq_len - len(ecg)), (0, 0)))\n",
    "            elif len(ecg) > seq_len:\n",
    "                ecg = ecg[:seq_len]\n",
    "            \n",
    "            # Bandpass filter\n",
    "            if apply_bandpass:\n",
    "                try:\n",
    "                    ecg = bandpass_filter(ecg, sampling_rate)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Per-lead normalization\n",
    "            if normalize:\n",
    "                mean = ecg.mean(axis=0, keepdims=True)\n",
    "                std = ecg.std(axis=0, keepdims=True) + 1e-8\n",
    "                ecg = (ecg - mean) / std\n",
    "            \n",
    "            # Convert to (channels, time) for Conv1D\n",
    "            ecg = ecg.T  # (12, seq_len)\n",
    "            signals.append(ecg)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to zeros if loading fails\n",
    "            signals.append(np.zeros((12, seq_len)))\n",
    "            failed += 1\n",
    "    \n",
    "    signals_array = np.array(signals, dtype=np.float32)\n",
    "    print(f\"\u2705 Loaded {len(signals)} ECGs ({failed} failed)\")\n",
    "    print(f\"   Memory: {signals_array.nbytes / 1e9:.2f} GB\")\n",
    "    \n",
    "    return signals_array\n",
    "\n",
    "print('Pre-loading function defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRE-LOADING ALL ECG SIGNALS INTO MEMORY\n",
      "This will take a few minutes, but training will be ~30x faster!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 17100 ECGs:   0%|          | 0/17100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 17100 ECGs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17100/17100 [02:45<00:00, 103.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Loaded 17100 ECGs (0 failed)\n",
      "   Memory: 0.82 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 2155 ECGs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2155/2155 [00:21<00:00, 98.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Loaded 2155 ECGs (0 failed)\n",
      "   Memory: 0.10 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 2162 ECGs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2162/2162 [00:20<00:00, 107.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Loaded 2162 ECGs (0 failed)\n",
      "   Memory: 0.10 GB\n",
      "\n",
      "\u2705 All data loaded!\n",
      "   X_train: (17100, 12, 1000) (0.82 GB)\n",
      "   X_val:   (2155, 12, 1000) (0.10 GB)\n",
      "   X_test:  (2162, 12, 1000) (0.10 GB)\n",
      "   Total:   1.03 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PRE-LOAD ALL DATA INTO MEMORY\n",
    "# ============================================================\n",
    "# This takes ~5-10 minutes but makes training MUCH faster!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-LOADING ALL ECG SIGNALS INTO MEMORY\")\n",
    "print(\"This will take a few minutes, but training will be ~30x faster!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_train = load_all_signals(df_train, DATA_PATH, SAMPLING_RATE, SEQ_LEN)\n",
    "X_val = load_all_signals(df_val, DATA_PATH, SAMPLING_RATE, SEQ_LEN)\n",
    "X_test = load_all_signals(df_test, DATA_PATH, SAMPLING_RATE, SEQ_LEN)\n",
    "\n",
    "print(f\"\\n\u2705 All data loaded!\")\n",
    "print(f\"   X_train: {X_train.shape} ({X_train.nbytes/1e9:.2f} GB)\")\n",
    "print(f\"   X_val:   {X_val.shape} ({X_val.nbytes/1e9:.2f} GB)\")\n",
    "print(f\"   X_test:  {X_test.shape} ({X_test.nbytes/1e9:.2f} GB)\")\n",
    "print(f\"   Total:   {(X_train.nbytes + X_val.nbytes + X_test.nbytes)/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 535\n",
      "Val batches: 68\n",
      "Test batches: 68\n",
      "\n",
      "\u26a1 Sample batch loaded in 187.2ms\n",
      "   X=torch.Size([32, 12, 1000]), y=torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FAST IN-MEMORY DATASET & DATALOADERS\n",
    "# ============================================================\n",
    "\n",
    "class FastECGDataset(Dataset):\n",
    "    \"\"\"Simple in-memory dataset for pre-loaded signals.\"\"\"\n",
    "    def __init__(self, signals, labels):\n",
    "        self.signals = torch.FloatTensor(signals)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.signals[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets from pre-loaded data\n",
    "train_dataset = FastECGDataset(X_train, y_train)\n",
    "val_dataset = FastECGDataset(X_val, y_val)\n",
    "test_dataset = FastECGDataset(X_test, y_test)\n",
    "\n",
    "# Optimized DataLoaders\n",
    "# - num_workers=0 for parallel loading\n",
    "# - pin_memory=True for faster GPU transfer\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                         num_workers=0)\n",
    "\n",
    "print(f'Train batches: {len(train_loader)}')\n",
    "print(f'Val batches: {len(val_loader)}')\n",
    "print(f'Test batches: {len(test_loader)}')\n",
    "\n",
    "# Test loading speed\n",
    "import time\n",
    "t0 = time.time()\n",
    "X_sample, y_sample = next(iter(train_loader))\n",
    "print(f'\\n\u26a1 Sample batch loaded in {(time.time()-t0)*1000:.1f}ms')\n",
    "print(f'   X={X_sample.shape}, y={y_sample.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architectures\n",
    "\n",
    "### Model A: 1D CNN (Residual Blocks)\n",
    "- Temporal convolutions with skip connections\n",
    "- Global Average Pooling\n",
    "- Fully connected classifier\n",
    "\n",
    "### Model B: CNN + BiLSTM\n",
    "- CNN for local feature extraction\n",
    "- BiLSTM for temporal dependencies\n",
    "\n",
    "### Model C: CNN + Attention\n",
    "- CNN backbone\n",
    "- Multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1D output shape: torch.Size([32, 5])\n",
      "CNN1D parameters: 1,974,949\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MODEL A: 1D CNN WITH RESIDUAL BLOCKS\n",
    "# ============================================================\n",
    "\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.dropout(out)\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, n_leads=12, n_classes=5, seq_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv1d(n_leads, 32, kernel_size=15, padding=7)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res1 = ResidualBlock1D(32, 64, stride=2)\n",
    "        self.res2 = ResidualBlock1D(64, 128, stride=2)\n",
    "        self.res3 = ResidualBlock1D(128, 256, stride=2)\n",
    "        self.res4 = ResidualBlock1D(256, 256, stride=2)\n",
    "        \n",
    "        # Global pooling\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 12, 5000)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        \n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Test model\n",
    "model_test = CNN1D().to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = model_test(X_sample.to(DEVICE))\n",
    "print(f'CNN1D output shape: {out.shape}')\n",
    "print(f'CNN1D parameters: {sum(p.numel() for p in model_test.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_BiLSTM output shape: torch.Size([32, 5])\n",
      "CNN_BiLSTM parameters: 268,965\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MODEL B: CNN + BiLSTM\n",
    "# ============================================================\n",
    "\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, n_leads=12, n_classes=5, seq_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN feature extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(n_leads, 32, kernel_size=15, padding=7),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "        )\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(128, 64, num_layers=2, batch_first=True, \n",
    "                           bidirectional=True, dropout=0.3)\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 12, 5000)\n",
    "        x = self.cnn(x)  # (batch, 128, seq)\n",
    "        x = x.permute(0, 2, 1)  # (batch, seq, 128)\n",
    "        \n",
    "        x, _ = self.lstm(x)  # (batch, seq, 128)\n",
    "        x = x[:, -1, :]  # Last timestep\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Test model\n",
    "model_test = CNN_BiLSTM().to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = model_test(X_sample.to(DEVICE))\n",
    "print(f'CNN_BiLSTM output shape: {out.shape}')\n",
    "print(f'CNN_BiLSTM parameters: {sum(p.numel() for p in model_test.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Attention output shape: torch.Size([32, 5])\n",
      "CNN_Attention parameters: 106,109\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MODEL C: CNN + ATTENTION\n",
    "# ============================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=4):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, time)\n",
    "        avg = self.avg_pool(x).squeeze(-1)\n",
    "        max_ = self.max_pool(x).squeeze(-1)\n",
    "        attn = self.fc(avg) + self.fc(max_)\n",
    "        return x * attn.unsqueeze(-1)\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(channels, channels // 4)\n",
    "        self.key = nn.Linear(channels, channels // 4)\n",
    "        self.value = nn.Linear(channels, channels)\n",
    "        self.scale = (channels // 4) ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, channels)\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        return out + x\n",
    "\n",
    "class CNN_Attention(nn.Module):\n",
    "    def __init__(self, n_leads=12, n_classes=5, seq_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN with channel attention\n",
    "        self.conv1 = nn.Conv1d(n_leads, 32, kernel_size=15, padding=7)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.ca1 = ChannelAttention(32)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=7, padding=3)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.ca2 = ChannelAttention(64)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.ca3 = ChannelAttention(128)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attn = TemporalAttention(128)\n",
    "        \n",
    "        # Global pooling + classifier\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 12, 5000)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.ca1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.ca2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.ca3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Temporal attention\n",
    "        x = x.permute(0, 2, 1)  # (batch, time, channels)\n",
    "        x = self.temporal_attn(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, time)\n",
    "        \n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Test model\n",
    "model_test = CNN_Attention().to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = model_test(X_sample.to(DEVICE))\n",
    "print(f'CNN_Attention output shape: {out.shape}')\n",
    "print(f'CNN_Attention parameters: {sum(p.numel() for p in model_test.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup\n",
    "\n",
    "- Binary Cross-Entropy loss with class weights\n",
    "- Adam optimizer with learning rate scheduling\n",
    "- Early stopping on validation Macro F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined (with tqdm progress bars).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS (with progress bars)\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, epoch, epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Progress bar for batches\n",
    "    pbar = tqdm(loader, desc=f'Epoch {epoch+1}/{epochs} [Train]', \n",
    "                leave=False, ncols=100)\n",
    "    \n",
    "    for batch_idx, (X, y) in enumerate(pbar):\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Threshold at 0.5\n",
    "    pred_binary = (all_preds > 0.5).astype(int)\n",
    "    macro_f1 = f1_score(all_labels, pred_binary, average='macro', zero_division=0)\n",
    "    \n",
    "    return total_loss / len(loader), macro_f1\n",
    "\n",
    "def evaluate(model, loader, criterion, desc='Val'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'         [{desc}]', leave=False, ncols=100)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in pbar:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    pred_binary = (all_preds > 0.5).astype(int)\n",
    "    macro_f1 = f1_score(all_labels, pred_binary, average='macro', zero_division=0)\n",
    "    \n",
    "    return total_loss / len(loader), macro_f1, all_preds, all_labels\n",
    "\n",
    "print('Training functions defined (with tqdm progress bars).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "def train_model(model, model_name, train_loader, val_loader, epochs=50, patience=10, lr=1e-3):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'TRAINING: {model_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': []}\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        train_loss, train_f1 = train_epoch(model, train_loader, optimizer, criterion, epoch, epochs)\n",
    "        val_loss, val_f1, _, _ = evaluate(model, val_loader, criterion, desc='Val')\n",
    "        \n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            marker = ' \u2605'\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            marker = ''\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d}/{epochs} | '\n",
    "              f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | '\n",
    "              f'Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | '\n",
    "              f'{elapsed:.1f}s{marker}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f'\\nBest Val F1: {best_val_f1:.4f}')\n",
    "    \n",
    "    return model, history, best_val_f1\n",
    "\n",
    "print('Training loop defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING: Model A: 1D CNN\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# TRAIN MODEL A: 1D CNN\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m      5\u001b[39m model_a = CNN1D(n_leads=N_LEADS, n_classes=N_CLASSES, seq_len=SEQ_LEN).to(DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model_a, history_a, best_f1_a = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mModel A: 1D CNN\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, model_name, train_loader, val_loader, epochs, patience, lr)\u001b[39m\n\u001b[32m     17\u001b[39m patience_counter = \u001b[32m0\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     t0 = \u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     train_loss, train_f1 = train_epoch(model, train_loader, optimizer, criterion, epoch, epochs)\n\u001b[32m     23\u001b[39m     val_loss, val_f1, _, _ = evaluate(model, val_loader, criterion, desc=\u001b[33m'\u001b[39m\u001b[33mVal\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAIN MODEL A: 1D CNN\n",
    "# ============================================================\n",
    "\n",
    "model_a = CNN1D(n_leads=N_LEADS, n_classes=N_CLASSES, seq_len=SEQ_LEN).to(DEVICE)\n",
    "model_a, history_a, best_f1_a = train_model(\n",
    "    model_a, 'Model A: 1D CNN', \n",
    "    train_loader, val_loader, \n",
    "    epochs=EPOCHS, patience=PATIENCE, lr=LEARNING_RATE, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN MODEL B: CNN + BiLSTM\n",
    "# ============================================================\n",
    "\n",
    "model_b = CNN_BiLSTM(n_leads=N_LEADS, n_classes=N_CLASSES, seq_len=SEQ_LEN).to(DEVICE)\n",
    "model_b, history_b, best_f1_b = train_model(\n",
    "    model_b, 'Model B: CNN + BiLSTM', \n",
    "    train_loader, val_loader, \n",
    "    epochs=EPOCHS, patience=PATIENCE, lr=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN MODEL C: CNN + ATTENTION\n",
    "# ============================================================\n",
    "\n",
    "model_c = CNN_Attention(n_leads=N_LEADS, n_classes=N_CLASSES, seq_len=SEQ_LEN).to(DEVICE)\n",
    "model_c, history_c, best_f1_c = train_model(\n",
    "    model_c, 'Model C: CNN + Attention', \n",
    "    train_loader, val_loader, \n",
    "    epochs=EPOCHS, patience=PATIENCE, lr=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE ON TEST SET\n",
    "# ============================================================\n",
    "\n",
    "def compute_test_metrics(model, model_name, test_loader):\n",
    "    print(f'\\nEvaluating {model_name}...')\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    _, _, preds, labels = evaluate(model, test_loader, criterion, desc='Test')\n",
    "    \n",
    "    pred_binary = (preds > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'macro_f1': f1_score(labels, pred_binary, average='macro', zero_division=0),\n",
    "        'micro_f1': f1_score(labels, pred_binary, average='micro', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    for i, cls in enumerate(SUPERCLASSES):\n",
    "        metrics[f'precision_{cls}'] = precision_score(labels[:, i], pred_binary[:, i], zero_division=0)\n",
    "        metrics[f'recall_{cls}'] = recall_score(labels[:, i], pred_binary[:, i], zero_division=0)\n",
    "        metrics[f'f1_{cls}'] = f1_score(labels[:, i], pred_binary[:, i], zero_division=0)\n",
    "        try:\n",
    "            metrics[f'auroc_{cls}'] = roc_auc_score(labels[:, i], preds[:, i])\n",
    "        except:\n",
    "            metrics[f'auroc_{cls}'] = np.nan\n",
    "    \n",
    "    return metrics, preds, labels\n",
    "\n",
    "# Evaluate all models\n",
    "results_a, preds_a, labels_a = compute_test_metrics(model_a, 'CNN1D', test_loader)\n",
    "results_b, preds_b, labels_b = compute_test_metrics(model_b, 'CNN+BiLSTM', test_loader)\n",
    "results_c, preds_c, labels_c = compute_test_metrics(model_c, 'CNN+Attention', test_loader)\n",
    "\n",
    "print('Test evaluation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESULTS COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('TEST SET RESULTS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Summary table\n",
    "results_df = pd.DataFrame([\n",
    "    {'Model': 'Baseline (ML)', 'Macro F1': BASELINE_MACRO_F1, 'Micro F1': '-'},\n",
    "    {'Model': 'CNN1D', 'Macro F1': f\"{results_a['macro_f1']:.4f}\", 'Micro F1': f\"{results_a['micro_f1']:.4f}\"},\n",
    "    {'Model': 'CNN+BiLSTM', 'Macro F1': f\"{results_b['macro_f1']:.4f}\", 'Micro F1': f\"{results_b['micro_f1']:.4f}\"},\n",
    "    {'Model': 'CNN+Attention', 'Macro F1': f\"{results_c['macro_f1']:.4f}\", 'Micro F1': f\"{results_c['micro_f1']:.4f}\"},\n",
    "])\n",
    "\n",
    "print('\\n\ud83d\udcca MODEL COMPARISON:')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_result = max([results_a, results_b, results_c], key=lambda x: x['macro_f1'])\n",
    "print(f'\\n\ud83c\udfc6 BEST MODEL: {best_result[\"model\"]} (Macro F1 = {best_result[\"macro_f1\"]:.4f})')\n",
    "\n",
    "# Compare to baseline\n",
    "improvement = best_result['macro_f1'] - BASELINE_MACRO_F1\n",
    "if improvement > 0:\n",
    "    print(f'\u2705 BEATS BASELINE by +{improvement:.4f}')\n",
    "else:\n",
    "    print(f'\u274c Below baseline by {improvement:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PER-CLASS PERFORMANCE (BEST MODEL)\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print(f'PER-CLASS METRICS ({best_result[\"model\"]})')\n",
    "print('=' * 80)\n",
    "\n",
    "per_class_df = pd.DataFrame([\n",
    "    {\n",
    "        'Class': cls,\n",
    "        'Precision': f\"{best_result[f'precision_{cls}']:.4f}\",\n",
    "        'Recall': f\"{best_result[f'recall_{cls}']:.4f}\",\n",
    "        'F1': f\"{best_result[f'f1_{cls}']:.4f}\",\n",
    "        'AUROC': f\"{best_result[f'auroc_{cls}']:.4f}\"\n",
    "    }\n",
    "    for cls in SUPERCLASSES\n",
    "])\n",
    "print(per_class_df.to_string(index=False))\n",
    "\n",
    "# Compare all models per class\n",
    "print('\\n' + '=' * 80)\n",
    "print('PER-CLASS F1 COMPARISON')\n",
    "print('=' * 80)\n",
    "print(f'{\"Class\":<8} {\"CNN1D\":<10} {\"CNN+BiLSTM\":<12} {\"CNN+Attention\":<14}')\n",
    "print('-' * 50)\n",
    "for cls in SUPERCLASSES:\n",
    "    print(f'{cls:<8} {results_a[f\"f1_{cls}\"]:<10.4f} {results_b[f\"f1_{cls}\"]:<12.4f} {results_c[f\"f1_{cls}\"]:<14.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Curves & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING CURVES\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "histories = [history_a, history_b, history_c]\n",
    "names = ['CNN1D', 'CNN+BiLSTM', 'CNN+Attention']\n",
    "\n",
    "for i, (hist, name) in enumerate(zip(histories, names)):\n",
    "    # Loss\n",
    "    axes[0, i].plot(hist['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0, i].plot(hist['val_loss'], label='Val', linewidth=2)\n",
    "    axes[0, i].set_xlabel('Epoch')\n",
    "    axes[0, i].set_ylabel('Loss')\n",
    "    axes[0, i].set_title(f'{name} - Loss', fontweight='bold')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1\n",
    "    axes[1, i].plot(hist['train_f1'], label='Train', linewidth=2)\n",
    "    axes[1, i].plot(hist['val_f1'], label='Val', linewidth=2)\n",
    "    axes[1, i].axhline(y=BASELINE_MACRO_F1, color='r', linestyle='--', label='Baseline')\n",
    "    axes[1, i].set_xlabel('Epoch')\n",
    "    axes[1, i].set_ylabel('Macro F1')\n",
    "    axes[1, i].set_title(f'{name} - Macro F1', fontweight='bold')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL COMPARISON BAR CHART\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Macro F1 comparison\n",
    "models = ['Baseline', 'CNN1D', 'CNN+BiLSTM', 'CNN+Attention']\n",
    "macro_f1s = [BASELINE_MACRO_F1, results_a['macro_f1'], results_b['macro_f1'], results_c['macro_f1']]\n",
    "colors = ['#95a5a6' if f < BASELINE_MACRO_F1 else '#2ecc71' for f in macro_f1s]\n",
    "colors[0] = '#3498db'  # Baseline\n",
    "\n",
    "bars = axes[0].bar(models, macro_f1s, color=colors, edgecolor='black')\n",
    "axes[0].axhline(y=BASELINE_MACRO_F1, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "axes[0].set_ylabel('Macro F1')\n",
    "axes[0].set_title('Model Comparison (Macro F1)', fontweight='bold')\n",
    "axes[0].set_ylim([0.5, 0.85])\n",
    "for bar, f1 in zip(bars, macro_f1s):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{f1:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Per-class F1 for best model\n",
    "class_f1s = [best_result[f'f1_{cls}'] for cls in SUPERCLASSES]\n",
    "class_colors = ['#2ecc71', '#e74c3c', '#3498db', '#9b59b6', '#f39c12']\n",
    "bars = axes[1].bar(SUPERCLASSES, class_f1s, color=class_colors, edgecolor='black')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title(f'Per-Class F1 ({best_result[\"model\"]})', fontweight='bold')\n",
    "axes[1].set_ylim([0, 1])\n",
    "for bar, f1 in zip(bars, class_f1s):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{f1:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 80)\n",
    "print('ANALYSIS & DISCUSSION')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'''\n",
    "1. PERFORMANCE SUMMARY\n",
    "   ====================\n",
    "   \n",
    "   Baseline (ML):     Macro F1 = {BASELINE_MACRO_F1:.4f}\n",
    "   CNN1D:             Macro F1 = {results_a['macro_f1']:.4f}\n",
    "   CNN+BiLSTM:        Macro F1 = {results_b['macro_f1']:.4f}\n",
    "   CNN+Attention:     Macro F1 = {results_c['macro_f1']:.4f}\n",
    "   \n",
    "   Best Model: {best_result['model']}\n",
    "   Improvement: {'+' if best_result['macro_f1'] > BASELINE_MACRO_F1 else ''}{(best_result['macro_f1'] - BASELINE_MACRO_F1):.4f}\n",
    "\n",
    "2. PER-CLASS ANALYSIS\n",
    "   ====================\n",
    "   \n",
    "   NORM: F1 = {best_result['f1_NORM']:.4f}\n",
    "   - Strong performance (largest class, clear pattern)\n",
    "   \n",
    "   MI:   F1 = {best_result['f1_MI']:.4f}\n",
    "   - Deep learning captures ST-segment/Q-wave morphology well\n",
    "   \n",
    "   STTC: F1 = {best_result['f1_STTC']:.4f}\n",
    "   - Challenging due to overlap with other conditions\n",
    "   \n",
    "   CD:   F1 = {best_result['f1_CD']:.4f}\n",
    "   - Conduction patterns (BBB, AV blocks) are distinct\n",
    "   \n",
    "   HYP:  F1 = {best_result['f1_HYP']:.4f}\n",
    "   - Voltage criteria benefit from raw signal processing\n",
    "\n",
    "3. 500 Hz vs 100 Hz\n",
    "   =================\n",
    "   \n",
    "   - Higher resolution captures subtle morphological details\n",
    "   - Better for HF components of QRS complex\n",
    "   - Increased computational cost (5x more samples)\n",
    "   - Beneficial for CD and MI detection\n",
    "\n",
    "4. MODEL COMPLEXITY\n",
    "   =================\n",
    "   \n",
    "   CNN1D:        {sum(p.numel() for p in model_a.parameters()):,} parameters\n",
    "   CNN+BiLSTM:   {sum(p.numel() for p in model_b.parameters()):,} parameters\n",
    "   CNN+Attention: {sum(p.numel() for p in model_c.parameters()):,} parameters\n",
    "   \n",
    "   Trade-off: More complex models may overfit on PTB-XL size (~18k train samples)\n",
    "\n",
    "5. RECOMMENDATIONS\n",
    "   ================\n",
    "   \n",
    "   - CNN1D is efficient and performs well\n",
    "   - Attention helps with temporal dependencies\n",
    "   - BiLSTM may be overkill for this dataset size\n",
    "   - Consider ensemble of models for production\n",
    "   - Focus on improving STTC and HYP classes\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 80)\n",
    "print('\ud83c\udfaf PTB-XL DEEP LEARNING CLASSIFICATION - FINAL SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'''\n",
    "DATASET:\n",
    "  Total ECGs: {len(df_filtered):,}\n",
    "  Train/Val/Test: {len(df_train):,} / {len(df_val):,} / {len(df_test):,}\n",
    "  Sampling Rate: {SAMPLING_RATE} Hz\n",
    "  Sequence Length: {SEQ_LEN} samples\n",
    "\n",
    "MODELS TRAINED:\n",
    "  1. CNN1D (Residual blocks)\n",
    "  2. CNN+BiLSTM (Temporal modeling)\n",
    "  3. CNN+Attention (Channel + Temporal attention)\n",
    "\n",
    "RESULTS:\n",
    "''')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f'''\n",
    "BEST MODEL: {best_result['model']}\n",
    "  Macro F1: {best_result['macro_f1']:.4f}\n",
    "  vs Baseline: {'+' if best_result['macro_f1'] > BASELINE_MACRO_F1 else ''}{(best_result['macro_f1'] - BASELINE_MACRO_F1):.4f}\n",
    "\n",
    "KEY INSIGHTS:\n",
    "  \u2713 Deep learning can match/beat classical ML on PTB-XL\n",
    "  \u2713 500 Hz signals provide better morphological detail\n",
    "  \u2713 Raw waveforms work well without hand-crafted features\n",
    "  \u2713 Attention mechanisms help with interpretability\n",
    "  \n",
    "NEXT STEPS:\n",
    "  1. Try larger models (ResNet-18, EfficientNet-1D)\n",
    "  2. Multi-task learning with rhythm labels\n",
    "  3. Data augmentation (time warping, noise injection)\n",
    "  4. Ensemble methods\n",
    "  5. External validation on other datasets\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODELS & RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# Save best model\n",
    "torch.save(model_a.state_dict(), OUTPUT_PATH / 'cnn1d_best.pth')\n",
    "torch.save(model_b.state_dict(), OUTPUT_PATH / 'cnn_bilstm_best.pth')\n",
    "torch.save(model_c.state_dict(), OUTPUT_PATH / 'cnn_attention_best.pth')\n",
    "\n",
    "# Save results\n",
    "results_to_save = {\n",
    "    'baseline_f1': BASELINE_MACRO_F1,\n",
    "    'cnn1d': {k: float(v) if isinstance(v, (float, np.floating)) else v for k, v in results_a.items()},\n",
    "    'cnn_bilstm': {k: float(v) if isinstance(v, (float, np.floating)) else v for k, v in results_b.items()},\n",
    "    'cnn_attention': {k: float(v) if isinstance(v, (float, np.floating)) else v for k, v in results_c.items()},\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH / 'dl_results.json', 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "print('\u2705 Models and results saved to outputs_dl/')\n",
    "print(f'   - cnn1d_best.pth')\n",
    "print(f'   - cnn_bilstm_best.pth')\n",
    "print(f'   - cnn_attention_best.pth')\n",
    "print(f'   - dl_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}